<?xml version="1.0" encoding="UTF-8"?>
<entries><item><sectionId>58</sectionId><postDate>2023-08-16T07:00:00-04:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>370515</_authorId><id>995451</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>2f16158d-8951-4189-b6e0-9a7f0054eae3</uid><siteSettingsId>1481306</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>605860</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>The Why of Software Testing</title><slug>ep-11-why-of-software-testing</slug><uri>resources/podcasts/ep-11-why-of-software-testing</uri><dateCreated>2023-08-15T15:47:22-04:00</dateCreated><dateUpdated>2023-08-16T09:25:16-04:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>995451</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep-11-why-of-software-testing</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep-11-why-of-software-testing</url><authorId>370515</authorId><typeId>96</typeId><description>Software testing is important, but why exactly do we do it? And how should we examine our digital quality efforts over time? James Mortlock of Vodafone explores these questions and more.</description><publishDate>2023-08-16 11:00:00</publishDate><episodeNumber>11</episodeNumber><episodeLength>28</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>16gypj0rif</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p dir="ltr"&gt;Software testing is important, but why exactly do we do it? And how should we examine our digital quality efforts over time? These are necessary questions that many QA organizations don’t ask.&lt;/p&gt;
&lt;p dir="ltr"&gt;James Mortlock, Lead UAT Test Manager at Vodafone, joins the podcast to discuss what should go into a software testing strategy — and why. We also talk about delivering better value with digital quality initiatives, and why efficiency ultimately benefits everybody.&lt;br /&gt;&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>James Mortlock</podcastGuestName><podcastGuestPhoto><item>995452</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p dir="ltr"&gt;James Mortlock is the Lead UAT Test Manager at Vodafone. He previously held roles in digital automation, analytics implementation and served as a Scrum Master.&lt;br /&gt;&lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p dir="ltr"&gt;&lt;strong&gt;DAVID CARTY&lt;/strong&gt;: At the end of the day, some people prefer to relax with a glass of wine and their favorite TV show, if they have access to the remote control, but other people prefer to keep their hands busy and their minds active. That's what led James Mortlock to leathercrafting. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JAMES MORTLOCK&lt;/strong&gt;: I stumbled upon a specific YouTuber that did just no talking, just making leathercraft stuff, and it was beautifully shot and the audio was amazing. This was before everyone was playing ASMR across everything. It was beautiful, and I thought, how hard could it be? I tried it, very hard, and I just got less and less bad at it, effectively. There's like the kind of creativity side of my, I guess of my brain, that I don't necessarily always get to explore with every single part of my job. So it's good to be able to flex that sort of creativity side of the brain. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: There's something meditative in the hobby for James. Who knew making a wallet could bring you one step closer to enlightenment? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: I'm not a big fan of just sitting, watching TV, and that sort of stuff. I need to be kind of being productive and creative and doing something. So this was good because I could sit on the sofa and do it with other people around, and it's very meditative. I do sort of mental health neurodiversity talks and stuff like that, and talking about meditation, about focusing the non-complicated side of your brain and turning the complicated one off. That's like I've got to pay this bill and these need washing and the kids need picking up, all that sort of stuff, that you turn that side off and just let the left hand, right hand, left hand, right hand, take over and you just feel yourself unwind a little bit. So, yeah, it's meditative and keeps me busy. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And the rewarding part of leathercrafting is that it yields a tangible output, something you can hold in your hand or give to a loved one, not to mention that it's opened up James to a new community of friends with a shared interest. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: It started out just making like really basic card holders, so like three pockets, one either side and one in the middle type thing, practicing stitching, and yeah, it evolved into things like mugs, wallets, and backpacks and stuff like that. So, yeah, just wherever my brain sparked off to, do a sketch, and then try and turn it into a reality, which is a lot harder than just writing it down. It's easier said than done, comes to mind. I, myself, am neurodiverse, so that whole being able to have something to focus on that has a tangible output, that if you need something quick, you can make something quick, and gets you that really-- yeah, you can feel literally and figuratively and emotionally. You can feel that sense of achievement. When I start talking about something that I know bits and pieces on or I'm passionate about, there's no stopping me, like going and meeting like minded people and people coming to my stall and asking me about stuff. Yeah, I think that's the real achievement there, is being able to connect with other people and share a little dark corner of the world or illuminate a dark corner of the world that you didn't really know existed. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: This is the Ready, Test, Go. podcast, brought to you by Applause. I'm David Carty. &lt;/p&gt;
&lt;p dir="ltr"&gt;Today's guest is a leathercrafter and Lead UAT Test Manager at Vodafone, James Mortlock. Beyond his testing background, James has experience with automation. In fact, he constructed the first UAT automation of voice assistants via Raspberry Pi. He was also a product owner for analytics implementation, and he was even a Scrum Master. So, you can believe him when he says, the purpose of software testing is often poorly understood across the organization and, sometimes, even by testers themselves. It's easy to fall into a pattern of behavior and never question why you are performing a certain task and whether it delivers any real value. So, how should we reframe how we look at this critical task of software testing to ultimately deliver more value with the approach? Well, that's for James to know and for us to find out. &lt;/p&gt;
&lt;p dir="ltr"&gt;James, I love the premise of this episode, The Why of Software Testing. So let's start by answering that question, why do we test software and what should be the stated goal of testing software? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: So, yeah, interesting question, and it can be, I guess, as deep and as shallow as you want it to be. But I guess the why came from probably came from my personality a little bit because it's just this constant, why, why. Like if the answer doesn't solve the solution, I'll just keep going for the solution.&lt;/p&gt;
&lt;p dir="ltr"&gt;Let's start at the big one for me with testing and the way that development, solutions, and architecture, everything, has evolved, that testing isn't what it used to be. It's not the-- developers will produce code and, in comparison to modern day standards now, it was awful. And you would need that that testing was absolutely required to make sure that it functionally worked, and that's now not the thing. We're now in the state of play where people are like unit tests, question mark, like do we even need unit tests? Because the software quality is so high that we're now kind of moving out and up closer towards the kind of UI end of things. So it's completely transformed the kind of like what testing actually means and the test engineer or just tester doesn't really exist anymore.&lt;/p&gt;
&lt;p dir="ltr"&gt;It's now more like quality assurance, and it's very generalized. You can use quality assurance in any industry, any role, and you can have a decent conversation for 95% of it and not need to be in the same industry because you're there to make sure that you're being an ambassador for effectively doing the right thing. You're like the guardian of the user experience. A lot of it will be baking in quality right at the start, so making sure that you have quality assurance, not only you have an actual person that is regarded as quality assurance, but have it there as an ethos with everybody.&lt;/p&gt;
&lt;p dir="ltr"&gt;So you have things like the quality narrative, what's the quality narrative within your company. And it's like, who owns quality? Everybody does, right? If you want it to be fast and cheap and really high standard, it is possible, just everybody needs to be thinking the right way. So you're not only there to catch the human error right at the start, so static testing guess, but you're there to kind of beat it into people really, so that when the documents then reproduced or the next program or project is started, the level of quality is just higher even in the documentation process. It's like it's the same sort of idea of like testing was a big thing and unit tests were huge. And now, it's the opposite way around. So same sort of thing, we're building quality that you can focus on improving the customer experience, rather than efficiency in lines of code. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. And you touched on a few things there that I want to ask you about a little bit later on. But just because we should test software doesn't mean that we should blindly test everything, the way that it was done before. So how can organizations make sense of what does or does not deliver value when it comes to testing? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: See, I probably touched on this. Loads of times that I talk about this, but it's all about the results and to kind of, again, talk about it again because it's quite an interesting topic. It's pulling people's safety blankets away, in terms of getting rid of unit testing. How many times have you had a bug that would have gone into production that was caught by unit tests? It may take, what, 45 seconds to run and a couple of minutes to write, but extrapolate that across the lifetime of a project and whatnot, it's pointless.&lt;/p&gt;
&lt;p dir="ltr"&gt;You should be making sure that you are-- just the way some of the bigger tech companies have automatic deletion of dead bits of code, same happens with not only test cases and regression cycles and regression suites, but techniques as well. Sometimes techniques will die out and you need to repurpose them or remove them. So it's about making sure that you're not falling into the trap of just because it's quick means it's cheap and, therefore just do it. Squeeze every penny. Be really, really efficient with it. &lt;/p&gt;
&lt;p dir="ltr"&gt;And if it's the pesticide paradox, right, if you keep doing the same thing over and over and over again, and the bugs are going to get resistant, if you're talking about real life. But you're not going to find anything. All the bugs will be where you're not testing. So if you focus on-- that's where you've built the quality really, really high-- if you focus on that all the time, the bugs will be outside of this area. So stop testing there, start testing somewhere else. And if you start to see a re-emergence of issues and bugs, then that's where you have to be flexible and like re-implement new strategies and increase, kind of quality assurance in that area. So you don't need to test.&lt;/p&gt;
&lt;p dir="ltr"&gt;The testing, in theory, should just be-- testers in theory, shouldn't exist. You should be redundant because the quality of the software is so high that you don't need to test anymore, right. That's the idea. We're there as kind of like a backstop. It's not the silver bullet that everybody thinks it is. The silver bullet is better quality, it's not more testing. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Absolutely. And that's an interesting idea, talking about unit tests being redundant. And you hit on that a little bit already. Are there other QA techniques or approaches that organizations should take a little bit of a deeper look at in terms of the value that they deliver, and whether or not those are redundant as well? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: Yes. For every company. Even the ones that are like best in class, and even the ones that are just starting up and don't really have any quality assurance, it's just developer production. But it's like-- it's going to be one of those cliches, but it's personal to the company in terms of where the quality has been built. And it's where you're not seeing issues. This reminds me very much of the-- I'm not sure if you've seen the diagram of the bombers coming back from the war and it's showing you where they've been hit. And they've said, right, we need to put armor there. It's like, no, those ones are all the ones that are actually coming back and surviving. Maybe we should put the armor where they're not being hit because they're the ones that aren't coming back. That's obviously the weak point. We're not seeing the ones that crash because they don't come back. So that must be where the issues are.&lt;/p&gt;
&lt;p dir="ltr"&gt;So that's what we should be doing. Using data that's coming back from either testing or production to then be specific in the way that we're armoring our bombers as it were. Because otherwise, you just get too heavy. You can't fly, you can't fly as fast, and you're effectively hamstringing yourself by kind of over protecting or over testing as it were. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: This idea of adhering to your software quality narrative ultimately has a positive impact on everybody, right? I mean, you're all about efficient processes and a thoughtful approach to how you spend your budget, things like this. It's nice to save the business some money, but an efficient approach benefits everybody, right? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: Definitely. So the big one is-- or the easiest one to get past the shareholders and seniors is money. It's cash at the end of the day, isn't it? If you can make efficiencies in costs, you're going to probably get stuff signed off. But with that comes-- like say you've got a test environment that's just-- like we've got a place called Spaghetti Junction, and it's just a overlap and cross of roads and bridges and whatnot. If you've got an environment that's like that, it's going to be a nightmare being able to keep it in sync and your cycle time instead of being like a day will be like 10 days because the intricacies are so huge. You go through and rip that out and make it streamlined and efficient, you're going to save the business days and days and days. But also, your developers and your QAs are going to be so much happier. Because being able to just test something or just deploy something is going to be so much easier. So you save the business loads of money, then all of your employees within development are way happier, therefore they're more productive, therefore you make more money. So it's like the infinite reward. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And this idea, this outcome, hinges on that shared responsibility approach to digital quality, right. So understanding the why of software testing probably goes a long way toward getting the rest of the organization to buy in, which is what you started talking about earlier a little bit. Beyond that, what else can organizations do to foster this idea of a shared approach to digital quality, and why is that so important? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: So long story short is it's cheaper, for one. So if you get whoever writes your initial documentation, whether it's a business requirement document or a user story or a Cucumber scenario, whatever it is, if that quality is higher, then therefore, it will extrapolate out to less bugs. And you catch all of the bugs-- you go as far left as possible and catch as many bugs as you can because there's QA engineers and developers with the right mindset. And software developers in test actually in the kind of sizing and shaping, you're going to add more quality there. It's almost free there because those people are being paid for. You remove that. And if everybody's invested in quality, they'll want to remove those. Whereas back in the day, you would just do your normal dev test, it would go into prod, you'd find all the bugs, you'd put them back in. That's a mega, mega expensive way of doing it. That's probably going to get everybody kind of behind it.&lt;/p&gt;
&lt;p dir="ltr"&gt;But again, it's the it's the happiness thing as well. You're going to get happier people. You get less of the, oh, what is this coming through from the developer. Being like, oh, this doesn't make any sense. This is impossible. It's not going to be three story points, this is like 21 story points. It's got to go back through. And then you get a developer that's interpreted a piece of requirement that they haven't seen yet, and/or that's been written poorly. And it goes over to test and they're like, I was in that refinement. This is not what they were talking about. We have to redo this. If you breed this quality mindset rather than just testing and unit test coverage and all that sort of stuff, you get it by being lean, right. Build quality in rather than kind of shoehorning it in the end and trying to manufacture quality. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And this takes time, right? I mean, this takes a lot of time, a lot of effort. You have to win over a lot of hearts and minds. I'm sure that's not always an easy battle, right?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: Yeah, definitely. So the strategy-- there's lots of strategies to go about this. You can be like a belligerent warlord and just battle people until they just surrender, I guess, or you can-- &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: I'm guessing that's not the way to do it, though. I'm guessing there's a better way. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: Well, it depends. It depends how great you want to be, I guess, and how long you've got. It's one way. It's like hammering a screw into a wall. You can do it, it just won't be pretty. It will probably be able to hold a picture up but I wouldn't want to put my TV on it, let's put it that way. And you'll make an awful mess.&lt;/p&gt;
&lt;p dir="ltr"&gt;So I took-- so I went through a transformation process with my company, and a secondary one as well, in terms of environments. And taking a sales strategy or a pervasive marketing strategy on this is very advantageous. Where you go for the early adopters, that's the best way to do it. You want to incite pervasive excitement, you go to the people that want to do it. You go to them first and you rally them up and you pep them up, and then you watch the seniors who weren't involved say, about all this excitement and I wasn't involved and this thing sounds like something that I should be behind and I should be the sponsor of this. And it's pervasive that way. If you try to battle through the people who don't want to do it, you're going to have a really tough time. And then the propaganda that comes out of that is going to be a bit lackluster anyway. Whereas if you go for the people that are there for quality, they're there because they want to change because make money or make happy or whatever, that pervasive happiness will be, yeah, exponential. That's the way I'd go about it. Not just because it's easier. It's just more productive in the long run, for sure. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And at the end of the day too, it's really all about delivering a product that the customer not only wants but will want to use and there's a differentiation there. How can organizations better prioritize the customer's needs as part of their digital quality strategy? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: Yeah, I guess this is an interesting one. I've spoken about this, I guess, a couple of times, where you've got to remember you're not delivering code. Code is the enabler to customer experience. So you're delivering-- like people say, oh, yeah, it's just a user story. You've got to remember it's like concentrate. It is a user story. It's a story, right. And it's got to make sense. It's not-- as a product owner, I want this field to equal that so that this will then have that output. That's not a story, that's just a requirement. That's kind of-- you're kind of missing the point of writing things that way. If you're going to do that, go back to just writing requirements. You want a story of a user, and you want to be inclusive of journeys and see how people experience the code as it were. You need to be kind of thinking like that. You're not a work item machine, or a code delivery machine, or a user story machine. You are delivering user stories or user journeys that create customer benefit and business benefit. You need to stop thinking like an engineer and start thinking like a customer. That's the big one. We've spent all this time trying to make sure that the code is as great as possible and super efficient. And really, really jazzy with all the new JavaScript and Node functions and whatnot. That's great. We've got there. We can quit that now. Let's have a look at the user experience shall we? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: James, in one sentence, what does digital quality mean to you? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: I would describe it like-- I remember the first way someone described punk rock to me. It's not just a genre, it's a way of life. And it's very similar. It should be. It's not just a title or it's not just a thing that you say, it's an ethos. You know, like people talk about DevOps or Lean or Kanban or whatever. You can't just do it, you need to be it. And the company needs to be there. So yeah, it needs to be-- almost, it needs to be like a feeling and a movement rather than like a process, because people tend to switch off from stuff like that. And people are going to need to want to do it. So digital quality is everybody wanting that customer experience at the end. And the employee experience, in terms of deployment and delivery, being really high quality and very low effort. That's the gold isn't it? We want to get to that. And digital quality helps enable that, in my opinion. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: I'll have to follow up with some band recommendations or something like that, James. So stay tuned on that. But let's go to the next question. What is one software testing trend that you find promising? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: So we're having this-- I was having this discussion with our principal engineers. And one thing that I want-- I'm desperate to talk to them about, and we're going to dedicate an entire working group session to, is mutation testing.&lt;/p&gt;
&lt;p dir="ltr"&gt;So it's kind of like-- it's when you can crawl and you figured that out, and then you can walk, yeah, you figured that out, you can run, yeah, and then there's the sprinting. So don't sprint before you can crawl. So we're not quite there yet so we need to figure out the running bit. But mutation testing is, in my opinion, is the next best thing. And it's kind of silly because it's like testing whether your testing is testing properly. It shows the quality of your testing, so you're purposefully mutating things and checking to see that your quality processes and your testing processes are robust. So it's like, who will guard the guards? Like you're making sure that the guards are guarding properly, and your tests are testing correctly. A mutation testing is kind of like an offshoot of chaos testing. Will check like the robustness of your quality systems. So I'm quite interested in that. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And as we've talked about this, clearly a need a need for that introspection, right, and to evaluate your own processes. So that makes sense. James, what is your favorite app to use in your downtime? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: I spend a lot of time in YouTube. Really, really-- yeah, I'm on the beta for YouTube as well, and have been for a while. I'm testing even when I'm not testing. That's quite sad, just thought of that. &lt;/p&gt;
&lt;p dir="ltr"&gt;That's just someone that-- or if I refer to them, if I'm to personify a company, that's someone who knows what they're doing in terms of, we want to deliver a customer experience, so what do we want them to experience? And it can have bugs, potentially. But it's the fact that the thought that has gone in to enrich a small area of an application, that's very important for me.&lt;/p&gt;
&lt;p dir="ltr"&gt;One of the earliest moments of that-- that is just one of those-- that is just excellent kind of moments is, in Slack, on the mobile app, if you used to be able to grab the top corner of a picture and then swipe, instead of the picture just falling down, it would spin based on the physics of where you've put your finger and where you've thrown it. It's absolutely unnecessary, I don't need that, but very much enjoy it. And it's that level of detail that, I think, makes a good app. People can have P1s and P2s and you just say, yeah, it's broken, or the back end is down. But it's the P3s and the P4s that really make an app outstanding. And that sort of stuff that was obviously like a hackathon, like a Hack Day thing, that's gone into prod. That's what makes it great app, in my opinion. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: It's the little touches sometimes. Just those little details go a long way. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: Exactly. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And James, finally, what is something you are hopeful for? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: Oh, wow. Something that I'm hopeful for that-- I'm hopeful for, I guess, two of the points that we've touched on today is that quality, like ethos that mindset, is like baked into everybody rather than it being trying to deliver things and deliver code. It's about delivering customer experience and delivering it with quality. I'm hopeful for that. The momentum of that seems to be picking up because we're getting into that area where the old techniques, software quality techniques, are becoming redundant because that area has just been. You've rounded that circle so much, it's absolutely perfectly round, so you can move on to the next shape now. I think that's a big one for me. Almost everything that we've spoken about, I think I'm quite hopeful that the tables are kind of turned from how much can we kind of plumb into development. It's now, how much can we kind of invest in quality because it's going to be cheaper in the long run? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Well, James, this has been fun. Thank you so much for joining us. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MORTLOCK&lt;/strong&gt;: Thank you very much. Thanks for having me. And hopefully everybody's learned something or enjoyed it.&lt;/p&gt;</podcastTranscript><resourceImage><item>995453</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Watch Now</resourceButtonText><resourceButtonUrl>https://www.applause.com/resources/podcasts/ep-11-why-of-software-testing</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2023-08-15T16:11:32-04:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle>The Why of Software Testing</seoTitle><siteNamePosition></siteNamePosition><seoDescription>Join James Mortlock of Vodafone on this episode of the Ready, Test, Go podcast as he discusses the key elements of an efficient testing strategy.</seoDescription><seoKeywords></seoKeywords><seoImage>{{ seomatic.helper.socialTransform(entry.resourceImage.collect()[0], "base", 0, "crop") }}</seoImage><seoImageWidth>{{ seomatic.helper.socialTransformWidth(entry.resourceImage.collect()[0], "base", 0, "crop") }}</seoImageWidth><seoImageHeight>{{ seomatic.helper.socialTransformHeight(entry.resourceImage.collect()[0], "base", 0, "crop") }}</seoImageHeight><seoImageDescription>James Mortlock</seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><robots>true</robots><canonicalUrl>true</canonicalUrl></inherited><overrides><seoTitle>true</seoTitle><seoDescription>true</seoDescription><seoImage>true</seoImage><seoImageDescription>true</seoImageDescription></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets></sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromCustom</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds></seoImageIds><seoImageSource>fromField</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item><item><sectionId>58</sectionId><postDate>2023-07-19T07:00:00-04:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>370515</_authorId><id>991898</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>061dfc25-c49c-4be0-a519-4e92aadf6fa0</uid><siteSettingsId>1475717</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>602224</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>Fostering Data Quality and Governance</title><slug>ep-10-fostering-data-quality-governance</slug><uri>resources/podcasts/ep-10-fostering-data-quality-governance</uri><dateCreated>2023-07-18T15:40:13-04:00</dateCreated><dateUpdated>2023-08-01T15:48:28-04:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>991898</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep-10-fostering-data-quality-governance</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep-10-fostering-data-quality-governance</url><authorId>370515</authorId><typeId>96</typeId><description>Civic tech service designer and author Lauren Maffeo discusses where organizations miss the mark on data quality and governance, a problem that costs businesses millions of dollars per year.</description><publishDate>2023-07-19 11:00:00</publishDate><episodeNumber>10</episodeNumber><episodeLength>34</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>ujbnxecyvy</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p dir="ltr"&gt;Many businesses have immature data governance processes reminiscent of bad diets — garbage in, garbage out. Insights gleaned from data, whether by human eyes or AI, are only valuable if the data is readable and relevant. For too many businesses, it’s not.&lt;/p&gt;
&lt;p dir="ltr"&gt;Civic tech service designer, author and lecturer Lauren Maffeo joins the podcast to discuss where organizations miss the mark on data quality, a problem that costs businesses millions of dollars per yet. She’ll discuss findings from her book, Designing Data Governance from the Ground Up: Six Steps to Build a Data-Driven Culture, and how governance-driven development can foster shared data responsibility across the organization.&lt;br /&gt;&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>Lauren Maffeo</podcastGuestName><podcastGuestPhoto><item>991899</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p&gt;Lauren Maffeo is an award-winning civic tech service designer at Steampunk. She is the founding editor of Springer's AI and Ethics journal and an area editor for the open access journal Data and Policy. Lauren is the author of the book Designing Data Governance from the Ground Up: Six Steps to Build a Data-Driven Culture, and has also written for publications such as Harvard Data Science Review, Financial Times and The Guardian. She is also an adjunct lecturer at George Washington University and a former Gartner analyst.&lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p dir="ltr"&gt;&lt;em&gt;(This transcript has been edited for brevity.)&lt;/em&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;DAVID CARTY&lt;/strong&gt;: Are you a dog person? Or are you a cat person? There's no right answer. Well, I guess, it depends on who you ask. Growing up, Lauren Maffeo was more of a cat person, but in her adult life, fostering dogs has become one of her devoted passions. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LAUREN MAFFEO&lt;/strong&gt;: I have loved animals my whole life. But it's funny, because when I was growing up in Massachusetts, my neighborhood was very cat-centric. It's hard to say when I really started getting more attached to dogs. I think, that was actually in adulthood, probably, in my 20's. And I do like both, but I would say, I'm definitely more of a dog person. I bought my own apartment, and I knew immediately that I wanted to start fostering, with intention to adopt when it was a good match.&lt;/p&gt;
&lt;p dir="ltr"&gt;So I started doing that two weeks after I moved into my apartment. I ended up adopting the third dog that was brought with me to foster. She was a senior pit bull with advanced kidney disease, and so she didn't have a lot of adoption interest. And I got pretty attached to her, so I did end up adopting Ella. And she only got about eight more months with me after the adoption before passing away, but that was what we knew was going to happen.&lt;/p&gt;
&lt;p dir="ltr"&gt;I did end up adopting another dog, and so I do have it now, a three-year-old rescue. We don't know yet what his breed is, I haven't done the test yet. But he's probably like a husky, Jack Russell, pitty mix, so he's much smaller. He's got bright blue eyes. And I still foster a dog about every three months because he likes having a buddy around. So with any luck, I should be getting another foster this weekend. And if I do, that will be my 10th in two years.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Over the last two years, Lauren has taken care of quite a few furry friends such as:&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: Delilah was my first. She was a 1-year-old pit bull. Amy, who was a puppy, and she was a lab mix of some sort, Ella was definitely a pitty-lab mix. I did watch Spots for a while, he was an English setter. Cece, who was a Chihuahua lab retriever mix. Cleo, who was a black lab-Chihuahua mix; Lady, who was a shepherd mix of some kind, and then, there was Penny, who, I think, is maybe like a schnauzer mix. So those are my best guesses for the ones that I've had so far. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: But Lauren only goes the rescue route. With so many pets in need of forever homes, Lauren takes her responsibility seriously, as fostering just one dog can actually help save several.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: Shelters are very pressed for space, especially in summer months. And so that means that, if animals are not adopted, or they're not fostered, they get stuck in shelters. And if they're in a high-kill shelter, that means that they're at risk of being euthanized just for being in the shelter when they don't have a behavioral problem or any major health issues. And so then, if you foster, you get a dog out of the shelter, but you also make room for another dog to come in.&lt;/p&gt;
&lt;p dir="ltr"&gt;And so fostering is something I'm really passionate about encouraging people to do. I meet a lot of people who say, that's nice that you do that, I could never do that, and I don't really believe that. Obviously, not everybody has the time or space to do it, but I think-- especially for people who love animals, but don't want the full time responsibility of a pet-- fostering is perfect. And it literally does save several lives because every time you have a pup or cat come into your home, you're making space for another animal to come in and be cared for at the shelter. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: This is the Ready, Test, Go. podcast brought to you by Applause. I'm David Carty.&lt;/p&gt;
&lt;p dir="ltr"&gt;Today's guest is dog foster and civic tech service designer, Lauren Maffeo. Lauren's work is everywhere, when it comes to data governance and policy. She is the founding editor of Springer's AI and Ethics journal and editor for the Open Access Journal, Data and Policy, and she's written for The Guardian and Financial Times, among many others. Her latest book called Designing Data Governance from the Ground Up published in January. Data drives the decisions that businesses make, obviously, but what kind of data is collected? How is it processed and stored? Is it compliant with regulations and standards? And are you making the most out of it? Heck, do you even know strategically what you want to do with all that data as it keeps piling up? Well, as it turns out, these are difficult questions for many organizations to answer. Lax data governance and poor data quality can be a recipe for disaster. That's what Lauren wrote about, and that's what she talked about too.&lt;/p&gt;
&lt;p dir="ltr"&gt;Let's start by understanding the scope of the problem. Some of the most influential companies that we have are data-based business models, and you write about this in the book. Poor data quality costs businesses millions of dollars per year, so how did businesses get to this level of immaturity? And how pervasive is the problem? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: That's a great question. If you think about people who work in tech, and if you work in tech yourself, which I know you do, you're familiar with this concept of technical debt, it's the idea that you get into an environment, and you are responsible, not just for creating something from scratch, you're also responsible for fixing whatever that big problem is. And as somebody who works in civic tech, which is not known for being the most forward-thinking sector in regards to technology, you go in, and you go in knowing that the problem is going to be bad. And then, the degree to which it is bad still continues to shock me, in terms of the outdated systems used, the lack of practices, the lack of automation of very repetitive tasks, which takes a lot of time from humans that they could be using to put towards other strategic initiatives.&lt;/p&gt;
&lt;p dir="ltr"&gt;And so when we think about this concept of technical debt, which is still very pervasive in tech, in code, now, think about technical debt in the context of data and think about it, not only in that context, but in the context of how much data is created and consumed by organizations every day, the scope of the problem is-- unwieldy doesn't even begin to describe it. It's really beyond anything that I think we can really comprehend because not only are you ingesting huge amounts of data per day and producing new amounts of data, you also have, probably, millions of unique data points in your possession as an organization. And the concept of going in and trying to, quote, unquote, "govern all of them" is really overwhelming. It's overwhelming on a very basic level. Now, consider that, most organizations don't have the talent, in-house, to manage a project of that scale, even if you do have a chief data officer or a data scientist on your team, even if you hold those roles yourself. A big premise of my book is that, this work is too big for one team or one person to manage on their own, which is why you really need to invest in building a data-driven culture that co-owns and co-creates governance where everybody has some ownership and responsibility for it. Because without that ownership and responsibility, that technical debt, as it pertains to data, is only going to grow.&lt;/p&gt;
&lt;p dir="ltr"&gt;And the problem is really pervasive. There's a survey in the book that I discussed which surveyed C-level leaders at various organizations, and only one in four respondents to that survey said that their organization is data-driven. And that number was down from 38%, in the survey, the year before. So the number of organizational leaders who say that they're data-driven is going down, meanwhile, consider how much more data is being produced at a breakneck pace today. And this was taken way before ChatGPT came to market. And obviously, the last six to eight months have completely changed the game, in terms of making AI more mainstream. And so these are these are things to consider, that, basically, the amount of data being produced continues to grow exponentially, while the amount of leaders who say that they're data-driven is going down. And that really does present a big problem, especially as it pertains to data quality, especially as this data becomes more used by the public in a wider range of contexts, &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. A massive problem and a growing problem, right? I mean you just phrased it perfectly. And you mentioned a few things there that I want to touch on later in this episode as well. But let's start with maybe some of the building blocks that organizations can put in place to help build a more mature data governance plan. What does that look like? And I understand that there are some lessons that we can learn from their product strategy efforts, right? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: That's right. So you don't necessarily have to reinvent the wheel when it comes to data governance because it can feel like a very abstract concept. It can also feel like it's purely a set of barriers to innovation. Everybody talks about legal coming in and saying, you can't do that, and squashing a project. And so governance does not have a good connotation with many people in the tech world. But the reality is that if you look at what you do as an org for product strategy, developing a roadmap, developing key tasks and milestones with owners of each milestone, there's a lot that carries over into data. And actually, one of the biggest opportunities in big data management right now is this concept of data as a product. It's this idea that you-- that rather than giving out data as a service, where it's managed, top down, by, let's say, the CIO and they control access to the data, as well as all of the definitions, even though they might not have that domain expertise-- when you manage data as a product, you really take a domain-focused attitude towards it.&lt;/p&gt;
&lt;p dir="ltr"&gt;So you have defined what your key domains are, which are the areas that you collect data about thematically. So for instance, you could have a sales data domain. You could have a marketing data domain, a customer success data domain. These are the core building blocks of your business that allow you to categorize data and its subsequent metadata. And that ultimately allows everybody to not only find the data that they're looking for more easily, but it also gives you an opportunity to define what those data points mean in the context of your business.&lt;/p&gt;
&lt;p dir="ltr"&gt;And then, ultimately, what you can do is take a product mindset towards rolling that data out for use across your organization. Now, this does not mean that you have to give every single person access to every single data set. Not only is that foolhardy. That's also just not possible, really, and it's not best practice. But what you should be doing is setting up data stewards who own the different data domains. And because they're the domain experts, they're the best people to decide who should get access to their data sets to define what the data points are, to really get that data prepared for your data engineers who can then load it into the environment, into pipelines, and all of the technical aspects of the environment that you work in. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And you mentioned it's not just one person's responsibility, and this is where a data governance council can be useful, which I know is something that you advocate for. So can you explain what your ideal data governance council might look like and how that group can exert some influence throughout the organization? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: Sure, I think this is really the most critical aspect of doing data governance well for a few reasons. One is that this really has to be a top-down effort. If you look at any big initiative that succeeds in an organization, it succeeds because it has the right executive sponsorship and the right buy-in from someone at the very top. And so you can be the most senior member of the data team in your organization. You can be the chief data officer. If you don't have the backing of your CIO or your COO, your CEO, you're only going to get so far because your success really depends on the time, money, and autonomy that you are given to solve this problem. So one of the biggest things that is really important for any data governance council is to get the right executive sponsor. We're past mentorship at this point. You really need a sponsor for the council who is dedicated to making sure that it succeeds, making sure that the council is working on data projects that benefit the business and can give the resources, in both time and talent and money, to help that council succeed.&lt;/p&gt;
&lt;p dir="ltr"&gt;And then in terms of who should be on the council, you really want to make sure that all data stewards who are overseeing each major data domain have representation on that council, because you want to create an environment where there's cross-functional data leadership meeting on a regular basis to come to a consensus on everything from data definitions, to the charter, to approving various tools that you can bring into your stack to help manage data. So for instance, if your team is interested in bringing in AWS SageMaker for machine learning and they want to-- and they want to do that in sales, that's an example of where that-- the data governance council could come in and look at the proposed use case and approve it.&lt;/p&gt;
&lt;p dir="ltr"&gt;As I described what a data governance council could do, I'm very aware that it can sound like I'm talking about death by committee. And I think, left unchecked, this could go wrong that way. I think if it's not managed well, like any council, there is the risk that it just becomes a meeting point without much action or initiative taken to follow up on whatever you discuss at your council meetings. And so it really is on organizations and on the chair of the council to ensure that these meetings not only occur regularly but that they have key takeaways, they have notes, they have specific action items that people are expected to fulfill in between those meetings. And if you think about it, this is really no different than serving on a board of directors or serving on any other committee you've been on. And so anybody who has nonprofit management experience is a great opportunity to serve-- there's a great opportunity for them to serve as a data steward because they can take that experience of building consensus across functions, of capturing action items, reporting on progress. I was pleasantly surprised to see how much-- how many aspects of nonprofit management data governance councils could learn from because they already do a lot in terms of transparency and reporting and meeting that I think people in data should be copying. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: That's an interesting thread. I wouldn't have considered that.&lt;/p&gt;
&lt;p dir="ltr"&gt;You're touching on this here, but one task that the council must undertake is defining data governance principles. And there's obviously the myriad regulatory and industry standards for data, right? And those are extremely important. But there's also the contract, written or otherwise, between the company and the customer, and regulations might not move fast enough or at all. You live in the Washington, DC, area. I don't have to tell you that-- to protect the customer's interests, right? So how can a business collect and manage data in a way that is ethical and also clearly defined for customers? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: I think that transparency above all else is really key here because, as you said, to say that we have a tech-illiterate group of politicians is being kind. The innovation is always going to move 10 steps ahead of the legislation. And the reality is I would love for it to not get to a stage where it needs to be regulated. I would love to see organizations take ownership of the data that they have, to be responsible stewards of it, to protect customer data. We have ample evidence to show that this is not what happens. We have ample evidence to show that when organizations' financial success rests on having data-driven business models, that they are going to exploit those loopholes and do whatever they can to monetize that data to at users' expense, even if they are giving them very-- even if they are giving away very valuable consumer data, like social security numbers, private photos, things like that.&lt;/p&gt;
&lt;p dir="ltr"&gt;And so when you think about your data council's principles, you really want to bring it back to that wider framework I talk about in chapter one of the book, where you're talking about the key areas that you want your data framework and governance council to follow. And I talk about how you don't have to invent this framework from scratch. There's already a lot out there that can help you decide how to govern your data in a more holistic way. And Gartner has a seven-step framework that I really like that I use as a model in the book because it focuses on trust, transparency and ethics, values and outcomes, risk and security accountability, and decision rights, collaboration and culture, and education and training.&lt;/p&gt;
&lt;p dir="ltr"&gt;And so, basically, when you're thinking about your data governance council, you want to ensure not only that all of your data domains are defined and that they have clear stewards to act as owners of the domains. You also want to tie any new project or tool back to at least one aspect of that framework. And as you see, trust is at the top of the framework. Transparency and ethics is second. If you, as an organization, are not transparent about what you're doing with consumer data, your consumers really have no reason to trust you or to think that you are stewarding it appropriately. And to date, we've been a pretty lax society about that, at least in the US. We, as a society, have very freely given away a lot of our data for convenience because it's easier for us to give it away than it is to have to do something a bit more manually. But I think in 5 to 10 years, that landscape is going to change. I think people are going to expect more of organizations. They are going to expect more transparency about data. And while I don't think that we will necessarily ever reach the stage of GDPR legislation in Europe, which gives private citizens much more rights-- many more rights over their data, I still think, nonetheless, that there's no question the tide is turning, and people, overall, are getting more savvy about what data they give to whom. And so organizations really need to get ahead of that by showing consumers how they are respectable stewards of data. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Hopefully, we'll be able to rebottle the genie there a little bit on that one. But as you write about in the book, there are actually a few examples that we can learn from the media industry. So what lessons can we learn from Blockbuster about defining a data roadmap? And if it helps, I probably still have my membership card. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: Wow, I'm impressed by that. There was a Blockbuster just up the road from my parents' place. And of course, it's long gone, and I lament that every time I drive by.&lt;/p&gt;
&lt;p dir="ltr"&gt;But Blockbuster is a very interesting story because, as we both acknowledged, it was very successful for decades. It was, really, a cornerstone of people who grew up in the '80s and '90s, in particular. And I open chapter four of the book, talking about how Reed Hastings, who founded Netflix, actually tried to sell Netflix to Blockbuster several-- a few decades ago now. And he had-- he thought that was going to be the best opportunity for Netflix. He thought that that was the best they could do, was to get acquired by Blockbuster because Blockbuster was the player in the video rental space, and Netflix was pure video rentals at the time. And so he made an offer to the then CEO of Blockbuster to buy. The CEO thought it was-- the number was too high and refused to buy it, and we all know what happened after that. Blockbuster went under, and Netflix thrived, to put it kindly. Blockbuster eventually completely went out of business while Netflix continued to grow. And even though they're experiencing a bit of a downturn right now, they've become a much bigger player in the video and streaming space than Blockbuster ever could have been.&lt;/p&gt;
&lt;p dir="ltr"&gt;And the takeaway there, I think, is that Blockbuster had a mountain of data about buying behavior, rental behavior, which industry-- which people rented particular genres of films. And so because they were the leader in that space, if they had seen that there was a move towards digital, they could have really acted on it and actually crushed Netflix before it became big. But because they didn't have that foresight, they ended up folding as an organization because they didn't innovate fast enough by acting on the resources that they had. So I think that is a real cautionary tale for people because it's not enough to just have all of this data in your possession. If you are not organizing it, combing it for quality, making sure that it is used in the right context by the right people, you risk falling to the same fate as Blockbuster. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Now, let's keep on the media thread and talk about a Netflix migration that you wrote about in the book. How can their approach to governance-driven development-- what can organizations learn from that to help establish a healthy data infrastructure, both at a point in time and as standards and technology evolve into the future? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: Yeah, I use Netflix as a case study in chapter five of the book because when I was reading this case study about how they moved all of their data over to a streaming platform, I saw a lot of takeaways that would be really valuable for managing big data in a governance-driven environment. And so one example that really comes to mind is that they were experimenting with new technology. Streaming architecture was very new at the time that Netflix was trying to move all of its data over to it. And the risk was very high. I mean, people never-- if you do your job right, your customers are never going to know that you did a migration. But they will, for sure, know if you mess it up. And you're dealing with new technology. They were dealing with open source technology. So the risk was high.&lt;/p&gt;
&lt;p dir="ltr"&gt;And something that I thought this engineering manager did that was so exceptional was he actually literally created a test environment where his team had room to fail. They had never worked with Kafka clusters before, as one example. And so he knew that if he just gave them all of the data and let them run with it, they were probably going to screw it up. So instead, he created this test environment where they could experiment with the clusters. They could move dummy data from one environment to the other. They could make mistakes, and then they could learn from those mistakes in time for the migration. It was not a fully pain-free migration when it did happen, but ultimately, it was very successful.&lt;/p&gt;
&lt;p dir="ltr"&gt;And similarly, I think, as the head of a data governance council, as a data leader in your org, you do have to allow for the fact that failures, if you will, are going to happen, especially because a big part of this work is education and training. That's one of the steps in Gartner's framework, and you really can't underestimate it. This is new for almost everyone. Even people who have been in the AI space for decades-- they have never dealt with data at this pace, at this volume. This is something that is new for everyone. And that does equal the playing field, to some degree, because regardless of your role, this is uncharted territory. And so if you are a leader, you not only have a responsibility to train your full team. You also are responsible for giving them an environment where they can learn about these aspects of data governance without being terrified that they're going to mess up and put their jobs at risk. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. And you've mentioned AI now, a few times. AI is advancing so quickly and has even changed so much since you released this book a few short months ago. Most notably, we've seen the rise of generative AI models, which show a ton of promise but has its immediate and long-term challenges, too, right? If organizations are already fairly immature with their data governance processes, what does that mean for generative AI and other advanced AI systems that interpret all of that data? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: I think we can expect that the quality of those models is going to decline over time because one of the things that these models do is that they're constantly ingesting and learning from new data to refine their results-- hopefully improve their results. But we have a lot of bad data out there, which means that it has not been governed. It hasn't been assessed against any quality standards. There is no one really checking if it's right or wrong in various contexts. And AI only learns from the data that it's given. So if you give it, quote unquote, "bad data," you're only going to get as good of a result.&lt;/p&gt;
&lt;p dir="ltr"&gt;And so the result is that, unless we are, as a society and as organizations, more thoughtful and strategic about how we clean, destroy, govern all of our data moving forward, I think there are going to be serious implications for the products that these models produce. If you look at ChatGPT, that's the most consumer-friendly version of generative AI on the market today, and I know people who use it all day every day for various reasons. And it can have a lot of benefits, for sure, depending on the use case. But I think what's going to happen over time is that these models, which depend on very large amounts of data-- if the quality of the data that they're fed does not increase, the quality of their results will decrease. And then, as a result, we won't make as much progress as we think we're making when it comes to AI.&lt;/p&gt;
&lt;p dir="ltr"&gt;And I also just want to say on that note that there-- there's this ongoing conversation about AI that is going to overtake humans, destroy the world, that it's as big a threat as nuclear war. And I really would encourage people to think more critically about not only those statements themselves but who is making them because those warnings, so to speak, are coming from many of the people who created these models in the first place. And that type of language implies that the machines have minds of their own, a.k.a. that they cannot be controlled by humans. And I, again, would like people to think a little more critically about that because, at the end of the day, AI is just data, and it only learns from the data that it's given. And it is only available in the world in so far as it is put into the world by the humans who make it. And so when we assign these human characteristics to machines, we shift the blame of the consequences from the people who create them to the machines themselves. And I would really encourage people to not do that because then it stops holding the people accountable who create these solutions in the first place. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Absolutely. So we're talking pretty potentially severe consequences if we get all of this wrong. Just one last question for you, Lauren, because I think it's always good to have different perspectives here. As a consumer and as a customer, you're as prone to data breaches as any of us. How do you protect yourself, and what recommendations would you make for anybody else? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: That's a great question. So I am a fan of VPNs. I bought a VPN for my personal computer and my cell phone last year. And so VPNs protect people from seeing your search history and looking at your phone. And so, if you if you want a little more privacy in that regard, that's important.&lt;/p&gt;
&lt;p dir="ltr"&gt;I would also say that you should enable two-factor authentication for all tools-- digital tools that you can. I know that Google, for instance, allows you to activate 2FA. And that basically means that when you're trying to log in to your Gmail on your desktop, it also sends a notification to your phone, so that should also be done.&lt;/p&gt;
&lt;p dir="ltr"&gt;And then you also want to be really aware of not clicking on links from people you don't know because phishing is getting more and more common. It's something that we deal with at my company every day. We get attempted phishing attacks. And I will say those phishing attempts are getting more strategic and more real in terms of how they're presented. I have had people impersonate my colleagues and text me asking me to call them back or click on a link, and it was not my colleagues. It was people trying to extort information from me. And so being very smart about which links you click on and where information is coming from is really crucial. And I do think that there's a lot that the data world can learn from cybersecurity and CISOs who are in charge of security. We know that the biggest source of a breach at an organization is a company's employee who, through no ill intent, ends up inadvertently letting in a hacker or exposing company data. And so as a result, we're seeing more organizations be proactive about educating their workforces to be cyber smart, showing them what a phishing attempt looks like, giving mini quizzes to test their knowledge and tracking the results to see how they increase or decrease over time. And there's no reason that CDOs can't be doing the same thing to create a more data-literate workforce. The specifics of that will vary depending on your organization, who's in it, what type of education they need. But you don't need to put everybody into a room for 10 hours, teach them everything about data governance, and then expect them to go on their merry way. It's really something that you have to integrate into everyone's workflow over time, and actually, that's a more sustainable way to go about it. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: OK, Lauren, in one sentence, what does digital quality mean to you? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: Digital quality means that your data is fit for its intended use and ready to be consumed by outside audiences. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: What is one software testing trend that you find promising? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: Well, I like hacker testing. I think trying to hack your own creations is a really cool way to get inside someone else's head and also test the strength of what you've built. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And that goes by a few different names, right? Destructive testing, chaos engineering. There's a variety of different ways of approaching that. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: Yep. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: What's your favorite app to use in your downtime? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: Oh, that's a good question. Well, I could give a more enlightened answer, but it's Instagram. I use Instagram pretty regularly-- not just to post and look at photos but also to keep in touch with friends. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Absolutely. And Lauren, what's something that you are hopeful for? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MAFFEO&lt;/strong&gt;: I'm hopeful about the conversation we're having, that AI does have downsides. This is a very large, extensive conversation, and I worry that it is going in many different directions rather than being more focused and concentrated. Having said that, the fact that people are aware of it and asking those questions about the possible risks that does encourage me because it shows that hopefully we, as a society, have enough vested interest in ensuring that the technology is used to benefit us rather than to harm us, either intentionally or inadvertently.&lt;/p&gt;</podcastTranscript><resourceImage><item>991900</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Watch Now</resourceButtonText><resourceButtonUrl>https://www.applause.com/resources/podcasts/ep-10-fostering-data-quality-governance</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2023-08-01T15:48:27-04:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle>Fostering Data Quality and Governance</seoTitle><siteNamePosition></siteNamePosition><seoDescription>Civic tech service designer, author, and lecturer Lauren Maffeo joins the Ready, Test, Go. podcast to share insights on data quality and governance.</seoDescription><seoKeywords></seoKeywords><seoImage></seoImage><seoImageWidth></seoImageWidth><seoImageHeight></seoImageHeight><seoImageDescription>Lauren Maffeo</seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><seoImage>true</seoImage><canonicalUrl>true</canonicalUrl></inherited><overrides><seoTitle>true</seoTitle><seoDescription>true</seoDescription><seoImageDescription>true</seoImageDescription><robots>true</robots></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets></sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromCustom</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds></seoImageIds><seoImageSource>fromAsset</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item><item><sectionId>58</sectionId><postDate>2023-06-14T07:00:00-04:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>370515</_authorId><id>983866</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>7d400f5e-e225-462e-a701-51d3fed928a5</uid><siteSettingsId>1464943</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>595458</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>The Danger of Bias in Tech</title><slug>ep-9-danger-of-bias-in-tech</slug><uri>resources/podcasts/ep-9-danger-of-bias-in-tech</uri><dateCreated>2023-06-13T21:07:35-04:00</dateCreated><dateUpdated>2023-06-21T13:09:03-04:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>983866</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep-9-danger-of-bias-in-tech</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep-9-danger-of-bias-in-tech</url><authorId>370515</authorId><typeId>96</typeId><description>Are glitches really the problem? Author and professor Meredith Broussard joins the podcast to challenge tech leaders to look deeper into the biases embedded in technology.</description><publishDate>2023-06-14 11:00:00</publishDate><episodeNumber>9</episodeNumber><episodeLength>38</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>mqj33901iu</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p dir="ltr"&gt;Technology solves some of the problems in our society today, but it exacerbates others, such as discrimination based on a person’s race, gender, income level or disabilities. As companies continue to invest heavily in AI/ML technology, they must take caution to ensure new products don’t fall back on old, problematic patterns that cause harm to users or others.&lt;/p&gt;
&lt;p dir="ltr"&gt;Meredith Broussard, author of the book More than a Glitch: Confronting Race, Gender, and Ability Bias in Tech, joins the podcast to discuss the myriad ways these issues — not glitches — present themselves in modern technology and digital products, and what we can do about it.&lt;br /&gt;&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>Meredith Broussard</podcastGuestName><podcastGuestPhoto><item>983867</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p&gt;Meredith Broussard is the Associate Professor at New York University’s Arthur L. Carter Journalism Institute and Research Director at the NYU Alliance for Public Interest Technology. She is the author of Artificial Unintelligence: How Computers Misunderstand the World. Her work has been featured in the New Yorker, the New York Times, the Atlantic, BBC, Wired and the Economist.&lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p dir="ltr"&gt;&lt;strong&gt;DAVID CARTY&lt;/strong&gt;: Our digital world is changing. As AI is becoming more intelligent, and the lines between information and disinformation become more blurry, we all need to be vigilant in not only what we consume but how we consume it. Meredith Broussard believes unbiased, meticulous journalism is one of the best ways to hold the powerful accountable. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MEREDITH BROUSSARD&lt;/strong&gt;: I specialize in a kind of data journalism called algorithmic accountability reporting. And that comes from the traditional function of the press, which is to say accountability, holding power accountable. But in a world where algorithms are increasingly being used to make decisions on our behalf, that accountability function has to transfer onto algorithms and their makers. My journalistic outlook is that algorithms need to be held accountable just like power needs to be held accountable. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: The field of data journalism in particular could be especially important moving forward. Data journalists who can take their time and be methodical in their pursuit of the truth will help illuminate real issues in our society. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: Data journalism, of course, is the practice of finding stories in numbers, using numbers to tell stories. Data journalism is a little more challenging because you're doing math and you're writing at the same time. I've actually never had a problem finding students. My classes fill up every semester because people are really interested in this subject. The students really understand that this is a crucial issue, that storytelling with data is something that happens, not just inside journalism, but in every field nowadays. So one of the things I teach my students is we do kind of advanced spreadsheet stuff at the very beginning of class, which is actually the same level of analysis that they do in business school, right? So if you are going to be an investigative reporter, who is looking at company financial records, you need to know how to read financial records So data training is a really essential part of the journalism school curriculum nowadays. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: While Meredith sees new students interested in data journalism every year, it takes a skillful blend of data analysis and storytelling to really do the job well. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: So algorithmic accountability journalists-- what we do is sometimes we investigate black boxes. Sometimes what we do as algorithmic accountability journalists is we write our own code in order to investigate social phenomena. I am really enthusiastic about the profession. And I think there are some really interesting stories coming out of the algorithmic accountability world. One of the things that I do in the book is I collect a lot of amazing journalism that's been done over the past couple of years, as well as some amazing scholarship, and put all of these stories together so that people can understand the weight of this problem, the intersection of technology of race, of gender, of disability. And it's going to take some really hard cultural conversations to work our way through it. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: This is the Ready, Test, Go. podcast, brought to you by Applause. I'm David Carty. Today's guest is data journalism advocate and author Meredith Broussard. Meredith is an associate professor at New York University's Arthur L. Carter Journalism Institute and research director at the NYU Alliance for Public Interest Technology. She is also an author. Her latest book, More Than a Glitch: Confronting Race, Gender, and Ability Bias in Tech, published in March.&lt;/p&gt;
&lt;p dir="ltr"&gt;You know, digital quality means a lot of things. It means software testing, it means user experience, and so much more that we talk about here on the podcast. But, as Meredith writes about in her book, some of the issues that still plague us in society today can creep into product development. We're going to tackle some tough issues on today's podcast. So we simply ask that you come into this with an open mind and a desire to create software and digital products that work for all of your users, not just some of them. OK, let's talk with Meredith.&lt;/p&gt;
&lt;p dir="ltr"&gt;Meredith, congratulations on the book, and thank you for bringing these issues to light. I thought it would be great to get into an excerpt from the introduction of your book. And it kind of reads as a thesis to me for your book. So would you mind reading that excerpt for us? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: Sure. "Digital technology is wonderful and world-changing. It is also racist, sexist, and ableist. For many years, we have focused on the positives about technology, pretending that the problems are only glitches. Calling something a glitch means it's a temporary blip, something unexpected but inconsequential. A glitch can be fixed. The biases embedded in technology are more than mere glitches. They're baked in from the beginning. They are structural biases, and they can't be addressed with a quick code update. It's time to address this issue head on, unflinchingly, taking advantage of everything we know about culture and how the biases of the real world take shape inside our computational systems. Only then can we begin the slow, painstaking process of accountability and remediation." &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Yeah, you know, I love that excerpt, Meredith, and I think it's a powerful call to action for tech leaders who might be listening to this podcast, right? So acknowledging that the answer is probably different depending on the business or the industry, what's a good first step to beginning to examine and, as you say, address these issues head on? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: It's a complicated issue, right? And I wish there were one answer. I mean, I wish I could wave a magic wand and say, "This is how you fix everything." But it took us 30 years to get into our current situation. And so it's not going to be an easy fix.&lt;/p&gt;
&lt;p dir="ltr"&gt;So I think the first step is adding more nuance to the way that we talk about technology. In that excerpt, I wrote about how several things are true about technology at the same time, right? It is terrific, and it is also racist and sexist and ableist, right? Both of those things are true. Human brains can hold multiple truths at the same time. Computers can't, right? So that's really important to recognize.&lt;/p&gt;
&lt;p dir="ltr"&gt;We do also need to challenge an idea I call technochauvinism, the idea that technological solutions are superior, or the idea that computers can solve every problem, because there are certain social problems that we can't code our way out of, right? So, all of that said, where do we start in terms of fixing our code? Well, one place to start is context. It doesn't make any sense to say, all right, I'm going to regulate all AI everywhere throughout time, because we don't really need to regulate all AI. We need to regulate some AI, and so context here is key. I really like the distinction that is made in the new EU AI regulation, the proposed AI regulation that is about to pass. And they divide AI into high-risk and low-risk uses based on context, right? So if we take facial recognition, for example, I mentioned before that facial recognition is often used in policing, that it is biased against people with darker skin, it's better at recognizing men than recognizing women, generally does not take trans and non-binary folks into account at all. It's a very fragile technology. It doesn't work as well as people imagine. But it is used in policing. And so facial recognition used in policing on real-time video feeds is going to misidentify people, primarily people, say, with darker skin, primarily women, primarily trans and non-binary folks. That is going to happen, and so that might be a high-risk use of AI. And under this EU regulation, that would have to be registered and monitored, or perhaps it would be banned. But a low-risk use of facial recognition might be something like using facial recognition to unlock your phone. Now, mine doesn't work half the time anyway. There is a passcode that allows you to bypass the facial recognition. To the best of my knowledge, those biometrics are not going to a lot of harmful places. So that's probably a low-risk use of AI. So again, context is key. And we can start by attaching a use of AI to a particular context and then making a decision about how it gets used or what gets used in that context. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right, and I want to ask you more about facial recognition in just a little bit. But you mentioned the phrase "technochauvinism," which I believe you coined that phrase. Is that correct? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: Mhm. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Great. So you mentioned you discussed that in your previous book, and you discuss it in this one as well. Can you explain what technochauvinism is and how it ultimately has an adverse effect, not only on digital products and services, but potentially a negative effect on society as well? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: Technochauvinism is a kind of bias. It's the sense that computers are superior. When we unpack that, we discover that what it's really saying is that the people who make computers are superior to others. Another subtext there is that math is a superior method of problem solving than other methods. And when we unpack that, we've realized, oh, yeah, there are lots of ways of understanding the world. Math is one of them. It's really great, but it's not inherently better than any others, because what are computers when it comes right down to it? They're machines that do math. We tend to anthropomorphize them. We tend to get really attached to our computers and our computing devices because we spend so much time with them. We trust, entrust a lot of the logistics of our lives to them. But, ultimately, it's just-- it's a machine that's doing math. It's a dumb brick. It's not your friend.&lt;/p&gt;
&lt;p dir="ltr"&gt;So we need to challenge technochauvinist ideas. And when you do start challenging them, it becomes a little bit easier to spot the problems inside automated systems. I really like a frame that is given to us by Ruha Benjamin in her book Race After Technology. And that's the idea that automated systems discriminate by default, right? And so a technochauvinist might say, oh, automated systems are going to be more neutral, are going to be more objective, more unbiased. They're got to be a better way of making decisions. But if we back off of that, if we say, all right, well, automated systems are going to discriminate by default, it becomes easier to see the problems. Now, why do they discriminate by default? Well, it's because of the way that they're built. So the way we build AI systems or machine learning systems, which are the most common kinds of systems in use today, is we build them the same way every time. We take a whole bunch of data that is collected from the real world, and we feed the data to the computer. And we say, computer, make a model. The computer says, OK. It makes a model. That model-- it shows the mathematical patterns in the data. And then it's very powerful. You can use this model to make new decisions, to make predictions, to generate new text or new images. These are very flexible, functional, powerful models. But with great power comes great responsibility. And we also have come to see that all of the problems of the real world, all of the historical problems, are embedded in the data that we use to train the machine learning systems, right? So some of the mathematical patterns are discriminatory because there's been discrimination in the world in the past. We all know history. It's not a surprise that there's bias, that there's discrimination in the past. And so we just need to not assume that the automated systems are somehow going to be better because they're mathematical systems. Like, they are sociotechnical systems. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right, and to that point, you write quite a bit about AI and machine learning in the book and how ML models are typically and inaccurately described as a black box. The math behind those systems is complicated, but if you have access to all of the information, it's within our capability to understand why these models arrive at the conclusions that they do. So between this black-box concept and maybe the scapegoating of insufficient or problematic training data, is there willful ignorance happening on the part of some tech leaders? And how can we do a better job of maybe sourcing or validating better training data to feed these models? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: Well, I think that, I prefer not to assume malevolence. I prefer to assume that developers are going about their day and trying to write good code and do their jobs honorably. I chalk a lot of these problems up to unconscious bias. We all have unconscious bias. We're all working on it. We're trying to become better people every day. But we can't see our unconscious biases, and it's an inescapable fact that people embed their own biases in the code that they create. So when you have code that's created by a small and homogeneous group of people like we have in Silicon Valley then the collective unconscious biases of those folks get embedded in the code, right? So there is definitely some willful ignorance happening, but there is also some unconscious bias.&lt;/p&gt;
&lt;p dir="ltr"&gt;And this is why we need more regulation. There was an editorial in The New York Times recently by Lina Khan, the leader of the Federal Trade Commission, where she wrote about how it's time to regulate AI. And she lays out a framework for regulating AI. And it starts with making sure that AI systems obey the existing laws of the world, right? Existing laws and regulations. And I really like this because we've been arguing for 30 years about what kind of new regulation we need for technology. Nobody's gotten anywhere, really, so I think that a different approach might be more effective. And that approach is just enforcing existing laws inside our technical systems.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right, and to go back to the facial recognition topic, I think it's safe to say you're critical or maybe skeptical of those high-risk applications of facial recognition technology. You write in the book about research, and you mentioned it before, that shows facial recognition works more effectively in identifying light-skinned people, men as opposed to women, and that it commonly misgendered trans or nonbinary people. Can you tell us more about the way that this technology tends to fail and why it's particularly problematic in a law enforcement context? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: This goes back to historical problems and the way that they get embedded in technological systems. So when we talk about today's technology, we tend to talk about it as if it is sprung from the head of Zeus fully formed. It has not. It comes from many, many years of iterative development, and it builds on previous work. However, when you have a system like that, the sins of the past get embedded in systems unless you proactively kind of root them out.&lt;/p&gt;
&lt;p dir="ltr"&gt;So facial recognition depends on computer vision technology. Computer vision technology depends on earlier representational technology, like color film. And color film came after black-and-white film. So we can see the history of sensing technology as a continuum, right? Well, with color film, there is a very long history of representational racism in color film because Kodak, the company that pioneered color film, did not represent an entire range of skin tones when they started selling color film. The way it worked was that you would take your film into a local lab to be developed. And the local lab had equipment that was either licensed by or provided by Kodak. The equipment had to be tuned. And Kodak would send out these cards called Shirley cards in order for local labs to tune their equipment to get the colors exactly right because there's a range of possible colors. And they're called Shirley cards because the first model on the card was a woman named Shirley. And Shirley had very light skin. She was pictured with some other, I think, primary-colored pillows. And so thousands of these cards were printed. They were sent to labs all over the country or all over the world, and that was how the machines got tuned. Well, because the Shirley cards didn't have a range of browns, the film developing and color photo printing machines did not have good representations of brown colors. And so if your skin was darker than Shirley's skin, you looked really muddy in color photos. Now, Kodak did start to provide a wider range of skin tones in their Shirley cards so that labs could update their technology in the 1970s. And it's great that they did that. But the reason they did that is a little problematic. They didn't do it because they realized, oh, yeah, we are not serving the majority of the world. They were doing this in response to furniture manufacturers. So they were trying to get furniture manufacturers to switch over from black-and-white printing to color printing. And the furniture manufacturers said, well, our mahogany and walnut furniture looks really muddy in color film, and so we are not going to switch over unless you make this technology better. So it was not about inclusivity. It was about capitalism. Now, we have this problem in camera technology. Well, guess what. In computer vision technology, there are sensing issues in, for example, video game technology. Video game sensors did not pick up on people with darker skin, especially in lower light conditions. They were better at picking up on people with lighter skin, right? So then what comes after video game technology? Well, it's facial recognition technology. Look, facial recognition technology has trouble. It's mostly tuned on people with lighter skin, right? So this is a constant issue.&lt;/p&gt;
&lt;p dir="ltr"&gt;The kind of blockbuster moment for facial recognition technology came with the publication of a paper called "Gender Shades" by Joy Buolamwini, Timnit Gebru, Deb Raji, and others. And what "Gender Shades" revealed was bias in all of the major facial recognition technologies. People tend to imagine that tech is full of all of these startups and all of this wonderful diversity. But actually, there's enormous consolidation in the tech industry, and there's only a couple of firms who are making all of the big core technologies of our time. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right, and to get to another example of algorithmic bias-- your book is full of different examples, like the furniture example you just mentioned-- academia is another area where we see this happen, with new examples emerging during the remote learning era of the pandemic. Bias has existed in education long before algorithms came into play, and it's a microcosm of many of the issues we see in our society today, right? Amid the protests around algorithmic harms in academia, what is or should be done to improve these systems in the future? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: See, now we're back to the magic wand, I really wish I had a magic wand for education. I would fix a lot of things.&lt;/p&gt;
&lt;p dir="ltr"&gt;One of the situations that I wrote about in the book, is where algorithms were used to generate imaginary grades for real students. So I think a really good place to start is, don't do that. [LAUGHS] That was a particularly egregious example. That was something that happened during the pandemic, where the International Baccalaureate organization, which is an organization that awards a very prestigious secondary high school diploma globally-- the IB decided that they were not going to be able to hold in-person exams for their seniors, which makes a lot of sense because the pandemic was raging, and it wasn't safe at that point to have a lot of people in a small room for a very long period of time. This was pre-vaccine. And so it was a good decision to cancel the in-person exams. But IB decided that they were going to use an algorithm to predict the grades that the students would have gotten if they had taken the tests that didn't happen, which sounds so absurd in retrospect. But during the pandemic, we all made some strange decisions. You know, I made a decision to write a book, right? And so I write about Isabel Castañeda who, at the time, was a high school senior in Colorado who was caught up in this mess, because what the IB algorithm did is it predicted that kids from poorer schools would do badly on the tests, and it predicted that kids from wealthier schools would do well on the tests.&lt;/p&gt;
&lt;p dir="ltr"&gt;And how do things break down in education along racial lines in terms of rich and poor schools? Well, the richer schools tend to be the whiter schools. The poorer schools tend to be the schools with more black and brown students. And if you have studied education statistics at all-- because, again, what we're doing when we're doing this kind of algorithm system, where we're doing machine learning, we're doing prediction, is we're doing statistics, right? If you know anything about education statistics, you know that wealth is a predictor of educational success, right? Wealthier kids do better in school than poorer kids. So if we wanted to increase educational attainment, in the United States, at least, we would eliminate poverty, right? And how do you eliminate poverty? You don't make an app. You give people more money. It's pretty straightforward, right? So Isabel is a heritage Spanish speaker, straight-A student, multilingual, just top of her class, and this algorithm predicted that she would fail her Spanish exam, which is absurd, right?&lt;/p&gt;
&lt;p dir="ltr"&gt;So algorithms in education or edtech in general-- it does not work as well as people imagine. There's enormous amounts of waste going on. We need to change our thinking around this. We need to do stuff like, we need to audit education algorithms. We need to change the purchasing methods at schools because schools are often locked into these vendor contracts, where they lean into using a particular technology, not because it works really well, but because they're stuck in a long-term software contract, which, of course, generates a lot of waste of public funds. We just-- we need change at every level, at the individual level, at the institutional level, and at the policy level. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Let's jump to accessibility and ableism. From your conversations with people with disabilities, you write that, "Today's tech is marvelously empowering until it isn't. Once you reach the outer limits of the tech's capacity, it becomes marginalizing.” And it sounds like you're encouraged by some of the progress being made in designing accessible systems, but there's still plenty of work to be done, right? &lt;/p&gt;
&lt;p dir="ltr"&gt;You explained a situation from a few years ago where you had a blind student sign up for a data visualization class you were teaching. And even the consultants at your school's disability services center were stumped by how to provide that learning experience, right? So where do we still struggle to enable everyone to use a product? And how do some people with disabilities become marginalized in this regard? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: I think you've characterized the argument that I make in that chapter well. Technology is terrific for increasing access and has been really good for increasing accessibility. And there is still more work to be done. So I am really grateful to the scholars and activists who shared their stories with me so that I could learn more about disability.&lt;/p&gt;
&lt;p dir="ltr"&gt;One of the really important things that I learned from my research is that there isn't a one-size-fits-all approach to disability. And so we need to listen to disabled people about what they need. And that's where we should start when we're designing. We should start with participatory design. So one of the things that I learned about was the concept of a disability dongle. And so this is something that a designer comes up with and thinks that it's going to be amazingly useful for people with disabilities, but it's really not, right? So a good example of this is a wheelchair that climb stairs. There have been a lot of these invented over the years. If you google "stair-climbing wheelchair," you get dozens and dozens of different images. And to a designer, sometimes it sounds like a great idea. But then, often, when the designers go out and present this to somebody who uses a wheelchair, the wheelchair user will say, yeah, I don't want that. That's not really what I need. That looks kind of scary, and it might attack me. Who knows? But really, what I want is I want more ramps, more ramps and more elevators. And it's really clarifying, right? Like, we don't need to overengineer solutions. We need to make sure there are ramps and elevators. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Now, for a lot of these problems we've discussed today, you've proposed a few ways to mitigate them. So I want to ask you about two of them. First, you write about the need for more public interest technology groups and initiatives. And you also are passionate about algorithmic auditing, which has yielded some positive progress in software development as well. Can you tell us about how these two areas can help combat some of these algorithmic harms that we see in our day-to-day lives? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: Yeah, I'm glad you asked about that, because I do end the book on a note of optimism. The book is not entirely a bummer.&lt;/p&gt;
&lt;p dir="ltr"&gt;And so public interest technology and algorithmic auditing are the places that they make me most hopeful right now. Public interest technology is exactly what it sounds like. It's about making technology in the public interest. So sometimes public interest technologists are algorithmic accountability reporters. They're the folks who are opening up black boxes and discovering problems. And other times, public interest technologists are working on government technology to make it better. So they're doing things like updating state employment websites so that when there is the next pandemic and millions of people are applying for unemployment benefits simultaneously, the site won't go down. So these are infrastructural improvements. The same way that we need to do infrastructure work on our roads and bridges and tunnels, we also need to update and maintain and continuously improve our digital systems, because digital systems are infrastructure.&lt;/p&gt;
&lt;p dir="ltr"&gt;And so algorithmic auditing can be considered a kind of public interest technology. You can do auditing from the inside or from the outside. You can do it internally or externally. So external audits-- I've mentioned a couple of times already the Lighthouse Reports investigation and the COMPAS Investigation are both examples of external audits, where folks went in. They did not have access to the inner workings of the system at the time of its creation, but they figured out later what was going on inside these systems. And an internal audit is something that you can do if you are inside a company, where you can evaluate your systems for bias. And then you can make any necessary changes. I mean, sometimes, you're going to have to throw the system out because it's impossible to update it. But if you discover that you are using an automated system that has bias in it-- which, PS, if you are using an automated system, it has bias in it. When you discover this bias, there are some mathematical methods that you can use to address it and remediate it.&lt;/p&gt;
&lt;p dir="ltr"&gt;So it's a hard conversation, though. People are sometimes reluctant to admit that these systems that they've invested millions of dollars in are flawed, right? So we just have to accept that these are going to be difficult conversations. We have to go into it with humility and with the understanding that, yeah, we made a thing, and it does not work the way that we expected, and we're going to have to do some remediation. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And you should want it to work for everybody, right? I mean, that only benefits the business, benefits society in the long term. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: Yeah. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: OK, Meredith, in one sentence, what does digital quality mean to you? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: Digital quality means creating systems that are inclusive, that are audited for bias, and are actually helping to make the world better. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: I like it. What will digital experiences look like five years from now? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: Five years from now, I think digital experiences are going to look largely the same as they do now. We're going to have different-looking gadgets, but they're going to be basically the same. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Meredith, what's your favorite app to use in your downtime? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: That's a good question. I am pretty proud of the way that I have my calendar set up. I have my whole family on the same calendaring platform, and we have a family calendar, and then everybody has their individual calendars. And then I have a calendar tool that I use for making appointments with people. And it's not super sophisticated, but it does make my life easier. So I'm really delighted that that exists. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: My calendar game could use an upgrade. I might have to follow up and get some advice from you.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: All right, we're going to talk about this later. [LAUGHTER]&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: All right, I like it. And lastly, Meredith, what's something that you are hopeful for? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: I am really hopeful for this administration's commitment to law and order in the digital realm. I'm really enthusiastic about the regulatory environment that says, let's enforce existing laws inside algorithmic systems. And that's going to mean things like people paying taxes and white-collar crime being prosecuted inside social media companies. And I think that's going to make things a lot better. &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Well, Meredith, this has been a really enlightening conversation. I just want to thank you not only for the work you're doing as an educator and as an author but for also taking the time to join us today. So thank you very much. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BROUSSARD&lt;/strong&gt;: Thanks so much for having me. It's really been a pleasure.&lt;/p&gt;</podcastTranscript><resourceImage><item>983869</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Watch Now</resourceButtonText><resourceButtonUrl>https://www.applause.com/resources/podcasts/ep-9-danger-of-bias-in-tech</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2023-06-21T13:09:02-04:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle>The Danger of Bias in Tech (Meredith Broussard)</seoTitle><siteNamePosition></siteNamePosition><seoDescription>Explore the impact of technology in society in this thought-provoking episode of the Ready, Test, Go. podcast featuring author and professor Meredith Broussard.</seoDescription><seoKeywords></seoKeywords><seoImage></seoImage><seoImageWidth></seoImageWidth><seoImageHeight></seoImageHeight><seoImageDescription>Meredith Broussard</seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><seoImage>true</seoImage><canonicalUrl>true</canonicalUrl></inherited><overrides><seoTitle>true</seoTitle><seoDescription>true</seoDescription><seoImageDescription>true</seoImageDescription><robots>true</robots></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets></sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromCustom</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds></seoImageIds><seoImageSource>fromAsset</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item><item><sectionId>58</sectionId><postDate>2023-05-04T07:00:00-04:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>370515</_authorId><id>968933</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>94b090e9-9af4-4b47-be20-731afffa14b5</uid><siteSettingsId>1445031</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>582797</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>Digital Quality Lessons from Star Wars</title><slug>ep-8-digital-quality-star-wars</slug><uri>resources/podcasts/ep-8-digital-quality-star-wars</uri><dateCreated>2023-05-03T15:30:59-04:00</dateCreated><dateUpdated>2023-06-07T14:30:49-04:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>968933</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep-8-digital-quality-star-wars</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep-8-digital-quality-star-wars</url><authorId>370515</authorId><typeId>96</typeId><description>Even if you live in a galaxy far, far away, many of the digital quality concerns that cause product releases to fail are relevant, as Adam Shostack discusses in this podcast.</description><publishDate>2023-05-04 11:00:00</publishDate><episodeNumber>8</episodeNumber><episodeLength>37</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>yd7ie3g1zu</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p dir="ltr"&gt;Even if you live in a galaxy far, far away, many of the cybersecurity and digital quality concerns that cause product releases to fail are relevant. Even the mighty Death Star is left vulnerable by one tiny flaw that should’ve been discovered and patched — and this is just the start of what we can learn about digital quality from the world’s favorite sci-fi universe.&lt;/p&gt;
&lt;p dir="ltr"&gt;Security consultant, expert and author Adam Shostack shines a light on the Star Wars series to reveal cautionary tales about software quality, cybersecurity, thermal exhaust ports and more.&lt;br /&gt;&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>Adam Shostack</podcastGuestName><podcastGuestPhoto><item>968947</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p dir="ltr"&gt;As President of Shostack + Associates, Adam Shostack delivers high-quality training and consulting in security engineering, including threat modeling and DevSecOps. Shostack also serves as a member of the Black Hat Review Board, RISCS Advisory Board and IriusRisk Advisory Board. He is also the author of three books, including &lt;em&gt;Threats: What Every Engineer Should Learn From Star Wars. &lt;/em&gt;&lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p&gt;&lt;em&gt;(This transcript has been edited for brevity.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DAVID CARTY&lt;/strong&gt;: For four decades now, the original Star Wars trilogy has delighted sci-fi fans all around the world, with so many great scenes, classic characters, and unforgettable moments. I mean, how about that classic twist that Darth Vader is actually Luke's--[BLEEP]. Oh, actually, we better leave that out just in case somebody's been busy over the last 42 years, but they plan to watch it in the next few months. We don't want to spoil it for them. Got it, great.&lt;/p&gt;
&lt;p&gt;Our guest today, Adam Shostack, has been a fan of the series ever since he saw the first installment, Episode IV--A New Hope, in theaters. The film resonated with him way back then, and he continues to take lessons out of it today.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ADAM SHOSTACK&lt;/strong&gt;: So you know, I was thrilled. I spent the summer insisting on getting the little Kenner toys and playing Star Wars with my friends it was pretty much the prototypical 1977 Star Wars experience.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: While many fans dig a bit deeper into the mythology as they get older, the magic Adam sees in the films is that they're essentially movies for kids. The story has simple themes, like good versus evil, the corruption of greed, or droids down on their luck. Adam tries to enjoy those simple pleasures of the Star Wars universe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: As a kid, it's hard not to love the climactic final battle. As an adult, the thing that I most appreciate about the movies is the world building, is the little touches that draw us in and believe we're looking through a window at this galaxy far, far away. You know, I don't know that, if you had told me that 40-plus years later, I'd still be talking about it I would even be able to conceptualize what that meant. It's clearly stuck with me.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: One thing Adam believes strongly--whatever your fandom, whether it's Star Wars, Star Trek, sports, or something else altogether, as long as it's helping you live your best life and it's not pushing anyone to the Dark Side, it's all fine by him.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: If you want to be a fan of whatever series you're going to be a fan of, I don't see any value in judging you, in describing it as healthy or unhealthy. If you're living a good life, and your fandom isn't getting in the way of that, why should we be any more judgmental about people who like Star Wars than people who like a sports team, or a band, or any of the other things that we see people get really excited? As fans, as enthusiasts, I think it's great.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: This is The Ready, Test, Go. podcast, brought to you by Applause. I’m David Carty.&lt;/p&gt;
&lt;p&gt;Today's guest is Star Wars fan and security engineering expert Adam Shostack. Adam is the president at Shostack + Associates, where he and his team help deliver high-quality training and consulting in security engineering, threat modeling, and DevSecOps. Adam serves on several advisory boards as well, including the Black Hat Review Board. He is also an author, and his latest book, called Threats: What Every Engineer Should Learn from Star Wars, came out earlier this year. Look, when it comes to security, much like digital quality, it takes preparation and a desire to uncover severe issues, whether you’re protecting a rebel base or just trying to support that new mobile app launch. Let’s learn more from Adam, who is the guest we're looking for.&lt;/p&gt;
&lt;p&gt;Everyone knows the importance of security, at least on a high level, but not everyone takes the necessary steps to build security in from the very beginning. And this is common with software testing, too, right? The notion that you can just add a pre-release step and everything will be fine, that's fiction, just like Star Wars. But before we get into Star Wars examples, tell me from a high-level perspective what a programmatic approach to security should look like in our modern day today.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: When we think about security, like other things that we can think of as quality, you can't bolt it on at the end. You can't sprinkle it on. You’ve got to design for testing, and you've got to design for security. You’ve got to think about what can go wrong as you're making choices about how to build things. You’ve got to think about that through the whole lifecycle of the software. Whether you're thinking about it in terms of Agile--what are we working on this sprint? Does it have security implications? Or, you're thinking about it as something much bigger and longer-term. I work with automotive companies, and they're on five-year build cycles. It doesn't make sense to try and impose an Agile lifecycle when you’ve got to source 100,000 chips and then program them. You’d better be thinking about, can those chips hold some nonvolatile memory which has a certificate in its you can do a digital signature check as you're loading software? If not, bolting that on at later stages becomes more difficult and more expensive. And similarly, if I design V1 of my API, I roll it out this sprint, and I'm like, well, we'll add security onto that later, and then teams within my organization--or even teams outside my organization--start taking a dependency on the current API, I've built myself a migration problem by not thinking through what I was building a little bit before I rolled it out. And I could avoid that quality problem. I could avoid all of the investment in having two infrastructures, multiple layers of service discovery. All of those sorts of things go away with a little bit of forethought.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: OK, now let's get down to brass tacks. Let’s talk about the thermal exhaust port. This 2-meters-wide security vulnerability takes down the entire Death Star. Now, there's obviously a lesson here in both digital quality and security. So when you think about it from that perspective, from that example, what do you take away from it, and what lessons can we learn?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: OK, so first, I’ve got to give a nod to a little video titled "The Death Star Engineer Speaks Out," in which he said, “Nobody told me there were space wizards who could make that shot. If somebody had told me that, I would have engineered it differently. “So at one level, it's a quality problem. At another level-- and it's a requirements quality problem. It’s not an implementation quality problem. At another level, we can think about it in terms of how the Empire handles failure. It’s a famous line. “You have failed me for the last time, “and then Darth Vader chokes you to death. If you have an environment in which people can't bring you bad news--"Hey, there might be a problem with this thermal exhaust port"--[MAKES CHOKING NOISES] then you’re not going to have quality. And so we've got--like I said, there's so much richness in the world-building. And we can take lessons about blameless cultures. We can take lessons about good requirements. We can take lessons about engagement with, maybe, the people building that thermal exhaust port were like, oh, should we add a couple of baffles here? Should we put a grate over this thing, maybe a pop-up--the right fix. As I said, I love fans. I love the enthusiasm that they bring, and we could spend an entire episode redesigning the thermal exhaust port. But the engineering reality, as shown in Star Wars, is they had multiple inhibitors to quality, deployment, and delivery that ended up killing billions of people, multiple years of massive investment, and really messing up the Empire's plan for all those planets to fall into line.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right, and if we're thinking about modern applications as Death Stars-- probably legacy apposing this case--with many different thermal exhaust ports opening over time, this is probably a complicated answer, but how can we best defend ourselves against intrusion or utter destruction?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: So the first thing is to fit the first thing as the first thing. Let’s think about the possibility of intrusion early. And when we think about that, we can think about the thermal exhaust port. We can also think about the interior of our systems. And we can use structures. We don't have to search our feelings and come up with these security issues because we were born with knowing about them. And the book is focused around two sets of structures. One is a mnemonic, STRIDE--Spoofing, Tampering, Repudiation, Info disclosure, Denial of service, and Expansion of authority. And we can use STRIDE to help us anticipate problems. And we can assemble those problems, those threats, into what are called kill chains--sequences of actions that people might take. So they'll deliver an exploit. The exploit will work. It will persist. It will talk to command and control. And we can think about each of those stages and what we as defenders can do to protect ourselves against them, detect them if they happen, and respond if they do. And can I go super geeky here?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Absolutely, it's a Star Wars episode.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: So here's the thing. I believe that when R2 plugs into the Death Star computer,R2 is actually connected to what's called a honeypot. It’s a system that's designed to isolate and observe what the attackers are doing. Because otherwise, how does R2 discover where the Princess is? That’s not information that should be exposed to everyone on the network. And so if we believe that R2 is connected to a honeypot, the Empire can observe that. They know that people are going to the detention bay. They understand that they have only so much time to put the tracking device on the Millennium Falcon, which is going to escape. And if it's not a honeypot, all of that makes even less sense.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Yeah, it's interesting to think about. And you get into so many different examples like this in the book. I do want to get back to the example that you mentioned before of the Empire handling bad news poorly in this command and control kind of culture. To me, it makes me think of the pathological culture of the Westrum model, where there's low cooperation, messengers are shot-- or Force-choked, whichever example you want to use--and people are made to be scapegoats. Now, I wouldn't expect Darth Vader to beat collaborative sort of leader. But can you tell us a little bit about why this type of power-oriented approach to leadership is problematic in a software development and delivery kind of context?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: The first thing I think about when I hear this is diversity and inclusivity. If we have a whole set of people that we're paying to do work, I would like them to bring their whole brains to the problems we put in front of them. And if we're scapegoating, we're blaming, we’re doing all of these negative behaviors, people withdraw. They deliver the minimum they feel they can deliver, and we're just not getting their best work. And so yeah, there's so many things wrong with it, and yeah, I don't know how deep you want to go. I don't have this--&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: It's fascinating to think about. I think a lot of us have probably worked jobs like that in our life, where you feel like you don’t have the support of leadership and, like you say, you withdraw. And it's not even necessarily a mental health issue so mochas it is a worker productivity issue. I mean, I think sometimes those points get obfuscated a little bit, but in order to get the most out of your people, you need to support your people, right?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: Yeah, absolutely, absolutely. And this-- I like to use the core four movies, but there's this wonderful scene in 8 where the Rebel Alliance is trapped on this planet. They’re in this cave and P--General Leia, excuse me, starts walking off, and everyone starts following her. And she turns around, and she's like, why are you all following me? Somebody else needs to pick up and lead here. And that leadership style of, hey, I want everyone to pick up a piece of getting things done, we’ve got a clear mission, we've got a goal, and we're all working towards it together--is really the thing that allows a small, scrappy Rebel Alliance to both win, but also, it harms them regularly because people are running off in every direction. They’re struggling to get everyone moving in the same way towards that goal. Everyone has their own opinion about how to get there. And so finding the right balance is complicated, not only in our world, but even in the world of Star Wars, which we were just talking about being simplistic. But this question of how do we get people to work effectively is complicated because it's complicated. We all have a different perspective on things, and balancing those and orienting people without being overbearing is a tricky subject.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Absolutely. That’s why there are entire sections of bookstores devoted to leadership books, right? But let's go back to your book. You have a chapter in the book on information disclosure and confidentiality, which I really found fascinating. Among other things, you explain how attackers can reconstruct cryptographic keys from the sound a CPU makes and other surprising methods of potential intrusion. What are some common ways you see businesses today err with sensitive data, and what does a safe data security posture even look like today?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: So the first thing to say here is I throw in some of these fun examples like the sound of CPU makes not because it’s the first thing that people should be thinking about, but because we should be thinking about the way--excuse me, we should be thinking about all of the side effects of the computation we do, but before we do that, we have to know where our sensitive data is, we have to know who's supposed to get to it, and we have to build our systems in a way that allows us to operate them knowing that some people will need that sensitive data, and knowing that we need to protect it. And so being able to think about how do we protect, how do we detect, how do we respond when there are problems, because we've thought about where this sensitive data is--if we're not doing that, our data ends up all over the place. It ends up in systems that don't have proper access control. For example, over the last 5, 10 years, we’ve seen just about every application in the world be refactored so that it no longer contains credit card numbers. We used to have credit card numbers scattered all over the place, and then attackers would break in, and they'd steal them, and everyone would have a new credit card and et cetera. And when it's credit cards, maybe that’s sort of acceptable. But when it's your Social Security numbers, when it's your medical records, when it’s sensitive information about your politics or your religion--which, in various parts of the world, can get you in trouble--having that information scattered throughout our systems creates a difficulty in protecting it. You've got to protect everything to the same extent.&lt;/p&gt;
&lt;p&gt;If you think about it up front-- if you say, wow, there’s an information disclosure threat, and because we've got people from all over the world working here, somebody brought this up early and said, wow, this is a problem--or maybe our lawyers brought it up and said, hey, GDPR means this data here is sensitive--we can think about that. We can think about putting it into one well-protected place and having everything else reference it, for example. We can think about, is this data that we want to sell? Do we want to let our advertisers see? For example, the FTC the Federal Trade Commission, has been fining companies that do medical-adjacent work because they were allowing Facebook to put Facebook tracking pixels onto their web pages, learn about patient diagnoses. If we thought about that and said, huh, we probably shouldn't have advertising on this page because of the sensitivity of it, we could manage that, and just a little bit of work asking what can go wrong up front can make a substantial difference.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Yeah, it's an interesting point. And I don't mean to project anything onto you here, but in reading the book, it sounds like you're troubled by the capabilities of mobile devices--or at least the capabilities of potential threat actors to exploit them. And there are a number of examples you bring up-- for example, how with mobile phones, it has the ability to read text off of a paper that might be way off in the distance. I mean, the kinds of things that the Rebels or the Empire might use against each other maybe with the help of droids, or something like that. So there are clear privacy and security concerns that manifests from mobile capabilities. Is there a way that mobile developers can program apps in an ethical way that helps protect their users? What would you recommend there?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: So the very first thing that I would recommend is be aware that the sensors in the device are way more powerful than anyone expects. But the second thing is, think about what you're doing, and think about the most--think about how the Empire might use this capability. Think about how a newspaper might report on it in a negative way and ask, do we want to do this? And we can get more specific, but I believe most engineers actually want to be ethical. Very few will raise their hand and say, the heck with all of it. I'm looking for the very most profit whatever the cost. Most people would be like, no, I would like to go home and look in the mirror and feel good about the work that I do. And so I feel like most developers are not doing the work that they do with the intent to be evil. They’re doing it with the intent to serve some customer base. And I do think that we need to start thinking more than we have about what that means. Generative AI is a really interesting example of this. Although I finished the book less than six months ago, and all of the things about ChatGPT and whatnot have happened since I finished it, as we roll these systems out into the world, I know the folks at OpenAI are working to think about the impacts of what they're doing. They have an entire team dedicated to the societal impact of this new thing they're building. If what you're building is really new, you can take a page from their book and say, what is the effect of releasing this? How will this impact the world? Can we build in safeguards? Maybe we get to the point of asking, should we build it? There’s a joke. Silicon Valley firm builds the evil doomsday machine from the book, For God's Sakes, Whatever You Do, Don’t Build the Evil Doomsday Machine. And they issue a press release announcing how excited they are to have fulfilled the vision of the book.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Yeah, that's interesting, you know? And Bluetooth is another kind of example of this. And you wrote about, in the book, how even a person's gestures and typing can be measured from the data that's being transmitted over Bluetooth. Perhaps the Death Star blueprints might have been intercepted via Bluetooth signal. I’m not sure. But Bluetooth, obviously, is--it's, obviously, become a standard for connected device manufacturers. But there's something troublesome there, too. So where should IoT device manufacturers go from here? What is a safer way to move forward?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: Yeah, well, it's a huge question. And there are so many emerging standards for connected device security that, again, thinking about it in terms of what is our device designed to do is a really important step. There was a thing that came out which is, one of these smart home devices had a microphone in it. And there's literally no functionality in V1 that uses a microphone, but it costs like a nickel to add, and they said, yeah, maybe we can add voice control later. And so they put a microphone in there with no control over, I’d like to turn the microphone off, or, please indicate that the microphone is listening. And so one layer of this is to consider these things as you're building these devices.&lt;/p&gt;
&lt;p&gt;But there's another important layer, which is, what are the standards committees doing? So let's say I have an Acme Anvil-Dropping IoT Machine, and it's phone-controllable. We have to have the standards makers asking, what can go wrong with this machine? And the fact that it drops anvils on people, we’re going to leave that aside. But, how do I authorize? How does it log? What information does it send where? These threats are things that we can think about at the protocol design time. And like I was talking about earlier, where, if I ship my V1 without security in it, it’s very hard to build that in later-- to get quality on later. And so we've got to be looking to standards bodies to incorporate more threat modeling, more clearly stated threats, so that we can expect the devices that we use to be done better, rather than saying, oh, it's the app developers' fault. They couldn't have done anything because the standard doesn’t support what we need. But we're going to look to the app developer and call them unethical, doesn't strike me as fair, and doesn't strike me as moving to the folks who are best positioned to really solve these problems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: You know, I've got my fingers crossed that for Father's Day I get the Acme Anvil Dropper. So I'm hoping the family watches this episode of the podcast. So, fingers crossed on that.&lt;/p&gt;
&lt;p&gt;But you know, Adam, you mentioned it’s hard to parse data structures reliably. And there's a great line in your book that I think helps get this point across about data being tainted. Quote, "Just like you can never quite get the smell of tauntaun out of your clothes, you can never quite get data to be perfectly clean," end quote. Aside from the vivid smell this conjures up--I can smell it just talking about it--what sorts of checks and balances should be in place to make sure that we're helping to sufficiently sanitize data?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: So thank you, I was really proud of that line.&lt;/p&gt;
&lt;p&gt;So there's a few relatively easy things to check for in the simple case--Checking for length, checking for expected character set, checking for semantic quality. I’ve been playing a lot with AI image generators lately, and I wrote a little code to work with one that uses a REST API. And so I call this REST API, and then it gives me back a URL.I didn't want to manually copy and paste the URL, so I did a thing to feed it to the Mac OS Open command. And now I'm taking URLs from this AI tool, and I'm feeding them into the system-level API on my Mac. So what I did was I said it's got to be less than 120 characters. It has to match A to Z, 0 through 9, dash, colon, slash, and it's got to start with http://deepai,blah, blah, blah. And so what I'm doing is I'm saying--and I think the most important of these is the character set. Saying that I have only these characters are allowable means that if I get anything weird, it'll get rejected, and it'll throw an error message. The nice thing about simple things like URLs--and URLs are not simple, by the way--but the nice thing about the relatively simple URLs that this gives me is I can apply those things. I’m applying business context rules that describe what I expect to have happen, and I constrain that to a method called checkURL that checks that the URL meets my expectations the way it’s used further down in the code. The more we check that the thing is what we expect, the less we're surprised later on. And this can get really complicated. If the thing I expect is an MKV video container, that’s a really complex container format with layers of containers inside of it that eventually contain video frame diffs. I don't know what date--I don't know how you validate that except to put the thing into some sort of sandbox that prevents it from doing weird stuff. So you can write some code. You can use a Mac OS sandboxd.You can use Linux AppArmor to say, hey, this process should never write to disk. It should read from disk, it should write to the monitor, and that's all it should ever do. And if we do that, so we're constraining the behavior of code, we don't necessarily need to anticipate all of the weird ways in which people can go wrong because we're focusing on the thing that we can model.&lt;/p&gt;
&lt;p&gt;And one of the choices I had to make in writing the book was, how much does every engineer need to know about writing software exploits? Software exploits are complicated. They’re a dark art. There's a lot of learning that needs to go into being able to do that. And I'm pretty pleased with the fact that I found some simpler, more actionable things like checking the business context of what you're accepting, checking the behavior of the app, that I think are more actionable for a normal developer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Adam, if our listeners watch the original Star Wars trilogy this weekend, what would you recommend that they look for or make note of that might be helpful for them to take back to their jobs?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: So the most important thing want them to catch is the Post-It notes with passwords that R2D2 takes advantage of. Every time he's plugging in, you can see there's a Post-It note, there’s a password.&lt;/p&gt;
&lt;p&gt;No, more seriously, enjoy the movies and recognize how they can teach us so much. We can use them as analogies for our day-to-day job, and they really do work.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And if you really pay attention, maybe you can even write a book about it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: Indeed, indeed. There's room for more books.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: All right, Adam. Final sprint questions here. In one sentence, what does digital quality mean to you?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: A lack of surprises--the thing does what we expect it to do.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: I like that one. That’s a great way of looking at it. What will digital experiences look like five years from now?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: I don't think they’ll look like the metaverse. I think that we're going to see an explosion of AI-generated content that's going to make it very hard to know what is real. And I think that that's a huge danger that we're barely beginning to grapple with.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Very interesting. What's your favorite app to use in your downtime?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: Can I be aspirational and say Duolingo, or should I own up and it's Plants vs Zombies?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Well, it's good to have a couple apps going at the same time, right?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: That's right, that's right.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And Adam, what is something that you are hopeful for?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHOSTACK&lt;/strong&gt;: The next season of Andor. I really think that Andor has been one of the best things done in the Star Wars universe in a long time, and I'm really excited about the slower-paced storytelling, the politics, the personalities that are coming out, and the expanding the Star Wars universe beyond some of the core characters to new characters gives the folks a chance to play with new styles of storytelling. And both as a fan and as someone who thinks about, how do we use storytelling to make the sorts of points I’m making in the book, I've just got so much respect for the way that Andor has been put together, which is simultaneously true to the core stories, and new and different, and a little bit more adult. And so I'm really looking forward to the next season.&lt;/p&gt;</podcastTranscript><resourceImage><item>968951</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Watch Now</resourceButtonText><resourceButtonUrl>https://www.applause.com/resources/podcasts/ep-8-digital-quality-star-wars</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2023-05-10T14:39:06-04:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle>Digital Quality Lessons from Star Wars</seoTitle><siteNamePosition></siteNamePosition><seoDescription>Security expert and author Adam Shostack joins the Ready, Test, Go. podcast to talk about threats in a galaxy far, far away — and here too.</seoDescription><seoKeywords></seoKeywords><seoImage></seoImage><seoImageWidth></seoImageWidth><seoImageHeight></seoImageHeight><seoImageDescription>Ready, Test, Go. podcast with guest Adam Shostack</seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><seoImage>true</seoImage><canonicalUrl>true</canonicalUrl></inherited><overrides><seoTitle>true</seoTitle><seoDescription>true</seoDescription><seoImageDescription>true</seoImageDescription><robots>true</robots></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets></sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromCustom</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds></seoImageIds><seoImageSource>fromAsset</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item><item><sectionId>58</sectionId><postDate>2023-04-19T07:00:00-04:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>370515</_authorId><id>965429</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>b80cac5f-049c-47f5-955a-176799401ebb</uid><siteSettingsId>1440174</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>579633</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>Getting Real Value Out of Your Testing</title><slug>ep-7-real-value-out-of-testing</slug><uri>resources/podcasts/ep-7-real-value-out-of-testing</uri><dateCreated>2023-04-18T16:12:54-04:00</dateCreated><dateUpdated>2023-04-18T16:32:57-04:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>965429</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep-7-real-value-out-of-testing</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep-7-real-value-out-of-testing</url><authorId>370515</authorId><typeId>96</typeId><description>As companies look to maintain lean budgets, Matt Heusser explains why it takes full organizational commitment and a clever approach to extract value from all QA resources.</description><publishDate>2023-04-19 11:00:00</publishDate><episodeNumber>7</episodeNumber><episodeLength>35</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>qf6t19zt62</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p dir="ltr"&gt;As companies look to maintain lean budgets or even cut them, how can organizations make sense of their digital quality investments, and optimize them moving forward? It takes full organizational commitment and a clever approach to take advantage of all resources — internal and external to the business.&lt;/p&gt;
&lt;p dir="ltr"&gt;In this episode of the Ready, Test, Go. podcast, software development and testing expert Matt Heusser of Excelon Development discusses the importance of efficient testing that delivers value. Hear from Heusser how flexible resources like crowdtesting can help fill gaps in testing coverage, and whether he sees a tide change in how leaders are approaching digital quality.&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>Matt Heusser</podcastGuestName><podcastGuestPhoto><item>948647</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p&gt;Matt Heusser is the Managing Director of Excelon Development. Heusser is an award-winning thought leader and speaker. He also writes for many tech publications and authored the Lean Software Testing/Lean Software Delivery methods and courses.&lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p dir="ltr"&gt;&lt;em&gt;(This transcript has been edited for brevity.)&lt;/em&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;DAVID CARTY&lt;/strong&gt;: What is your lot in life? You probably want more than you have, right? Better career, bigger house, faster car, more coffee. That's human nature. But stoics, like Matt Heusser, believe it's helpful to take a step back and appreciate what you have.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;MATT HEUSSER&lt;/strong&gt;: At one point in my life, I was having a difficult time, and someone recommended that I read Viktor Frankl's Man's Search for Meaning. And Viktor Frankl was a — he went through the Holocaust and World War II. He was in a concentration camp. And they limited his meals to something like 200 calories a day. He'd basically get broth with, if he was lucky, there'd be a few peas in the bottom. And then, what he had to do was he had to build gravel roads by hand with picks, ax, wheelbarrows, shovels, that kind of thing, every day. And he found pleasure in things, like a sunset or, maybe if he was lucky, he'd get a cigarette a week, something like that. But I thought my problems are not anywhere near this bad.&lt;/p&gt;
&lt;p dir="ltr"&gt;Then, I read not Admiral Stockdale's Thoughts of a Philosophical Fighter Pilot. He was a POW during the Vietnam conflict. During those 36 months, I think, he was in solitary confinement for 18 and leg irons for nine months. He was tortured, I don't know, dozens of times. And it's very similar, where focus on what you can control and don't worry about what you can't.&lt;/p&gt;
&lt;p dir="ltr"&gt;So this fundamental stoic idea of separating what you can control from the externals and then not worrying about your externals is really helpful to me.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Stoicism is a philosophical concept that dates back to ancient Greece. While the practice has changed over time through the Renaissance and into today, it's four primary virtues are wisdom, courage, temperance, and justice. The idea is, essentially, that the practice of virtue is all that is required to achieve happiness.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: I don’t mean to be morbid, but what if I didn't have a roof? What if my furnace broke? What if there was a bank run as has happened recently? And the companies that I invested in all went bankrupt, and I'd really be in trouble. House burns down. And it makes you appreciate what you have. What if, we didn't have — this is happening in parts of the country right now. They don't have access to clean water. Right now, I could just turn a sink on, a faucet, and I can get clean, cool, refreshing, healthy water available to drink on tap. What if I didn't have access to that? So by actually sitting in, then imagining the pain of the loss, then we come back. We appreciate everything all the more.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Stoicism also comes in handy as a parent. Matt has discovered that his young daughter can be quite convincing in getting what she wants, such as extra screen time. Those stoic practices of perseverance and behavior over belief can be really useful, whether you're dealing with a difficult coworker, or a defiant child, or a difficult coworker, who is acting like a defiant child.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: My challenge in the moment is in every single time, consistently, recognizing right. I'm called the parent here. This isn't my peer. This isn't an adult. This isn't someone I negotiate with. We have boundaries, and I need to consistently enforce them.&lt;/p&gt;
&lt;p dir="ltr"&gt;I think the same things happen in the workplace. There are personalities that will do the exact same thing in the workplace, and it's the same lesson. If you don't nip the behavior in the bud, you're going to see more of it.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: This is the Ready, Test, Go. podcast brought to you by Applause. I'm David Carty.&lt;/p&gt;
&lt;p dir="ltr"&gt;Today's guest is stoic father and software development expert, Matt Heusser. Matt is the managing director at Excelon Development. He's an award-winning thought leader, author, and consultant in the software development and testing space. Just because you test your software, that doesn't mean that you test it thoroughly. And even if you test your software thoroughly, that doesn't mean that you test it efficiently. In today's economy, every testing dollar must deliver value. But how do you go about measuring that value? Not just at a point in time, but as an ongoing, evolving program. Well, there's no easy answer to that question, but there is a Matt Heusser.&lt;/p&gt;
&lt;p dir="ltr"&gt;Let's start off our discussion by explaining why there's nuance to this topic in the first place. In our previous call, you said that people like an easy answer. Why is this a problematic idea, when it comes to test coverage? Everyone loves an easy answer, right?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: Sure. So what we do is, there's an infinite number of combinations of possible things in the whole universe that we could test. I mean, just a mobile app. Well, what operating system is it on? What's the form factor? Is it is it portrait or landscape? And then, that's just the combination of how much bandwidth do we have. Then, that's just a combination of the possibilities of the platform. And then, you have all the different — what if it's a calculator app? What if I do 1 plus 1? What if I do 1 plus 2? What if I do — just adding two numbers together, there's an infinite number of combinations. The first time you do it, there's a memory leak that could be the second time. So the coverage space is infinite.&lt;/p&gt;
&lt;p dir="ltr"&gt;What we want to do is, we want to narrow the complexities down, simplify it to something that is valuable but understandable. Maybe we have a tool to do that. And when you say oversimplify, that's one set of test ideas that we can run in an hour, if it is our smoke test, and then we say the software is good. And that's our coverage model. Or we measure the goodness of the software by counting the number of bugs. Or we say, testers find bugs good. Developers have to fix bugs bad, and we actually create an active conflict between the two, where the developers have an incentive to have no bugs. The testers have incentive to find a whole bunch. All of those are very, very simplified, easy to understand models for software testing. They're incredibly broken. They actually drive the wrong pages.&lt;/p&gt;
&lt;p dir="ltr"&gt;So Occam's razor says the simplest solution is usually the correct one, but there's another. I think it's HL Mencken. He says, for any problem there is a solution that is simple, that is intuitive, that makes sense, that is easy to apply, and is wrong. So our work is creating ever more complex models that our challenge is to do it in a way that don't become too complex to find that sweet spot in between no nuance, simple, everybody can understand it, but actually creates bad behavior and really, really complex. Nobody can understand, it doesn't make any sense. Comprehensive, but it's incomprehensible.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. And part of the challenge here and part of the nuance to digital quality is, we have thousands of devices, operating systems, as you mentioned, right? This is why it's important to consider the value you get from your different types of testing. You work with clients in a variety of industries. You're tapped into how they think and their action items. How good a job do organizations do generally when it comes to assessing the value of their testing and adapting to whatever those findings are?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: Oh, my goodness. So there's the state of the practice, and there's the state of the art. And those are two different things. So I'm almost embarrassed. The answer is so bad that I'm embarrassed to talk about it. One of my colleagues used to say, when it comes to actual time on task versus time waiting in queues for software work, the number is so low. It's hard to show the customer because it's hard for them to even believe it.&lt;/p&gt;
&lt;p dir="ltr"&gt;In terms of testing, most organizations do testing as a thing that has to happen so the software can go out. Thus, it should be done really, really quickly. In terms of assessing the value, that's not really a thing they do. It's just a blocker for release.&lt;/p&gt;
&lt;p dir="ltr"&gt;So a few years ago, I was talking to someone at a Wall Street financial trading company, and he had destroyed the entire QA department and replaced it with some outsourced group. And all they were really doing was the simplest of user interface usability. Type in the number, click, submit, see it shows up testing, like almost nothing. All the executives cared about was that he had reduced test cost by 93%. Because they had no idea of how to assess test value. All they had was test cost, make that number small. It reminds me of a company I worked with a couple of years ago that developed software for their mobile app that was AI ML. It's going to measure how you use the software, and then magically make predictions, and automatically implement those predictions in their internet of things products. Never worked. They spent, I don't know, $2-$3 million, 18-month project, very large team, contractors, very expensive. By the time I came in, they brought me into new project rescue. I filled out the Jira ticket to tick the checkbox out of user interface, so you just couldn't turn it on, which just disappeared. Never worked. I went to him and I said, hey, if you want software that doesn't work, that you never use, developed in 18 months, I'll do it for a half a million.&lt;/p&gt;
&lt;p dir="ltr"&gt;And the reason I tell those stories is that they don't have a real ability to assess on the value of testing. So all that's left is cost. And that's not universal, but I would really interested the people that — the people that watch this podcast, if they want to comment, how does your company evaluate test? And I bet you, I would expect the majority of the answers are overly simplistic model that drives the wrong behaviors, or we don't do it at all.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Yeah, I mean, that's really interesting, Matt. And just to think about this a different way. How do we go about fixing this? I mean, I know that's not going to be a one-size-fits-all approach. It's hard to enforce change, let alone an adaptive approach to digital quality, where you're rolling in that information you get from the feedback loop on your testing value. So how can organizations make themselves more open to a flexible approach that is considering that testing value and optimizing for it?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: I'm a fan of a book called Getting Things Done, personal process, personal project tracking. Not really from make-the-checklists and fill-out-the-forms perspective, but just figure out all that you're doing, put it in a box. Look at its effectiveness. Sort it by the things that are important and urgent, then important but not urgent, then not important and urgent, not important not urgent. And chuck stuff out of the box that isn't valuable. Focus on the things that's it.&lt;/p&gt;
&lt;p dir="ltr"&gt;So how would we do that in testing? Where I would — where do you start? Let's look at all of our testing activities. We do usability, accessibility, performance, load, unit, system, integration, outsourced. I'm working with a — we do some crowdsource. We do this. We do the — how do you possibly compare all them? They go in different buckets. They have different managers. They have different budgets. Where do you start?&lt;/p&gt;
&lt;p dir="ltr"&gt;I would start with the bugs, right? So to go into our bug tracking system or whatever it is. Maybe you're a high functioning Agile team, and you fix up the day you find them by. Track them for two weeks, man. It's two weeks. Get a list, say, where was the defect created, where was it found? Is that right? And you can sort that spreadsheet by, where was it found? You can find what's — where was it found? Where should it have been found, right? Often, this problem should have been found in design. Any time we have a tester arguing with a developer about what it should do, it's a design or requirements problem because we're clear on what it should do from the beginning. We get that list and then we can start to look at the priorities and effectiveness of our test efforts. So we can say, this tough effort is not valuable. None of the defects that are emerging are the kind that this thing should be checking.&lt;/p&gt;
&lt;p dir="ltr"&gt;So one company I worked with, we would do all of our work. This is a few years ago. We did all of our work under HTTP, and then we did all of it again under HTTPS, so in a secured environment. If we found one HTTPS bug, then the whole thing didn't work. So I said, what if we did all of our testing here and then spent 15 minutes there, called it good? And that was best based on the defect data. That's just based on the way defects show up in the world. So based on that, you can reprioritize your testing.&lt;/p&gt;
&lt;p dir="ltr"&gt;Now, we just did a paper together with Applause, where we talked about — and I had mixed success with send everyone a survey and ask them what they think we're actually doing and what we are doing, average the numbers, and then look at them. And you can also look at the distributions. So you can find out there's a radical expectation difference in how much time people think we're spending on unit testing, or everybody agrees we should be doing more of these and less of that. Those aren't firm numbers. It's an overly simplistic model. It doesn't really have any meaning. But when you aggregate it, when you put it over enough people, and you look at it, you can make better decisions about how to invest your time, which I think goes back to the vertical. How do we invest our time and limited resources to find the most important bugs early? I think that's what we want to do. Know your mission, then adapt your investments to your mission. Most of the teams I work with at scarce resources, they want to find the most valuable bugs.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And this gets into the idea. You say that we should think about testing as risk management. This gets into that space a little bit. Why is that? And how that guide organizations in their decision-making process, particularly as it applies to prioritizing tests and getting the most value out of those efforts?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: Yeah. So if you look at how many organizations do testing, what we do is, we say, we're doing stories or micro features, whatever you want to call. And then for every story or micro feature, we assign someone to do the testing, and that person does whatever they do. And then, it's tested. And then, maybe, at the end, we have some regression process that goes through and makes sure that they change this. It didn't break something over there, unintended. It's all tested about the same. All the testing is about this wide. I'm going to talk about some numbers, as if it were possible for us to model, that it's very difficult to do. What these numbers are going to be approximations. It's going to feel like you're loaded on the back of a napkin in a bar over a beer you'll be embarrassed, but it's going to be better than nothing.&lt;/p&gt;
&lt;p dir="ltr"&gt;So with a risk-managed approach, you look at the impact if the problem happened, percentage, chance that's going to be introduced into the world, multiply those two numbers with all of your senses of risks, and then you sort. And what will happen is, the most likely problems with the highest impact will go to the top. And then, you can invest your scarce resources and go on working down that list. So at least, the bottom might get a lot less attention than the things at the top. What that would mean for testing would be, let's look at these stories. In e-commerce, that's usually half the purchase. So search has got a worker that can't find the product. Add to cart has got to work. Check out has — these things are really — if these things don't work, create a profile. You can't buy product, every minute, you lose. Product reviews, not really in the same category. So if the impact is higher — and maybe we're talking about APIs that are consumed and used more commonly all over the site that are going to be reused, a lot of sites now have widgets. And they're actually composed out of widgets. So these APIs might be used in a bunch of different places, which increases the impact if something goes wrong and increases the impact that some developer accidentally uses it wrong. Or maybe there's a change in the version control, or it impacts the API, impacts the consumers of that API. So those things should get more attention.&lt;/p&gt;
&lt;p dir="ltr"&gt;But if we just always do what we always did, everything gets about this much testing. And then, we're missing the opportunity to reprioritize to catch the defects that have a disproportionate impact on our customer base.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. A bug here is more severe than a bug here, basically.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: Yeah. And look, again, I don't see — it's very common not see that prioritizing happen. Why is that? One, there's no model. Nobody's doing the work. And two, what testers do — and this is a real area of conflict — is we don't really know what tester's been doing. They're just doing whatever it is they do. It's just a thing you have to do before you get to production. And then if there is a defect, most of the time, there's someone else to point to, to take responsibility. Like, oh, whoopsie, or the requirements were unclear, or, well, everyone made the same mistake. The requirements were unclear, and the developer made the mistake, and I made a — if we hadn't all made the mistake, it wouldn't happen. Or this is a regression problem. It works just fine when I tested it, but it's other things — there's unintended consequences. There's a strange combination thing.&lt;/p&gt;
&lt;p dir="ltr"&gt;So there's not really a strong incentive to take a responsibility approach, model the whole system. It's more, I tested my story. My story worked. And then, what did I do to test it? I used my own judgment to come up with a test plan. That's probably the status, I would dare say, that that's the status quo for most developing nations. And it's even worse for legacy systems still have traditional test cases because it's — I just did what someone else who has left the company told me to do. The one thing we with a new build is that something has changed. That's a definite. We wouldn't need to test it. We just took the same executable artifact and dropped it on the website or dropped it on your desktop. The one thing we know is something has changed. The test case approach we say, something has changed. Let's inspect it the exact same way we did before. Because that's what you do when you have batteries on an assembly line. You run a million batteries a day. You test because they're all exactly the same. But the software assembly line, the continuous integration system, your Jenkins, or whatever it is that you're using, every build is different by definition because that's what it does. If it wasn't different, it wouldn't need a new build.&lt;/p&gt;
&lt;p dir="ltr"&gt;So that's why I think we need to actually have the capability to customize our test runs between build. And this is what the humans do to check it, and then there is this small percentage of that, that we institutionalize this tooling and automation that might run.&lt;/p&gt;
&lt;p dir="ltr"&gt;And so, then how much of that tooling should change for this new build so that we can check it correctly? Ideally, in many cases, if we have coverage, we could make our change, run our tests, see it fail. That's good. It should because an automated tooling is just an expectation for yesterday's behavior. Change it so that now it passes, reroute it, see it pass, and nothing else fails. And we have confidence system worked. Now, I just don't see people using test tools that way today.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Even today, it's surprising to hear. Because you would think, today, we live in uncertain economic times. A lot of businesses are trying to operate in a more lean way, or they're cutting back, and that can be really painful. So are you seeing any kind of tide shift there in terms of placing extra emphasis on delivering test value and optimizing for test value, or is this still a really nascent idea that hasn't taken hold yet?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: I don't mean to be overly critical. What I've seen in the past 10 years is, we've collapsed testing inside the team, so the testers are embedded members of the team. And then, we've often — the Silicon Valley standard now is to collapse testing into development work. So the there's benefits. To test your developer is the same person, I'm going to build it, and I'm going to check it. And it looks like it's good. And then send it, do something with it.&lt;/p&gt;
&lt;p dir="ltr"&gt;But how many of those people have read a book on testing? How many of those people have done anything other than, if we're lucky, they went to Selenium or some tool vendor's site and learned enough automation to be able to clickity click, clickity click to make sure it work. And when that happens, most of the time, they'll get stuck because there's some strange, weird user interface, for instance, very difficult to tool up in Puppeteer, or Playwright, or Selenium, or whatever you're using. It's awkward. It's weird, clicking the buttons in the right order, setting up the data. For instance, very common pattern is, set everything up. Click a button. See the order ID. Click the radio button for the order ID. Click Submit. How do you know what the order ID is? It's not deterministic. It's going to be some new number. Maybe if we grab is a nine digit number that is just one of the top of this — programmers write these workarounds when they're writing tooling. But a lot of the time, if nobody's looking over their shoulder, when they get to the really complex, hard to set up stuff, you just skip right in that test. And then, where's the bug going to be? It's going to be a really hard, complex to understand stuff.&lt;/p&gt;
&lt;p dir="ltr"&gt;So when we'll find a defect, who knows, three months, six months later, you might not even be on the team anymore. There's not a strong incentive for you to make it check. In fact, if you do the work to check it, it's more work. So there's a negative, immediate, certain incentive for you that is more work for you to set that up.&lt;/p&gt;
&lt;p dir="ltr"&gt;So to examine the systems that we're creating, the good news is, technology is giant business in the United States. IT, tech, software development, software testing, these have been growing at double digit rates every year for 20 years. The interesting thing about growing at greater than 10%, by definition, is that you're going to double every five years, every five or six years, which means, at any given point in time, half the people working in the industry have less than five or six years of experience. So we're relearning over and over again. The things that I've described to you are rookie mistakes. Yeah, there are companies that do better. But if they're growing company, and they're hiring new graduates, we're going to have to keep relearning these things over and over again.&lt;/p&gt;
&lt;p dir="ltr"&gt;I hope I haven't presented a scenario, where it's all doom and gloom. But what I think I'm trying to say is that, if you're looking for craft and excellence in software testing, you're swimming upstream. You've got in your canoe, and you're rowing, and the water is pushing you the other way. And it's going to be work, but — my family is from the Pacific Northwest. Salmon, that's how they survived, man. A few years ago, I did an analysis, and I talked to him about it. And he found something like, IT is growing at 12% annually. A growth rate of IT and software development, 12% to 15%, while testing was growing 3, 4, 5. It depends on how you measure. It depends on whether you count inflation. But what that meant is, there's always more testing. There's always more testers. But development is growing faster, so it feels like testing is shrinking, but we're really, really not. And I would say that to the extent that testing is — I pick out the test case, and I do the thing that someone told me to do a year ago, who's no longer with the company, that might or might not be relevant, yeah, I am OK with that role-shrinking.&lt;/p&gt;
&lt;p dir="ltr"&gt;So then, when we think of a wider view of testing as risk management to include performance usability, accessibility, what the developers do to improve quality earlier in the process, working on quality, testing the requirements at the beginning, and what roles could do that, well, we've also got another problem in that we don't want to just do this much testing. We want to do a lot of testing on this, a little bit on that, and a lot on that, a little bit on that. So we want to think of a portfolio of resources to do that work, and that portfolio of resources can include contractors, which is a lot of what Excelon does. They can include crowdsourcing, which is a lot of what Applause does, and it can include full-timers, see if we could transfer people over. So it's less common than it used to be, but a lot of companies, during crunch time, would transfer people from customer service and other subject matter expert roles to do — the developers can do a lot of this really neat. We're going to have this 18-second test for login. We're going to have this four-second test for add to cart. We're going to have this 11-second test for checkout. We're going to have a bunch of them. We're going to randomize the data, and we're going to run them for overnight in a random order, with random data, logging the data, and seeing if something breaks or defects that are input, transformation output, or in.&lt;/p&gt;
&lt;p dir="ltr"&gt;But when it comes to this screen looks wrong, that took a long time, we set up a very, very complex telecommunications. We set up a very, very complex bill of materials, where we have one account, where it has 15 different locations, and there's 200 phones. And there's a whole bunch of phone calls, and there's some text messages. And we're going to generate that. There's the rounding taxes. And we're going to generate — we want to make sure that the rounding worked correctly. You're going to want other resources to do that. Maybe you could institute. But just pure developers, programmers, unless you find one who's really interested in that, they're probably not going to think that way. That's not, we're going to test this really big because this telecom bill is a huge part of our cash, and it could be legal obligation if we get it wrong.&lt;/p&gt;
&lt;p dir="ltr"&gt;So where could those resources come from? It's a very similar exercise to what we talked about before. We write down all of our possible resources, write down all of our different possible kinds of doing testing, and then we try to figure out where our alignment is. So having crowdsourcing as an option and knowing how fast can we spool up people, what can they do, what are their limits, we're not going to have any subject matter expert. We're going to get a whole bunch of UI specialists, so we can do a lot of platform testing, localization testing, internationalization testing, bandwidth testing to know what is out there, to know how much it costs, and then on how much notice. 24 hours, 48, over the weekend. We can put it on a credit card and could get results on Monday like magic. I believe that incredibly rapid testing with clear results spool up on demand sufficiently advanced, it's indistinguishable from test tool. It doesn't matter. I've got a button I can push to get results. Do I care if there's 500 people all over the North America doing it for beer and pizza money? Or do I care if it's running in the cloud, and I'm paying by CPU hour. What is the quality of that result? How fast can I get results back? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Matt, in one sentence, what does digital quality mean to you?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: I expect my software to work the way I expect it to work the first time.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Simple enough. If only it was that easy to implement, right?&lt;/p&gt;
&lt;p dir="ltr"&gt;What will digital experiences look like, five years from now?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: Gosh, don't we all wish we had a crystal ball?&lt;/p&gt;
&lt;p dir="ltr"&gt;Quickly, I would say that there's a chance that the Oculus virtual reality takes off more, especially for gaming. We're going to see — we're going to see digital experiences. Your automobile is going to continue to become more integrated. Your automobile is going to feel more and more like a laptop computer. And the handheld device is going to continue to integrate all of the things. So we're going to see more internet of things. We're going to see more Ring doorbells that operate with your phone, home security systems that operate with your phone. 20 years ago, we were talking about turning the lights on and off in your home with your phone. For the people that want to use Alexa, the ability to integrate the home experience, the home electrical experience and see more of it.&lt;/p&gt;
&lt;p dir="ltr"&gt;And we're going to have some laggards, where things don't change a whole lot. But that's what I see. I see people continuing their mobile first now. So I see more and more software is going to be done on your phone instead of the laptop. So if the world continues as it goes, we'll see more of that.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Matt, what is your favorite app to use in your downtime?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: Right now, if you were to ask my computer what I'm spending on, I'd say Reddit. And that's really a communal oriented group, where you can find people that are interested in the same things you are, and you can talk to them and share ideas.&lt;/p&gt;
&lt;p dir="ltr"&gt;I am also on Facebook a fair bit in the meal ideas groups, which is a little crafty a corner of the internet where you can experiment and get better and better at something. It excites me.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And finally, Matt, what is something that you are hopeful for? &lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;HEUSSER&lt;/strong&gt;: One thing I'm hopeful for is, we have these ideas, like this book No Asshole Rule, and Boundaries By Townsend and Cloud, the idea that we can make and keep commitments to each other, so we know where things are.&lt;/p&gt;
&lt;p dir="ltr"&gt;And on top of that, I would layer, so much of our thinking is zero sum. I want a contract, right? I'm going to make these numbers up. They're not right. If I pay someone $20 an hour, and I fill out at 25. I make a $5 profit. If I pay them more, I make less. And if I charge my customer more, they make less. So, ultimately, at some level, there's some amount of zero sum going on. But usually, I find, can I offer you flexible hours? What if people are working remote? What if we gave a small annual bonus for employees, so they could just get out of dodge, go to a vacation spot on Saturday, have Sunday with your family. Work through the week, but be done at 5:00 PM. Enjoy that, whether it's family or whatever it is, and then come back on Saturday. So you still work in the whole week, but you get to experience a retreat or a corporate retreat. What if we just covered your travel? How can we — and then that that's tax deductible now, as opposed to you covering your own travel, which pay with after tax dollars.&lt;/p&gt;
&lt;p dir="ltr"&gt;What can we do to find ways to make the work more palatable and better for everybody? What's a win-win outcome? And how can we honestly negotiate with each other in a problem solving way? This is a problem to be solved. How do we get this software done instead of a blame or responsibility oriented way where there's finger pointing. And part of what helped me go independent was that allowed me to change the position so that I could say, this engagement isn't working out for me. I'm done. And as an employee, it's very, very difficult to say, I don't accept an assignment where I can't be successful. I can't be successful here. &lt;br /&gt;&lt;/p&gt;</podcastTranscript><resourceImage><item>965475</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Watch Now</resourceButtonText><resourceButtonUrl>https://www.applause.com/resources/podcasts/ep-7-real-value-out-of-testing</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2023-04-18T16:25:06-04:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle>Getting Real Value Out of Your Testing</seoTitle><siteNamePosition></siteNamePosition><seoDescription>As companies look to maintain lean budgets, Matt Heusser explains why it takes full organizational commitment and a clever approach to extract value from all QA resources.</seoDescription><seoKeywords></seoKeywords><seoImage>{{ seomatic.helper.socialTransform(965457, "base", 0, "crop") }}</seoImage><seoImageWidth>{{ seomatic.helper.socialTransformWidth(965457, "base", 0, "crop") }}</seoImageWidth><seoImageHeight>{{ seomatic.helper.socialTransformHeight(965457, "base", 0, "crop") }}</seoImageHeight><seoImageDescription>Matt Heusser, Managing Director of Excelon Development</seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><canonicalUrl>true</canonicalUrl></inherited><overrides><seoTitle>true</seoTitle><seoDescription>true</seoDescription><seoImage>true</seoImage><seoImageDescription>true</seoImageDescription><robots>true</robots></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets></sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromCustom</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds><item>965457</item></seoImageIds><seoImageSource>fromAsset</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item><item><sectionId>58</sectionId><postDate>2023-03-21T07:00:00-04:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>370515</_authorId><id>957161</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>c0e15022-838d-483c-a8bc-3ea342ffe64d</uid><siteSettingsId>1428764</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>572608</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>Innovating for the Common User</title><slug>ep-6-innovating-for-the-user</slug><uri>resources/podcasts/ep-6-innovating-for-the-user</uri><dateCreated>2023-03-20T13:41:14-04:00</dateCreated><dateUpdated>2023-05-08T13:32:02-04:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>957161</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep-6-innovating-for-the-user</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep-6-innovating-for-the-user</url><authorId>370515</authorId><typeId>96</typeId><description>Learn why an innovative product often succeeds through brilliant simplicity rather than a rich feature set, and why new market launches require real-world feedback.</description><publishDate>2023-03-21 11:00:00</publishDate><episodeNumber>6</episodeNumber><episodeLength>33</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>uav5xulpx2</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p dir="ltr"&gt;Feature-rich products don’t necessarily lead to widespread customer adoption. In fact, the everyday user likely prefers a stripped-down, easy-to-understand product to one that offers all the functionality under the sun. Striking a balance between speed to market, inventive products and an emphasis on digital quality can sometimes lead to a push-pull that sinks innovation initiatives.&lt;/p&gt;
&lt;p dir="ltr"&gt;Turns out a diverse background can go a long way toward substantive innovation — just as Gary Larkin, the Chief Strategy Officer of Marker Trax and Koin Mobile. In this podcast episode, Larkin, whose background includes marketing, graphic arts and advertising, explains why an approach to product innovation must be simple yet refined and thoughtful yet profitable — not always easy needles to thread in the global marketplace.&lt;br /&gt;&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>Gary Larkin</podcastGuestName><podcastGuestPhoto><item>957162</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p&gt;Gary Larkin is the Chief Strategy Officer of Marker Trax and Koin Mobile. Through decades of leadership experience, particularly in the payment solutions space, Larkin has learned the ins and outs of successful product innovation and adoption, which often boils down to forming meaningful and ongoing bonds with customers. &lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p dir="ltr"&gt;&lt;em&gt;(This transcript has been edited for brevity.)&lt;/em&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;DAVID CARTY&lt;/strong&gt;: We are the product of our personal experiences. Perhaps that's why we look back on our past with fondness. Even if you're like Gary Larkin, whose days of youth were a bit on the wild side in the outskirts of Sydney, Australia.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;GARY LARKIN&lt;/strong&gt;: I mean, very different time. I was a child of the late '50s and '60s, and growing up in Australia, which was still I guess awakening at that point in time. What we call the great Australian bush was my backyard. We grew up in an environment where pretty much everything you saw would kill you. So you learn to be aware of your surroundings pretty quickly. But it was a wonderful childhood. It was a different time. One of those things, I think we all look back on our childhood and wish we could give it to our children, but I truly do believe I was blessed and had the gift of good luck on birth that you just can't buy.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Gary moved to the US in the 1980s to help launch Foster's Beer. Yeah, really. He's been here ever since helping some iconic brands and even television shows achieve relevance here in the States.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: In my early careers, I was involved in a wide range of film, entertainment, radio, TV, and subsequently put together a global marketing company that was specialized in sport and entertainment. And we had many Australian clients for whom we would operate both internally, but we were lucky enough to get the contract to help what was then Carlton and United Breweries launch Foster's globally. So we actually had the global launch through UK, Europe, Canada, and then into the US. But I was addressing our other business with my partner in the US at the time. He was engaged to an American and he had intended to take up root here. And I can recall clearly him calling one day and asking if I could get on a plane and come up and help him for a couple of weeks. He said, we really need some assistance here, this launch is getting a bit beyond us. And I can remember packing for two weeks, 37 years ago, and have never lived in the country since.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Gary's background includes everything from radio, television, and live events production and marketing to graphic arts and advertising. That might be a far cry from where Gary is today, but those experiences helped shape his perspective as an innovator.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: I think the through line really is, the business fundamentals apply. It really doesn't matter what you're applying it to, you have to remember that what's important to business is, at the end of the day, more money coming in the front door than going out the back, delivering a service that people like and come back and use more of as often as possible. And if you apply that critical thinking to whatever field you're endeavoring to move through, you'll find your footing fairly quickly. Solutioning through industry problems, looking for the bottlenecks, what are the points of abrasion, trying to think it backward from a customer experience perspective. And I think that kind of thought process of always starting with the customer, who's using this, what do they want out of this, not what do I want to give them, but what might they find valuable from what I'm trying to push forward is a really consistent thing.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: This is the Ready, Test, Go. podcast brought to you by Applause. I'm David Carty.&lt;/p&gt;
&lt;p dir="ltr"&gt;Today's guest is Aussie expat and Chief Strategy Officer of Marker Trax and Koin Mobile, Gary Larkin. With decades of experience in leadership positions across various payment solution companies, Gary Larkin has learned what works in innovation, and what doesn't. Gary helps companies successfully execute on their innovative visions in the expanding but highly regulated casino gaming industry. Innovative minds can dream up some big, grand ideas, but they need to be grounded. Let's find out from Gary how to approach that.&lt;/p&gt;
&lt;p dir="ltr"&gt;Now Gary, it's one thing to have a great idea for a product that will revolutionize an industry, and it's another thing to have it launched successfully in the real world. So what are some of the key criteria to executing on a product vision?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Look, I think the first thing is be prepared to fail fast. First and foremost, you really have to find out if the dog is going to eat the dog food. It doesn't matter how good for the dog it is, if the dog's not going to eat the dog food, it's not going to sell. And you need to do that with the consumers. Again, back to the consumers. Take it to the coalface. Find out, does -- I have a great idea, but does everybody else think it's a great idea? And even if they think it's a great idea, will they adopt? You can be easily drawn into sitting in a vacuum and talking to yourself when you're doing products development and trying to revolutionize or bring new products to market. It's very important, I think, to get a proof of concept out into people's hands quickly at the risk of being told it doesn't work.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: And do that early enough to pivot, come back, learn from that, and keep iterating on that product, thought, and design. Don't get too hung up in the technicalities. Don't let perfection be the enemy of good I think is the important part of how to bring something into market. And to be measured in where you release it. Don't get over your skis, put it everywhere. Try to do it in a measured fashion so you do have the opportunity to adjust in the product design and development and packaging before it gets into a fully open market environment.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Now it can be difficult to have a detailed grasp of how the customer is going to use the product, the customer journey when you're developing an innovative product. What are some of the ways that you can help get an understanding early on into how the customer will eventually use that product in their journey?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Look, I think it's really rather dependent on the type of product and the end consumer. Clearly, getting to the typical target and consumer quickly for input is important, but most products have interlocking circles of target customers. That center of the bullseye doesn't always jump out at you. And sometimes your product's going to fit a little better for the customer profile you didn't anticipate. So you've got to be open to this. I think it is important to check yourself and make sure you're not putting your thumb on the scale. Really, really be careful and try to be as clinical about how you allow the customers to come to your product as possible. If you're there helping them, that's not a true experience. Find ways to remote yourself and look in at what's happening. Get outside the paradigm and look back in, and learn from that, and really, really listen, and again, at the risk of learning what you didn't want to hear. Better to know early. Fail fast. And then learn and regroup and move on.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And as part of that, there's always going to be a little bit of a trade-off between speed and depth of product, right, having a really feature-rich product and having one that gets to market fast. So what is the balance between a minimum viable product that really, really hits on one or two great things and a more expansive, full-featured product.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: I know there are exceptions that don't prove this rule, but in my experience, the simpler the product is -- if you're moving a market, if you're trying to get a consumer to adopt a different behavior, buy a different product, shop at a different store, don't look for the ultimate solution out the gate. Iterate the -- move them forward gradually. Bring new products, new things in behind it. Build on the basis of the relationship you formed with that customer. Far too often I have seen, I have been guilty of this myself in my career in over-engineering products, putting all the bells and whistles in, and not realizing in the process I'm confusing my customer. That the true value proposition is getting buried in a lot of glitter and it's not resonating with them and needs to be stripped back to a more naked version of itself in order to find that earlier adoption. So there isn't a one size fits all. There's no, regrettably, no panacea. I think it takes a lot of thoughtful minds to solution a product's vision into a marketplace sustainably. And then the other part of that is always, how do I repeat this? How do I keep doing it? And have I built a product that is robust and fault tolerant, and scalable, and able to meet the supply chain needs and keep coming out to market day after day?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And this is an especially important idea if you're creating a product for the common consumer, the meat-and-potatoes type of user, right? So how can you develop a successful product that really connects with a broad consumer base that might not care about all the flashy bells and whistles, all the glitter, as you say, behind the product?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: I think there has to be a common thread of need, or at least value. Get clear on your value proposition. Really understand what you're doing for the consumer. That can sometimes be a little confused and muddied in products that get delivered through a supply chain, retailers. Or as we are in the gaming industry, we deliver a solution to a player through the casino who has a set of requirements, beliefs, and sometimes limitations. So working through that supply chain and understanding how to nuance that towards the end goal, not letting your mission and your message get hijacked by the distribution channel. And that can happen very, very quickly. So keeping close what you're doing and giving value to the supply chain, but not giving your product away in the process. Careful dance. And sometimes you don't know you've lost it until you look back and realize you just gave it all the way to the channel. So keep an eye on how the product is going to be delivered over time and make sure you factor that in in how you develop packaging, messaging -- customer support is a big part of this. These pieces are important if you want to own your product. While it seems sometimes easy to default that away to the channel when you do that, you also default the relationship of the customer to the channel.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And speaking to that relationship to the customer, they care first and foremost that the thing works, right? So how important is digital quality along this innovation journey, especially when it comes time to put a product in the actual hands of the consumer?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Look, I think it's very important. I think in today's market, that with incredible media clutter, with incredible competition for front-of mind awareness for time -- screen time, just simply getting people to want to click your app, use your whatever -- there's a lot going on there. It's incredibly important to ensure you have solutioned the smoothest possible customer onboarding experience to your product. Is it easy for them to use? They don't want to think. None of us do. We want to think about the things we like, but we want all the stuff that we get to just work. When we get our iPhone out of the box -- and I think that's the great example. We all want the iPhone equivalent product, and I think it's the great, somewhat of a gold standard. Get that -- lots of things to fault about iPhone, but by and large, consumers know they get an iPhone, pull it out of the box, plug it in, it's going to work. And that's what they've come to rely upon. I think looking for that as our gold standard and making sure whatever you do, no matter how little, and often it's less than you want, but no matter how little you do, do it well. Make it faultless. Let the customer know they can rely upon it to work persistently over time, they'll come back.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: I definitely co-sign on that idea, by the way, of, I don't want to think, especially at the end of a workday. The kids have exhausted me. Last thing that I want to do when I'm finally have time to myself is to think about the product that I'm working with there. So I co-sign on that. Speaking to that digital quality concern, is there a little bit of a push and pull and a balance there, too, especially when you're talking about innovation efforts? Can it be difficult to establish that success criteria in a way that doesn't interfere with the innovative goal there?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Well, look, I think you've said it earlier, it's a give and take. If you have enough value in what you're doing, a consumer will put up a little bit of pain. But they'll move quicker and stay longer for convenience than they will for cost. Most people tend to focus on, I can be cheaper tends to fall down the list of what motivates people to stay with something or move to something else. We'll all say it's the cost, but it's the convenience. And so if there's a lot of value, which isn't necessarily monetary, but if there's a lot of value for the consumer's participation with your products or services, they'll put up a little pay. But again, you want to be careful how big an audience you give that pain to. I think it is about trying not to keep making it painful as you iterate your product. Try to work a lot of this out.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Now today's marketplace is arguably more global than ever before. What's the key to success as you expand your product, both in terms of functionality and expanding it into new markets?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Well, look, I think you have to understand your lane. And I did a lot of export development in my earlier careers for the Australian government, Swedish government, Norwegian governments, bringing tech products and other products into specifically the US market, or other global markets. While the world is global and the opportunities are vast, I think you have to understand your niche, your segment. It may not be every geography, or it may be every geography but a much more limited niche within those geographies. Understand who you're selling to and what real potential you have to move into those global distribution channels. And is it valuable? Do you chase every shiny bauble, or do you focus on what's near and able to be mastered?&lt;/p&gt;
&lt;p dir="ltr"&gt;Be careful not to overreach because many companies have failed with growth. The expansion has overrun them and they've collapsed. Don't get beyond what you can sensibly support and control to keep integrity in your product, and make sure your resources -- and often if it's a new technology, the resources are limited, even if your budget isn't. The technical skills the domain expertise that's needed to build support and nurture your products delivery are often not in abundance. So they can be overrun pretty quickly, and good people can be buried in a sea of technical debt trying to catch up.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. It's a great point. And to your point, it has to be a viable fit in a new market, right? If local regulations prohibit what you're doing, online gaming, for example, it really doesn't matter how well it works, doesn't matter what the innovative vision is, it's simply not going to work. But even beyond that, there are many considerations when it comes to localization, the culture of the users in that market, for example. So is there anything that you can do at the planning stages of a new market launch to really help reduce that risk that you're taking on with expansion?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Well, again, you do need to spend a little time with the consumers. I recall in the early days of taking high-risk credit products into Mexico for Cash America then, as it was, we were trying to emulate products we'd successfully launched here in the US, presuming a somewhat similar adoption given the demand for credit. And I can remember staffing up for the large employment opportunities. We'd go out to large industrial complexes and we'd try to get employees to sign up to the program. And it was fairly typical at that point in time for financial service companies, banks particularly to have great looking gals in short skirts go out and try to promote their products. And so we followed. We thought that was a good thing to do. And we had a lot of attention and large crowds, and nobody signed up, or very few signed up. And what we realized is that they like to look at the girls, but they felt intimidated by them. So they wouldn't come over and sit down and sign up. No problem, we decided we'd put guys in suits and make it look more like a bank. Nobody signed up. And we eventually discovered that we needed a much friendlier, maternal kind of look and feel, and we hired housewives. And suddenly, adoption was immediate because they didn't feel threatened and they felt like they could interact.&lt;/p&gt;
&lt;p dir="ltr"&gt;So I think understand that culture, what is that culture. Don't project your thoughts, your cultural experiences into that new market. And that's true even within a geography like the US. I mean, it's often overlooked that the US is 50 states, and they're not all the same. And a big mistake that's made by a lot of people coming into the US, they just say, well, it's 330 million people, and you know, I'm doing so well in this market of 20 million people, this is going to be fantastic. And then it fails miserably because they don't understand the complexities and the geographies and the socioeconomic differences as you move across even states, let alone from state to state.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Yeah, it's funny how a conversion or a purchase can sometimes just come down to a gut feeling. It's always kind of interesting to think about that.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: And don't overlook that. I mean, believe me, at some point, every visionary has backed their gut. Steve Jobs, against all odds.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: You know. And now there are many, many stories of people who have done that and failed. But at the end of the day, somebody's got to make a call. And that's the other part of it. Don't overthink it either. It's analysis to paralysis sometimes. You do need to move. Learn going forward, not sitting in the workshop trying to make it perfect. You do need to put it on the street, generally for two reasons. One, you don't get any revenue till you do, and that's an important ultimate event. But also, you learn more out there than you'll ever learn in the boardrooms or the huddles.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right, the real feedback, absolutely. You have a rich background in payment solutions in particular. This is a challenging industry right now between global expansion, different forms of centralized and decentralized payments taking hold. There's really a lot to be mindful of there, right? So how can brands innovate in this space in a thoughtful but substantive sort of way?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Look, I think that much of what we do in payments today hearkens back to the early days of Bank America morphing into Visa. It's about trusted relationships. The very essence of our global banking system relies upon good faith relationships between banks and governments, that, at the end of the day, if you tell me you will pay, you will pay, and that the money was there. We put controls on that. We've layered anti-money laundering and Bank Secrecy Act and OFAC screening, and all of these things to make elements of the financial transactions safer or more secure for governments, but at the end of the day, underlying that you have this core need for trusted services.&lt;/p&gt;
&lt;p dir="ltr"&gt;So if you're going to get into the financial services sector, I would say first and foremost, plan to be a trusted partner, first and foremost. Remember that everybody operates with a sense that they're at some risk. So respect that. And while it may move slowly, and it does, as does gaming innovation, it does for a reason. There is a lot of risk associated with the transactions that take place when they get to scale. And typically, things don't start going wrong until you get to scale. The frauds don't happen until you have visibility, by which time you better be buttoned up. So how do we innovate? I'd say firstly, understand the entire landscape. You're dealing with banks at the end of the day. Crypto has proven to be the unreliable new kid on the block who one day is great, the next day is falling apart. It'll, in my opinion, in my humble opinion, will ultimately find its footing when it adopts regulation. And I think it will get there. And I think banks will move towards blockchain as the underlying technology to drive future financial transactions. We're seeing a lot more of that already.&lt;/p&gt;
&lt;p dir="ltr"&gt;But you need to look at who's there, what are you really trying to do, and what's out there. First and foremost, if it exists, don't build it. A lot of people run off and they start building technologies believing they're going to invent the better mousetrap. This is a tightly regulated environment. There are a lot of huge players in there. I prefer to look how to leverage what they do around a niche opportunity that we add value to in some way, which, quite frankly, is what we've done with Koin for the gaming industry as a mobile wallet, and what we've done with Marker Trax. Neither of those two products, essentially, are any different than things that go on every day of the week all over the world in financial services. They just haven't happened inside the gaming floor. And what we had to do was to identify what existed that we could lever, how did we build a channel, create a connectivity between it and the gaming floor, what had to be regulated and make sure we don't overregulate ourselves, and then we solutioned back from that. And we're still learning. It was a long process. I remember the day when we began it 3 and 1/2 years ago with Marker Trax, we thought, how hard can this be? We just block the cash. Well, 3 and 1/2 years later, we laugh about that. It can be really hard. But it's born an incredibly successful business in an industry that is burgeoning in the US market, burgeoning in the global markets, and we find ourselves in high demand.&lt;/p&gt;
&lt;p dir="ltr"&gt;Lot to do going forward, but much of what we talked about earlier here today we tried to apply. Been guilty at times of forgetting it and losing our way, and then having to come back to center. But we had the luxury of some outstanding patents that protected some uniqueness of what we did and gave us a little latitude to be wrong once in a while and still come back from that. Not all opportunities bring that with them. We've been blessed with that.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Gary, our world is more extensible all the time, our digital world. All of these APIs that need to connect, that need to speak with each other, they rely on each other. And you see this in your line of work, as well, right? Cashless products need to synergize, they need to come together, they need to create a better experience, for the gamer, in your case. So this question is going to depend on the openness of the particular industry, but how important is extensibility and even partnerships between different products when it comes to that end user's experience?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Look, in industries as mature or those siloed historically as the gaming industry, partnerships are essential. Our product couldn't exist if we weren't first able to form a partnership with one of the leading system manufacturers in the gaming industry. Konami was our first partner. I give them credit. They've been great. Having that and a test bed with a founder's property to play with as we learned, you know, would the dog eat the dog food, will they come back, what does it take gave us the chance.&lt;/p&gt;
&lt;p dir="ltr"&gt;But beyond that, the need to constantly partner with evolving parts of this marketplace. We found over time by staying open to the prospects that competitors aren't enemies, they just have a different product they're selling, and maybe one day we'll do that together, has allowed us over the 3 and 1/2 years to come back and partner with people who would have initially thought us to be complete rivals. And we continue to do that and are open to doing that. And thankfully, they are, too. We have to add value. You've got to be clear, ensure that the business partnerships you forge have value for each of them. And it's a big industry and, fortunately, we're able to coexist. But yeah, partnerships are essential both in the industry and outside the industry because, in our case, we're bringing traditional financial services -- banks, processors, payments aggregators -- into this environment through relationships we've forged and intermediary technology we control.&lt;/p&gt;
&lt;p dir="ltr"&gt;We're partnered with a large global entity, Euronet, who's been a phenomenal partner to us strategically, technologically. We've been able to bring a lot of what they do into our products to improve them, make them more robust, more scalable, more fault tolerant, more able to run alongside the growth we're finding inside our business. And that's the other reason, I think, to constantly think about partnerships. What it takes to be a startup, what it takes to be an early stage company, and what it takes to be a productized enterprise class deliverer of financial services, completely different things. And at each point along the way, knowing when to transition into that next architecture, it really is -- and it's not just the tech. It's the company. You need to be mindful that often what you start out with as a competency set has to be morphed over time to meet a growing business opportunity or you'll stifle your own growth.&lt;/p&gt;
&lt;p dir="ltr"&gt;So it's the great challenge of business. It's the thing I love about business. And it's the constant through line in whatever business you are in, whatever industry you have been in, anywhere in the world. And so if you can get those fundamentals locked in early and you bring that to the table each time you embark on a new adventure, ultimately, you'll find them paying off over time.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: OK, Gary, final sprint questions here. In one sentence, what does digital quality mean to you?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Digital quality mean to me? I think it's delivering a seamless experience to an end consumer, whoever that is, that satisfies their need, and persistently satisfies their need.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: That's perfect. What will digital experiences look like five years from now?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Oh my God. I wish I could, I wish I could imagine -- different. Different. Completely different.&lt;/p&gt;
&lt;p dir="ltr"&gt;I mean, I think of a world that much of what we rely upon in the day weren't existed, where laptops, iPhones are moving away. We don't have them. And then what does that mean when we decouple? What point do we -- I mean, I think of the discussions that are looming around singularity, you know. Awesome and frightening topic. I think the thing that's going to change us most in all industries is what probably is most frightening, and that's AI.&lt;/p&gt;
&lt;p dir="ltr"&gt;What AI will do to every industry, what it will allow us to do that we've never thought of doing, the power it will give to our innovation is going to so greatly change the shape, I think, of our daily lives in the next five years. I'm excited to see it. I'm a little scared as to what it might do because I've seen some, as we all have, we've seen some great downsides of some of the technology revolutions over the last 15, 20 years. It hasn't all been good. We haven't mastered our technologies well to our end goals. But hopefully, we'll do better as it continues to evolve.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Gary, what is your favorite app to use in your downtime?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: My favorite app to use. I'm going to admit to this to my wife's chagrin, and that is I'm a TikToker.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Yeah?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: Yeah, I do. I mean, I play a few minor games. I like to keep my mind active, but I'm a little concerned that I overstimulate. You know, I'm a bit guilty of constantly reaching out for the phone. But yeah, I love that instant gratification, that eye candy that TikTok gives you with little bits of what's going on in life. But I'm a news guy, too. So I like to get on the news feeds and catch up with what's going on around the world.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: And Gary, what is something that you are hopeful for?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;LARKIN&lt;/strong&gt;: What am I hopeful for? I mean, in a business sense or in a general sense? I mean, in a business sense, I'm hopeful that we find greater harmony with the technologies that we're making available to ourselves. I truly am. I think there's a tremendous potential to have great outcome. I don't think we're doing it well enough today. I think we can do it better, so I'm hopeful for that.&lt;/p&gt;
&lt;p dir="ltr"&gt;On a more general note, I'm hopeful that we can step back away from some of the vitriol we've been able to inject into our culture over the last 5 or 10 years. I think we use the screen of social media to vocalize things we would never say to somebody's face. And that kind of bullying or just bad behavior, I'm hopeful we'll find its way out of our culture.&lt;br /&gt;&lt;/p&gt;</podcastTranscript><resourceImage><item>957176</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Watch Now</resourceButtonText><resourceButtonUrl>https://www.applause.com/resources/podcasts/ep-6-innovating-for-the-user</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2023-03-29T15:53:48-04:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle>Innovating for the Common User</seoTitle><siteNamePosition></siteNamePosition><seoDescription>In this podcast episode, Gary Larkin, Chief Strategy Officer of MarkerTrax and Koin Mobile discusses the practical limitations of product innovations.</seoDescription><seoKeywords></seoKeywords><seoImage></seoImage><seoImageWidth></seoImageWidth><seoImageHeight></seoImageHeight><seoImageDescription>Gary Larkin, Chief Strategy Officer of Marker Trax and Koin Mobile</seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><seoImage>true</seoImage><canonicalUrl>true</canonicalUrl></inherited><overrides><seoTitle>true</seoTitle><seoDescription>true</seoDescription><seoImageDescription>true</seoImageDescription><robots>true</robots></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets></sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromCustom</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds></seoImageIds><seoImageSource>fromAsset</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item><item><sectionId>58</sectionId><postDate>2023-02-08T06:00:00-05:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>4618</_authorId><id>944302</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>6b8366ab-3a03-4530-850e-1aa300d83088</uid><siteSettingsId>1410460</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>561833</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>Don’t Be One of These Testers</title><slug>ep-5-dont-be-these-testers</slug><uri>resources/podcasts/ep-5-dont-be-these-testers</uri><dateCreated>2023-02-07T15:08:27-05:00</dateCreated><dateUpdated>2023-03-15T13:15:38-04:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>944302</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep-5-dont-be-these-testers</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep-5-dont-be-these-testers</url><authorId>4618</authorId><typeId>96</typeId><description>The testers who thrive in today’s fast-paced, oft-evolving digital world manage to strike a balance between deep domain knowledge and a curiosity to expand their repertoire of skills.</description><publishDate>2023-02-08 13:00:00</publishDate><episodeNumber>5</episodeNumber><episodeLength>29</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>qfah0dwcjg</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p&gt;Software testing attracts people with a variety of backgrounds and skills. But the ones who thrive in today’s fast-paced, oft-evolving digital world manage to strike a balance between deep domain knowledge and a curiosity to expand their repertoire of skills.&lt;/p&gt;
&lt;p&gt;QA engineer, expert and author Kristin Jackvony coined six tester personas who struggle to evolve and advance in their careers. We’ll talk about two of those diametrically opposed personas, Job Security Jim and Conference Connie, and where the middle ground is between deep domain knowledge and being open to new processes or technologies. Kristin is the author of the book The Complete Software Tester: Concepts, Skills, and Strategies for High-Quality Testing.&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>Kristin Jackvony</podcastGuestName><podcastGuestPhoto><item>944303</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p&gt;Kristin Jackvony is an experienced QA manager and tester, specializing in both improving legacy software and supporting new software from the earliest stages of development. She organizes systems and processes to support all areas of testing. Kristin is the author of the book The Complete Software Tester: Concepts, Skills, and Strategies for High-Quality Testing, which you can find &lt;a href="https://www.amazon.com/Complete-Software-Tester-Strategies-High-Quality-ebook/dp/B09NGVVCJ9/" target="_blank" rel="noreferrer noopener"&gt;here&lt;/a&gt;.&lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p dir="ltr"&gt;&lt;em&gt;(This transcript has been edited for brevity.)&lt;/em&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Want to stump Kristin Jackvony? Good luck. Kristin loves puzzles and escape games. Really, any situation where she can use her wit and the tools available to her to solve a problem, she thrives.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY: &lt;/strong&gt;Somehow, I came across, I think it was called Doors, where basically, it's all these little mini puzzles where you have to solve the puzzle to be able to open the door and go into the next room. And I was just hooked. I have probably played hundreds of these games now. So yeah, they're a lot of fun.&lt;/p&gt;
&lt;p dir="ltr"&gt;And I think one of the things I like about them is that you know you have all of the tools that you need to solve the puzzle, as opposed to some of the puzzles that people like to solve that are sort of like lateral thinking, where you have to think outside the box and imagine things. You know all the tools are there. You just have to figure out how to put it together.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Kristin has been interested in puzzles from a young age. But her journey into puzzles and escape games as an adult has led her to some interesting intellectual places, including an interest in a new language.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: It seems like there are a lot of escape games that are Japanese. And because of that, I actually started to get interested in the Japanese language. So I've been learning Japanese on Duolingo. And it's fun now when I'm playing a Japanese escape game, usually, they will give you all of the clues in English. But sometimes, there will be like a sign that's in Japanese. And it was really fun for me the first time that I could read that what the sign said was ramen. I'm like, I know what that says. So that was really exciting.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: It's not puzzling to see why Kristin became a QA manager and tester. Just like an escape game, testers are always on the hunt for clues in an application, even if it's not as simple as finding them behind a hidden bookcase door.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: The reason why I am a software tester and not a software developer is because I really enjoy being presented with sort of, not a finished product, because, of course, it's not finished yet, but a product that all of the clues are there, I guess you would say, kind of like with the escape game. So somebody says here, we've coded this new feature. We need you to test it.&lt;/p&gt;
&lt;p dir="ltr"&gt;And a lot of times in my career, I've discovered that even though we're supposed to have documentation about how features work, a lot of times, there's no documentation. So it takes a lot of experimenting, a lot of playing around. And I really enjoy that. I really enjoy the exploration part of trying to figure out how a feature works and then thinking, what are some ways that I could test this? Or what are some edge cases that we might need to explore? And then thinking, are there any ways that I could break this? I think that's really, really fun.&lt;/p&gt;
&lt;p dir="ltr"&gt;And I think that's different from what developers do. And I appreciate developers so much, because they are the ones that are building the things. They are starting with nothing. They're starting with a blank sheet, with no code on it, and trying to figure out, I need to build this feature. How am I going to do that? So that's really an admirable skill. But my skill set is more exploratory, figuring out how something works, figuring out all the ways that it might not work, and then trying those out.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: This is the Ready, Test, Go. podcast, brought to you by Applause. I am David Carty. Today's guest is master puzzle solver and test engineer, Kristin Jackvony. Kristin is an experienced QA manager and tester, currently serving as the principal engineer in software testing at Paylocity. She organizes systems and processes to support all areas of testing, whether that's for improving legacy software or supporting new software from the early stages of development. Kristin also speaks at conferences and is the author of the book The Complete Software Tester, Concepts Skills and Strategies for High-Quality Testing. So, what does it take to be a complete software tester? Kristin would know.&lt;/p&gt;
&lt;p dir="ltr"&gt;Kristin, first of all, congratulations on your book, The Complete Software Tester. So from a high-level perspective, what should a complete software tester look like today, particularly as it applies to their skills and their competencies?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: Well, the most important thing — and this is often sadly overlooked — is they need to be able to find bugs. That's why we are software testers. We are trying to find the bugs before the software is released and the customers find the bugs. So that's absolutely the number one thing.&lt;/p&gt;
&lt;p dir="ltr"&gt;Another thing that is very important, especially these days, is to understand APIs and how APIs work and how to test them. So that's very important.&lt;/p&gt;
&lt;p dir="ltr"&gt;And then I would say the next level is test automation. You need to be able to automate. We cannot, at the rate that software is being developed today, we cannot get by on just manual testing. We need test automation.&lt;/p&gt;
&lt;p dir="ltr"&gt;And then finally, it's very important for software testers to understand some of the adjacent software testing areas, such as security testing, accessibility testing, performance testing. These are all very important. And they are becoming more and more important each year.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. Now, you've discussed and you've written about six testing personas to avoid, right? So today, we're going to talk about two in particular, Job Security Jim, and Conference Connie. I keep wanting to say Jim and Pam, but it's Jim and Connie. So can you walk us through what those two personas look like?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: Sure. So Job Security Jim is a guy who's been at his company for a really long time. And because he's been there so long, he understands the application backwards and forwards. He knows exactly how to configure things and how things are supposed to work, which is great. But if the company starts to evolve, as are all companies that need to survive do, and they might have a rewrite of the application and in a new coding language, or they might be making some changes, Job Security Jim sometimes feels like he doesn't have to really learn that. He doesn't have to learn automated testing, or he doesn't have to learn automated testing with a new tool. So what winds up happening is he's stuck sort of in one place with his one skill set. And that's very dangerous, because as the company evolves, he's going to wind up getting left behind.&lt;/p&gt;
&lt;p dir="ltr"&gt;And then Conference Connie, she's the one who's very, very excited about new things. She loves to go to conferences. She loves to find new ideas, learn about new tools. And all of that is really wonderful. But Conference Connie will often come back to her workplace and not do anything about the new tools that she learned about or new techniques. Or sometimes — this is sort of a variation of Conference Connie — she might go back to her workplace and say, we need to scrap everything that we've done so far with our automation and start all over again with this new tool. So both of those things are very disruptive. And both of those things are — Connie is learning, but she's not learning deeply. Connie is not learning in such a way that she's actually going to be able to benefit her team.&lt;/p&gt;
&lt;p dir="ltr"&gt;So the thing that Jim and Connie have in common is both of them are not learning and growing.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. It's interesting, because they are diametrically opposed in some ways. But they actually have some similarities here. So to go back to Job Security Jim, this person brings a lot to the team, right? I mean, they have some deep domain knowledge that can be really useful in that respect. But there's also some negatives. So can we dig into a little bit more of what they bring to the team with respect to what they offer their fellow team members, and what some of their weaknesses are there?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: Yeah. So the positives of Job Security Jim are that he knows where all the bodies are buried, as it were. So he understands all the little tips and tricks for if something's not working correctly, or what those bugs that have been sitting around forever and nobody has bothered to fix, and how to work around them. He might also have knowledge about how to configure complicated testing situations. So those are all real positives.&lt;/p&gt;
&lt;p dir="ltr"&gt;But the negatives are, as he falls behind, as his skill set doesn't improve, while what's needed for the job becomes more complex or more advanced, he's going to start to become a bottleneck. He's going to be the guy that is continuing to do the old manual testing or using the old slow tool, while everybody else is waiting for him to get his work done so that they can continue to move forward.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. And you said test automation is one of those skills that a complete software tester should have. It's one of the things we talked about right off the bat. So as time goes on, more testers will be expected to embrace automated testing. So this is an area where Job Security Jim in particular, is likely to get left behind, right?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: Oh, absolutely. Yeah. Without question. I mean, there are few jobs left today that require manual testers. But there are very few. They don't pay well. They don't allow for good career advancement. So you've got to learn how to automate. I mean, you're just as smart as everybody else is, speaking to the audience here. And if other people can learn how to do it, so can you. So you got to start.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. Now, I've done the conference circuit before, as have you. It can be a great source of inspiration and ideas, that kind of thing. But that's some of the challenge. How can the Conference Connie persona get a little bit more focused or a little bit more grounded to come back to their team or their organization with some real concrete, actionable ideas?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: Well, I've got a couple of ideas about that. So one idea would be that Conference Connie should think about what problems she's trying to solve before she goes to the conference. So let's say, for example, on her team, they realize, oh, we're struggling with our mobile automation. We want to get better at mobile automation. So she could go to that conference and sign up for some of the presentations that are going to help her answer that question. So then she can focus, and she can start making a list of actionable information that she can take back to the team.&lt;/p&gt;
&lt;p dir="ltr"&gt;Another thing that she could do, if there's no particular problem that the team needs to solve, she can go and while she's attending the sessions and taking notes, she can be thinking about, what is an actionable step I could do when I get back to the team about this one thing that I'm learning? And because she's so excited, she's probably going to come up with 20 different actionable steps. But what she needs to do is maybe on the plane ride home, she needs to be looking at, what did she have on her list, and picking maybe just one or two things to take back to the team.&lt;/p&gt;
&lt;p dir="ltr"&gt;And definitely, it's not a great idea to go back and say, we need to completely rewrite our automation, because that's just going to stress everyone out. So don't do that. If there's a real problem with the automation, then maybe you might want to say, I'm going to do a proof of concept on this one new tool. And then we'll discuss it afterwards. Something like that's okay.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. Right. And to that point about stressing out your fellow team members — Conference Connie can be perceived as a dog chasing cars, right? There's always another bright, shiny one coming around the corner. So if you are already Conference Connie with all of these grand ideas you come back with, but you're trying to be more grounded, you're really trying to actually make these things actionable, what can she do to sort of fight that negative stigma that might be attached to him or her?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: I think she needs to start building some trust. So when she comes back — you know, she's going to come back. She's going to be excited. She's going to be filled with ideas. I would encourage her to maybe write a blog post about it, like if the company has maybe a place where they can share those things, or just write a document. But then when she meets with the team, just have one idea. Like, ‘Hey, everybody, do you remember that problem we were having where we were having trouble figuring out how to parallelize our testing? Well, I learned this one tip that I think will help us.’ And have it be something that the team can adopt without a great deal of pain and stress.&lt;/p&gt;
&lt;p dir="ltr"&gt;And so when the team adopts that, then they're going to say, ‘Hey, you know, Conference Connie's got some good ideas. So maybe we'll listen to her a little more next time,’ instead of ‘Oh, no. Here we go again. She's going to come back with 10 things that we can't use.’ So I think it's just slowly building up trust that way.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. That makes sense. Now the stark difference between these two personas, it speaks to that DevOps-y kind of idea of evangelizing knowledge. And that all sounds good until you've got these two very, very different types of people. One is resistant to change, but really digs in deep and absorbs information on a detailed level. The other loves the idea of change, but might struggle to implement it or kind of get to that next level or even understand it in a concrete way sometimes. Is that how you see it? And how can managers kind of affect change with these two personas and evangelize knowledge in a way that will really stick?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: Oh, that's a great question. And I think that that's something that a lot of companies struggle with, because there's always going to be a tension between, we need to grow and evolve and change, and we need to keep things the way they are. And there's cases to be made for both sides of those things.&lt;/p&gt;
&lt;p dir="ltr"&gt;I mean, leaving things the way they are ensures that your software is going to continue to chug along nicely the way everybody's expecting. But making sure that you're adapting and growing means that your software is going to adapt and grow and provide more value, which is what your customers are going to expect.&lt;/p&gt;
&lt;p dir="ltr"&gt;So I think the most important thing to do is to strike a balance between the two and make small, incremental changes, evaluate those changes, and then come up with what the next step is and continue to move forward.&lt;/p&gt;
&lt;p dir="ltr"&gt;And I think it's also very important, and there are a lot of teams, especially managers like to do this, they like to come up with the big idea and expect that everybody's going to pivot to the big idea. But then, they get distracted. They think about something else. And then the big idea is lost. And those who are trying to implement it are stuck with, well, ‘What do we do now? Do we keep trying to implement the big idea? Or do we go back to the way things were?’ So I think slow and consistent progress is the best strategy.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. And probably a case to be made for keeping things the same and implementing change at the same time, depending on the project, or depending on the team even. So it definitely speaks to that what you're talking about striking that balance. So when these personas hit the job market, what are some immediate red flags that you can spot? Are there items on their resume, or things lacking from their resume that really stick out to you right away?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: Yeah, I think there's two different things that I look for that are potential red flags. And one is the job history.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, Job Security Jim might have been at his company for 20 years. And sometimes, that's really wonderful because that means that he found a good place, that he was making a difference, that people were valuing him. But it can also mean that he got into a job that he thought was cushy, and he just dug in his heels and he stayed there. So think about that. If you're a person who's been with your company for a really, really long time, you're going to want to make sure that you're showing some movement. What kinds of projects did you work on? What kinds of new things did you learn?&lt;/p&gt;
&lt;p dir="ltr"&gt;So, then, at the opposite end, Conference Connie can sometimes be a bit of a job hopper, because she gets really excited about whatever the new tool is. Oh, there's this new startup, and they're using the new testing platform. So I want to get on to that. So if I see somebody who's hopping jobs every year for several years in a row, that's a red flag to me because that says, if I hire this person, they're going to get bored. And they're going to want to move on very quickly. So to be able to show that projects were completed, to be able to speak to why it was that you left one job for another, that would be very helpful.&lt;/p&gt;
&lt;p dir="ltr"&gt;And then the other thing that I look for on resumes is also skills. What skills do these people have? And so if I see Job Security Jim is using one antiquated tool that no other companies are using, that's a red flag for me because I think Job Security Jim hasn't bothered to learn, let's say, Node.js. And we use Node.js all the time. So how do we know the Job Security Jim's going to be able to transfer his skills to our team?&lt;/p&gt;
&lt;p dir="ltr"&gt;But, then, Conference Connie can sometimes tend to put every single language or tool that she's ever been exposed to, even a tiny bit, onto the resume. So then it looks like she is sort of jack of all trades, master of none. And that's concerning too, because at my company, we're going to have specific tools that we use. We're going to want Connie to be able to really dive into those tools and use those. And if she only has a limited knowledge of everything, we're not so sure how well she's going to learn. So think about when you're including tools on your resume, think about what projects you used those tools for. So make sure that you have that kind of experience on your resume that will help ease the hiring manager's mind.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. And the workforce, the workplace has changed so much in the last few years. I'm putting you on the spot a little bit with this question. But are there any emerging personas that you are researching or writing about in today's landscape that reflect the changes that have happened in the world?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: Well, I think one of the personas that I list in my book is Automation Annie. And this isn't really new. But I think the problem is continuing to get worse. We are continuing to see ‘software testers’ — and I'm using that in quotes — who really what they know how to do is write test automation. Test automation is great. But if you don't understand your application, and you don't understand what it is that you should be testing for, really, all you're doing is busywork. You're writing code that might not provide any value at all.&lt;/p&gt;
&lt;p dir="ltr"&gt;And one of the things we do at my company when we are hiring software testers is, we give them a challenge where we give them a buggy application. And we say, please find the bugs and give us a bug report of all the bugs that you found. And it is astounding the number of applicants I have seen who have as much as 15 years of software testing experience who can't find the bugs. But on paper, their resume looks wonderful, because look at all of the wonderful things they've automated. Look at the code they know, the tools they use. But if they can't find the bugs, they're useless to me. So I think that's a disturbing trend. And it has continued to get worse.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: That's interesting. In some cases, is it somebody that is struggling with the pressure of finding the bug in that particular situation? Or is it just a skills issue there?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: I think it could be a skills issue. I don't think it's a pressure issue because it's not timed. The challenge that we give them is, take a few days. See what you can find. So I don't think it's that. I think for a lot of Automation Annies, I think they actually aren't interested in finding the bugs. What they like is, they like to write the code to test the software. They find that fun.&lt;/p&gt;
&lt;p dir="ltr"&gt;And I think maybe, it's sad to say, but I think there are some people who enjoy writing code, but they don't want to be software developers because they don't want to be held accountable for making a feature that then people can't use. But if they write test automation that is flaky and has lots of false failures, they might not be held accountable, especially if they're job hopping from place to place. So I think that's what it is. I think it is a lack of skill set and in some, a lack of caring.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Wow. That's an interesting trend to keep an eye on. And it really speaks to the value of effective, exploratory testing, as much as we want to focus on automation. It really gets back to that point. So, Kristin, if there's one blanket piece of advice that you could give to a tester who is really struggling to figure out the next step in their career, what would that advice be?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: Well, one piece of advice would be, read my book.&lt;/p&gt;
&lt;p dir="ltr"&gt;But, seriously, I think, don't feel like you have to boil the ocean. You know, that's the expression where you just take on too much all at once. I think sometimes testers can be hard on themselves, and they can feel like there's so much that I don't know. But all you need to do is find one area to work on, say to yourself, what is something that is a weakness that I have right now that I could learn, a skill set that I could improve on, that would have a big impact on my career?&lt;/p&gt;
&lt;p dir="ltr"&gt;And, so, for some testers, it may be automation. And, so, what they can do is, there are lots of great tools and programs that they can use to learn how to automate testing, including, I've got a whole section in my book about test automation and how to get started with test automation. So set a small goal for yourself. Maybe this month, I'm going to look at what my team has in test automation, and I'm going to try and make a change to one test to improve it, or something like that. Or if your team doesn't have any test automation at all, you could say, I'm going to pick a test automation tool, and I'm going to do a little proof of concept and run one test, just small things like that.&lt;/p&gt;
&lt;p dir="ltr"&gt;Another thing that's very helpful — and I got this tip from a coworker — is to actually put time on your schedule for professional development. So if you say to yourself, every Thursday at 3 p.m., I'm going to take an hour, and I'm going to work on improving my skills. And mark it off as a meeting for yourself. Nobody has to know that it's just a meeting with yourself, so they won't invite you to whatever other meeting is happening at that time. And then you can say, I'm busy. I'm in a meeting. And you can use that time to improve. So start small and be consistent.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Right. And I know in your book, you were really purposeful about including some different things that the reader can work through, some different examples, right?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: Yes. In the automation section, I actually have a GitHub repos that you can pull down, and you can actually run those things on your own computer at home to see how they work. One of the ways that I really like to learn things, especially around coding is, I like to see how somebody else did it. And then I can transfer — I can say, ‘Oh, well, that's the syntax to do such and such. So now, I can just copy that and put it in my own project.’&lt;/p&gt;
&lt;p dir="ltr"&gt;I've also created as part of this book, to go along with the book — although you don't even need to buy the book to use it — there's an application, a web app, with an underlying API that anybody can use to test. And they can test making API calls and then seeing the result of those calls if they go to the web app. And they can see how those two things work together. They can actually use it to practice writing test automation. So that's a useful tool. And with the book, you can really kind of step through how to start to acquire those skills.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Okay, Kristin, final sprint questions here. In one sentence, what does digital quality mean to you?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: To me, digital quality means applications that are providing value to their users and are doing so reliably.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Perfect. What will digital experiences look like five years from now?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: I'm expecting that five years from now that people will have a more personalized experience with all of their applications.&lt;/p&gt;
&lt;p dir="ltr"&gt;One of the things that I've noticed over the last few months is that every time I get a new computer or get a new app on my phone, I'm being given the opportunity to choose light mode or dark mode. And I think that that's only going to expand, that people are going to say, what color would you like the app to look like? Or which things would you like to see on the home screen? So I have a feeling that as we move forward, we're going to see more and more of that personalization.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: I'm [on] team dark mode. So I hear you on that.&lt;/p&gt;
&lt;p dir="ltr"&gt;What is your favorite app to use in your downtime?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: So it was hard to choose. I have three, because I like to always have my brain going no matter what I'm doing.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, if I'm running on the treadmill, or if I'm doing the dishes or folding the laundry, I like to have something going on in my brain. So I really, really enjoy Audible. I enjoy listening to books while I am walking or running. That just keeps me engaged and keeps me thinking about how much I'm suffering on the treadmill.&lt;/p&gt;
&lt;p dir="ltr"&gt;And, then, I also really like podcasts. I have an app called Podcast Republic. So I'll listen to podcasts while I'm doing things like folding the laundry.&lt;/p&gt;
&lt;p dir="ltr"&gt;And, then, for while I'm working, I love Spotify. I like to listen to music while I'm working, especially if I'm doing like deep testing, I like to have that music kind of going on in the background. So all three of those. I'm not sure I could live without any of those.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;CARTY&lt;/strong&gt;: Sure. Totally understandable, and Ready, Test, Go. is available on a podcast player near you.&lt;/p&gt;
&lt;p dir="ltr"&gt;And final question for you here. What's something that you are hopeful for?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;JACKVONY&lt;/strong&gt;: I am hopeful for more software quality. So I have noticed over the last several years that web applications have become more consistent, more consistently reliable. And that is really wonderful. I'm really happy about that. But every now and then, something will still surprise me, like I will be signing up for a new financial service, and I'll discover that the form won't take zip codes that have a leading zero, which basically leaves out all of the Northeast. So every now and then, I'll find something like that.&lt;/p&gt;
&lt;p dir="ltr"&gt;And I feel like mobile applications are still a little bit behind. I think we've gotten used to the fact that, ‘Oh, sometimes the app will crash,’ or, ‘Oh, sometimes it won't be responding, so I'll have to kill it and open it up again.’ I would love that in the future we get to the point where that's a real rarity. So I'm keeping my fingers crossed for that.&lt;br /&gt;&lt;/p&gt;</podcastTranscript><resourceImage><item>944305</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Watch Now</resourceButtonText><resourceButtonUrl>https://www.applause.com/resources/podcasts/ep-5-dont-be-these-testers</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2023-02-24T12:21:47-05:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle>Don’t Be One of These Testers</seoTitle><siteNamePosition></siteNamePosition><seoDescription>In this podcast episode, Kristin Jackvony discusses why testers must strike a balance between deep domain knowledge and a curiosity to expand their skills.</seoDescription><seoKeywords></seoKeywords><seoImage></seoImage><seoImageWidth></seoImageWidth><seoImageHeight></seoImageHeight><seoImageDescription></seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><seoImage>true</seoImage><seoImageDescription>true</seoImageDescription><canonicalUrl>true</canonicalUrl></inherited><overrides><seoTitle>true</seoTitle><seoDescription>true</seoDescription><robots>true</robots></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets></sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromCustom</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds></seoImageIds><seoImageSource>fromAsset</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item><item><sectionId>58</sectionId><postDate>2022-12-22T15:57:00-05:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>4618</_authorId><id>910510</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>d4e706de-d08b-4d29-9def-760f197db8d7</uid><siteSettingsId>1371329</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>539249</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>What Elevators Can Tell Us About CX</title><slug>ep4-what-elevators-tell-about-cx</slug><uri>resources/podcasts/ep4-what-elevators-tell-about-cx</uri><dateCreated>2022-12-22T15:39:34-05:00</dateCreated><dateUpdated>2023-01-03T16:20:14-05:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>910510</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep4-what-elevators-tell-about-cx</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep4-what-elevators-tell-about-cx</url><authorId>4618</authorId><typeId>96</typeId><description>Automation expert and author Adonis Celestine talks about putting the customer journey first, and craft high-quality products that directly address their wants and needs.</description><publishDate>2022-12-22 05:00:00</publishDate><episodeNumber>4</episodeNumber><episodeLength>25</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>0r51pihmq9</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p&gt;A product-centric approach to development can leave blind spots in the user experience. All the functionality might work, and all the testing might be thorough, but it can still fail if customers simply aren’t interested in the product.&lt;/p&gt;
&lt;p&gt;Automation expert and author Adonis Celestine recommends organizations flip it around — put the customer journey first, and craft high-quality products that directly address their wants and needs. Learn more about how leading brands accomplish this objective, and how this approach will evolve in the future.&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>Adonis Celestine</podcastGuestName><podcastGuestPhoto><item>910517</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p&gt;Testing expert and advocate Adonis Celestine is the Director of Automation at Applause. He has written three books on digital quality, including Quality Engineering: The Missing Key to Digital CX. He also speaks at conferences all throughout the EU.&lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p&gt;(This transcript has been edited for brevity.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DAVID CARTY:&lt;/strong&gt; Where do you like to travel on vacation? Do you prefer the hustle and bustle of the city? Maybe do a little bit of bar hopping or take in a musical. How about traveling to a big sporting event or a concert with thousands of people in attendance? Well, you won't find Adonis Celestine there. No, no. He prefers a quieter, more secluded type of vacation. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ADONIS CELESTINE: &lt;/strong&gt;Yeah, I love traveling, mostly in Europe. So I love islands, especially in places where there is low human impact, where I don't see anybody. Yeah, Canary Islands is one of my favorite ones, especially because of the nature there. &lt;/p&gt;
&lt;p&gt;The entire island is like a volcanic eruption. And when you are there, especially during off-season, if you're there on a beach, you see, like — next to, you see a huge pile of volcanic mountains and get this surreal feeling of the power of nature. I love those places. I like to walk around it and spend some time over there. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Of course, even if you're seeking the solace of nature, it doesn't count as a vacation if you didn't take any photographs. Pics, or it didn't happen. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; What it is if you don't have those social media likes next to you? Life is boring. I love mountains as well. So just like climbing a few — not a big climber, I am, but small mountains where we do a small bike up and then have a small picnic when we come down. &lt;/p&gt;
&lt;p&gt;So I do it with my family. So my wife also likes to walk. So we go around to the holidays. &lt;/p&gt;
&lt;p&gt;I love Croatia as well. They have a lot of small islands, unexplored islands. And some of them are really remote, which you can get on with a boat, like, maybe once or twice a day when there's a boat service from the main island to a smaller one. So I love it. &lt;/p&gt;
&lt;p&gt;It's an amazing place. I would recommend you to go there. I think Europe itself is quite beautiful. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY: &lt;/strong&gt;For Adonis, these vacations offer a critical time to unplug, reduce the noise, and center himself, not to mention it's also a great way to avoid any emergency work calls. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; I work a lot during my day, work on several stuff, including projects, company, my personal stuff. I love to speak. I like to write. I spend a lot of time on that. I also play hard. So on these vacations, I like to shut out myself, be away from the world. So these kind of vacations help me to kind of hit a reset button. But I can — after the holidays, I go back to work with a fresh mind with some fresh inspiration. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; This is the Ready, Test, Go. podcast brought to you by Applause. I'm David Carty. &lt;/p&gt;
&lt;p&gt;Today, we're speaking with seclusion seeker and automation expert Adonis Celestine. As the director of automation at Applause, nothing satisfies Adonis more than a happy customer. &lt;/p&gt;
&lt;p&gt;As an advocate for test automation, he strives to make software testing easy, yet innovative for brands all around the world. Adonis has written three books on digital quality, including his most recent, which he published in September, called Quality Engineering: The Missing Key to Digital CX. He also speaks at conferences throughout the EU, but right now, he's speaking with me. &lt;/p&gt;
&lt;p&gt;All right, Adonis. Let's start off with the broad strokes question. How do you define digital quality, especially as it relates to the customer experience? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; Yeah for me, from my perspective, quality is really hard to define. It depends on who you ask. Like, if you ask my kid, who likes to play Roblox, they know — they would never care about quality. It's like, it's in a game. Like, they're climbing stairs. And after like 10, 20 steps, there are no steps anymore, and they don't care. It's just a feeling, the experience they get. That is what matters to them. &lt;/p&gt;
&lt;p&gt;Same in the business side as well. It depends on who you ask. You ask a business guy what is quality, this is all the requirements. They write it up.&lt;/p&gt;
&lt;p&gt;And if you ask a quality engineer what quality is, it's all about test cases that he has written down. But in the end, quality is a perspective. It cannot be properly defined. And that perspective can change based on the experience of the user. And that's why I see quality and digital experience are something very similar to each other. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Now, Adonis, the title of this episode is "What Elevators Can Tell Us About CX." So, Adonis Celestine, what can elevators tell us about CX? I understand there's an example that points back to the Industrial Revolution. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE: &lt;/strong&gt;Yeah, that's indeed, because elevators have and quality and experience of something in common. During the Industrial Revolution, when — after the Industrial Revolution, when the passenger lifts were introduced, people were really afraid to take the lift. They were feeling claustrophobic, and they were complaining that the lifts were really slow. And they are even better if they walk up the stairs, which is much faster. &lt;/p&gt;
&lt;p&gt;So they did several things, like bringing up the speed of the elevators, making it really fast and usable. But still, people didn't like it. And then a smart engineer came up with an idea. &lt;/p&gt;
&lt;p&gt;So what he did, instead of approaching it technically, he put on some mirrors on the elevators. So the mirrors not only gave them a feeling of there is more room to avoid claustrophobia, but it also kept them occupied. So people were using it for all kinds of stuff, like adjusting their hair, putting on makeup or lipstick. So they did not notice the passage of time, which gave them a feeling, OK, it is quite better. &lt;/p&gt;
&lt;p&gt;You can also see the same in your digital applications. You see this progress bar when you are opening a web app, or you see all the spinners. All these are like putting these mirrors on your elevators — to make the customer feel that your application is faster and more efficient. So that's why for a digital experience, it's not always about technical stuff. It's about managing this kind of stuff, which brings up — elevates the experience of the user which is very important. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Now, if any elevator engineers happen to stumble upon this episode, might I suggest a minibar or an espresso machine? That would be my next innovation. Just a suggestion. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; Yes. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; That one's for free. You can take that one.&lt;/p&gt;
&lt;p&gt;So I want to get back to something you said before, Adonis. So, you were talking about business requirements, user requirements. These typically define the test cases, right? So, how far do these go toward actually addressing user needs and concerns? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; Yeah, it depends on project to project. I have seen projects where you have these user requirements or so-called business requirements which have been worked on for 12 months in an Agile way. We develop something, and then we go to the customer just to find out that there is actually no market for that product itself. So that's why it's very important to have some design thinking mindset where we first agree on an idea, make small prototypes, take it to the market, validate with the right customers and users, and then build on top of it. So it's an iterative method with valid user feedback. So that is where I see, like, a gap in the current development methodology where the user requirements are somebody's thought, or someone higher up in the organization thought this is what the customer wants, which can be far from reality in some cases. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Right, so it sounds like there are definitely some user-centric gaps there that really aren't covered by these user requirements and test cases. Can we drill down on that a little bit more? What other kinds of gaps do you see in this kind of situation? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; I can give you an example of a ketchup bottle, for example, from Heinz. So you have this upright bottle which is like a very nice product. And you also have this ketchup bottle with the down-facing lid, which is more popular. If you look at the market analysis from Heinz, the bottles with opening at the bottom sold much more than the ketchup bottle with the lid on the top. So you see two distinctive designs here. One was more product-centric, which was based on requirements. User requirements, based on functionality — tested, probably, on the test cases derived out of it. And the other one is more like what the actual user wants. &lt;/p&gt;
&lt;p&gt;While the ketchup is — Heinz ketchup is a great product, if you have struggled to get it out of it, especially when it is cold and when it is, like, only the few ketchup left in the bottle, that is where the user-centric approach comes into picture. So I see a lot of gaps, And companies which have managed to identify this and make revenue or make an iterative development in their products have been really successful &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Yeah, that's a great example. So you ultimately propose a shift from a product-centric approach to a user-centric approach to development. Now, this all makes sense, but it's probably no small effort. So, what sorts of challenges can businesses expect to encounter as they make this kind of shift? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; You're right about it. It's more easily said than done. Of course, the first thing to find out is, like, what does your customer actually need or want? That's a $1 billion question which companies are trying to solve. But it is also not always what the customer wants. As Henry Ford said it, if you ask my customer back in those days what they want, they would have probably replied like, ‘OK, I need a faster horse.’ They don't see the need for a car there. So it is not always about what the customer wants, but it is about balancing what you think. There is a market for a product and what the actual customer wants. So balancing this and getting this outside-in perspective into your design process, into your development process, is the key. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; So there's a happy medium there, right? I mean, this is also very different from the days of old. I mean, you can go and make that monetary investment to gather that input from your customers, right? So is it a matter of some of these companies just not spending that money the right way or not valuing the customer's opinions enough? What's your perspective on that? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE: &lt;/strong&gt;Yeah, it's like — it's a process. So we have to try it out first. It's not always easy. So there is always some kind of resistance to change. &lt;/p&gt;
&lt;p&gt;So my perspective is, like, just try it out. Invite your customers probably for your product backlog meeting or when you have developed a nice prototype. Just show it to them to see, like, how do they think about it, get that feedback into your process, and then iteratively develop it. I think that's a good way to approach — to bring in this user perspective into your process. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; And if you still have some opposition when you're trying to make this shift, there will always be naysayers or people that don't want to change their ways of working or thinking about a product. How do you proactively deal with that opposition? Because it's going to pop up with any kind of change that you make, right? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE: &lt;/strong&gt;Yeah, of course, all this process that I talk about requires a real mindset change itself. And that is not easy to happen, easy to make it within an organization. So it's all about giving them a taste. &lt;/p&gt;
&lt;p&gt;For example, when I was working for a bank, we invited a few of our customers to our office. We just put them in a room and asked them to use our application with some minimum instructions. And like, we were, like, waiting out — it's like an interrogation room. &lt;/p&gt;
&lt;p&gt;So we could watch what they are doing. They cannot see what we are up to. So with the way that they were navigating the application as developers and testers in an Agile team, that gave us a lot of insights. That gave us a lot of feeling about, ah, that's a different perspective. I never thought about it. I never thought about a few things even my own application, which I have been building for the last five years or so. So, those insights, if they can get a taste of it, probably, like, that resistance from people who are willing who are not willing to change will go down a bit because they have seen it in action. They have seen it working. They get a perspective out of it. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Right, and to help emphasize the point, from your perspective, what are some brands that succeed with an aggressively user-centric approach? Anybody come to mind? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE: &lt;/strong&gt;I think I have a lot of practice. So if you look at the modern tech companies, they are all about digital experience. Let's take the example of Netflix, Uber, Airbnb, Booking.com. All these companies have some kind of like X factor which makes them fly, and that X factor is the CX factor, in my customer experience, factor. So if you look at Uber, Uber is nothing about — Uber don't own any cars by themselves. They are nothing about travel industry itself. It's a tech company which revolutionized how travel needs to happen. I can give an example. &lt;/p&gt;
&lt;p&gt;As I said in the beginning, I travel a lot. So before the Uber days, whenever I go book a taxi to the airport, the tester in me kicks in. So I used to doubt everything. Did I book it correctly? Did they register my name properly? What if the driver’s car breaks down? All those things creates anxiety. For me, I don't care if Uber is expensive or not. That anxiety has been taken away with the use of Uber. For me, that's a good experience of a service. So all the tech companies that we see today are focusing on customer experience, and that has been their key success. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; But I bet the tester in you still checks the app five times to ensure that you put the right address in there, right? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; Yeah, of course. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; I can relate to that 100%. So I'm with you. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; Yeah, that's why Uber has an app which gives you some real time insights. They know exactly — they acknowledge that they have received your request. They tell you exactly where the driver is. All these things reduces those kinds of anxiety.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY: &lt;/strong&gt;Absolutely. Now, I'd imagine narrowing down the customer journey isn't easy for everybody, right? So what are some ways to verify and refine how you're developing for and testing against a customer journey? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; Yeah, there are three different ways. The most common one is, like, to first build out some kind of personas to identify who are your end customers, what kind of customers you have, segmented based on the age group or based on some kind of segmentation, and then work on those journeys, like how would those customers use my app, and build your application for those personas and also test your application for those personas. That will give you a good insight about what your customer is doing and also to ensure that that journey is seamlessly well.&lt;/p&gt;
&lt;p&gt;And there is also the second way where it's like a trial and error way, which we call the A/B testing in the quality world where we send out a feature, see if it is working— for example, in the US elections, back days when President Obama was there, they used this A/B testing to find out which of the website was attracting more hits from the common public. So they had a website with a nice US flag on it. They also had a website with a family picture of Obama on it. Of course, the one with the family picture attracted a lot of people. So that was a way for the people to find out, OK, this picture works quite well. You can do the same with products and features. We can build something, show it to a larger group of people to see which one works, and then go about that path. &lt;/p&gt;
&lt;p&gt;And then there is also the third way, which is more a technical way. Use AI machine learning to also synthetically simulate how a user journey would look like. So for example, in flight situations, nobody can actually test an emergency situation. It's all done through simulation. So those journeys and stuff could also be simulated and developed based on that and also tested for quality based on simulation. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Yeah, whether you use AI or whether you use A/B testing, these customer journeys are dynamic, right? Everything from technical changes to geopolitical to economic factors, these can all change how a customer interacts with your brand and your product. So how do you provide a reliable, high quality experience if and when that journey evolves? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; That's a really good question because a lot of people in this customer experience digital world think like, OK, I made a customer journey, and then I'm done with it. But your customer journey also should go through your change management process. It is not a good journey map unless you keep changing it frequently as and when your customer persona's profile changes. &lt;/p&gt;
&lt;p&gt;So, awareness like, OK, you have — this is not maybe the journey document. It's the first step. And then also updating it based on the feedback — every time that you get a feedback from your customer, you also keep updating your customer journey map. So, over a period of time, it will evolve. And yeah, that will help us to sustain the changes that are happening in the customer profile itself. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Now let's go back to using AI. How can organizations use AI to help assist with predicting customer behavior? And are there some common mistakes that some organizations make there? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; Yeah, I think AI and machine learning, when used at the right level, can bring in a lot of good experience. For example, when you're shopping at Amazon, it will show you some kind of related products. That helps me a lot because I can compare the prices, and I can also see what is there in the market in their product range or in that category which helps me to make that buying decision. But that is kind of a personalization. Next time when I come and search for a similar product, it also recommends me, OK, these are the products that you look before. Probably these are the ones you wanted. &lt;/p&gt;
&lt;p&gt;For all these to work for the analytics, you need data. And that data, I know it's collected from me. I'm OK with it until a level. But there are lots of companies where, OK, if I'm discussing with my girlfriend or with my wife about buying something, the next — and even without any Googling or without getting into any of the internet stuff, the next day morning, if I open a website and if I get recommendations to buy that product, that gets really creepy. So AI and machine learning are useful if you keep the personalization to a particular level in such a way that experience is still really sweet. Once you— &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Even that can change over time too, right? We might become more open and embracing of these kinds of predictive behaviors over time too as people, right? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; Yep. Yeah, I think, yeah, if you look at the market, like, hyper-personalization is, like, what everybody is looking at. Nike, everybody, every company, digital company, they want to give that tailored experience. But hyper-personalization beyond the point can also be quite creepy and piss off customers. A lot of people don't want to give away their data. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY: &lt;/strong&gt;Yeah, there's definitely a fine line to walk there, to be sure. Now, Adonis, moving forward the way users interact with digital products is going to evolve even more, whether that means the metaverse or just some other version of an integrated digital experience. The way consumers interact with their digital worlds will probably look very different in the near future. So how does this evolution dial up the table stakes for businesses, and what should they be strategizing for over time? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; I think change is inevitable in the digital world. Even very technical people who call themselves very tech savvy, they could also find themselves in 20 years, like, completely out of the digital world. It's changing so fast, especially in the metaverse world. But I think if you look at the bigger companies or the digital companies, they are already started to think about it. If you look at Nike or if you look at Gucci, for example, all fashion brands, retailers, they already have a store in the metaverse that people can buy stuff. So businesses that adapt to these changes, they will survive. &lt;/p&gt;
&lt;p&gt;And others, of course — the Kodak example often comes into discussions in these kinds of scenarios. You have to move digital. You have to move forward, and you have to adapt new technology as you go on. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; All right, final sprint here, Adonis. I have a few more questions for you before I let you go. In one sentence, what does digital quality mean to you? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; For me, digital quality is all about experience and perspective. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Simple. I love it. What will digital experiences look like five years from now? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; That's hard to predict. Probably we need an AI engine to do that. But it will move on from a nice to have position that we have now into a necessity for businesses to survive. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; What is your favorite app to use in your downtime?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; I have a lot of favorite apps. I play a lot of mobile games. I play "War and Order." That probably takes a lot of my time every day. But still, it's like a stress buster for me. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; Absolutely. What's something that you are hopeful for? &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CELESTINE:&lt;/strong&gt; It's about — in the digital world, it's about inclusivity and accessibility. As I said, a lot of people would be left out, even the technical people, in the future. How do we bring all these people in the metaverse world, for example? That's a key for the future. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARTY:&lt;/strong&gt; That was the Adonis Celestine. If you happen to bump into him on vacation, say hi to him for me. Oh, actually, you know what? Better yet, just leave him alone. Let him enjoy nature, OK? Thank you for listening. Shout out to our producers Joe Stella and Samsu Sallah and graphic designer Karley Searles. Feel free to reach out at podcasts@applause.com, and we will catch you next time.&lt;/p&gt;</podcastTranscript><resourceImage><item>910516</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Listen Now</resourceButtonText><resourceButtonUrl>https://www.applause.com/resources/podcasts/ep4-what-elevators-tell-about-cx</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2023-01-03T16:20:14-05:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle></seoTitle><siteNamePosition></siteNamePosition><seoDescription>Automation expert Adonis Celestine talks about putting the customer journey first, and craft high-quality products that directly address their wants and needs.</seoDescription><seoKeywords></seoKeywords><seoImage></seoImage><seoImageWidth></seoImageWidth><seoImageHeight></seoImageHeight><seoImageDescription></seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><seoTitle>true</seoTitle><seoImage>true</seoImage><seoImageDescription>true</seoImageDescription><canonicalUrl>true</canonicalUrl></inherited><overrides><seoDescription>true</seoDescription><robots>true</robots></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets></sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromCustom</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds></seoImageIds><seoImageSource>fromAsset</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item><item><sectionId>58</sectionId><postDate>2022-10-31T17:36:00-04:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>4618</_authorId><id>897054</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>7915fca5-e69a-4789-91f8-539676fa1a4a</uid><siteSettingsId>1352048</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>526669</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>Why So Ceremonious?</title><slug>ep3-why-so-ceremonious</slug><uri>resources/podcasts/ep3-why-so-ceremonious</uri><dateCreated>2022-10-31T17:22:07-04:00</dateCreated><dateUpdated>2022-11-03T16:50:31-04:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>897054</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep3-why-so-ceremonious</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep3-why-so-ceremonious</url><authorId>4618</authorId><typeId>96</typeId><description>On this episode Jeff Payne discusses what real Agile testing looks like. It’s not a ceremonious approach to a Waterfall way of working — it’s about substantive organizational change.</description><publishDate>2022-11-02 04:00:00</publishDate><episodeNumber>3</episodeNumber><episodeLength>34</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>7lb8cfgo19</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p dir="ltr"&gt;Agile is a great way of rethinking product development. But, if all you’re doing is slapping some meetings on the calendar and calling it Agile, you won’t see real change. &lt;/p&gt;
&lt;p dir="ltr"&gt;As the CEO of Coveros, a consultancy, which also owns TechWell, the host of popular software engineering and testing conferences, Jeff Payne talks with experts in the field all the time. He knows their pain points and challenges — and while some progress is being made, other organizations are fundamentally struggling to enforce change.&lt;/p&gt;
&lt;p dir="ltr"&gt;On this episode of the Ready, Test, Go. podcast, Payne discusses what real Agile testing looks like. It’s not a ceremonious approach to a Waterfall way of working — it’s about substantive organizational change. On top of rethinking how products are developed and tested, organizations must be proactive to remove obstacles and get disparate teams working together.&lt;br /&gt;&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>Jeff Payne</podcastGuestName><podcastGuestPhoto><item>897056</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p&gt;From technical lead to CEO of Coveros, Jeff Payne has made a 35-year career of helping solve digital quality concerns. Jeff has published more than thirty papers on software development and testing, and even testified before the U.S. Congress on digital issues.&lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p dir="ltr"&gt;&lt;em&gt;(This transcript has been edited for brevity.)&lt;/em&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;David Carty&lt;/strong&gt;: Jeff Payne collects vintage football cards and memorabilia. As a fellow sports card collector, I was very pleased to hear this. I collect ultra-modern baseball cards of today's current players, but Jeff's passion is a bit more expansive. He collects memorabilia from football players over a 100-plus year span. Everything football from 1869 to 1988 -- cards, programs, ticket stubs, photos, matchbook covers, and postcards, Jeff collects it.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Jeff Payne&lt;/strong&gt;: I collect anything vintage American football, from the inception of the game in 1869 through about 1988, both baseball and football cards. And then when my boys got into sports, they started picking up and wanting to collect things, and I just kind of went along for the ride for a while, and then just kind of caught the bug and decided to keep rolling going forward.&lt;/p&gt;
&lt;p dir="ltr"&gt;I have some pretty nice Jim Thorpe items. Probably my most prized possession is, I have a 1911 Jim Thorpe large cabinet photo of Thorpe from his days at Carlisle when he played in college that's autographed on the front. I've only seen one other one. It was on Antiques Roadshow, actually. So I've seen another. But it's my prized possession because he is my favorite all-time football/sports person.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Jeff got back into collecting when his children took an interest in it, and he quickly found that the hobby provides a few life lessons for young collectors.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: How to negotiate. Also how to ask for things, right, the right way. So you know, when they first started, they didn't know a lot. So I used to walk around with them and I would point out who the star players were or the people that were pretty well-known. And then I would help them figure out what they should ask for in terms of a price, and do a little negotiation.&lt;/p&gt;
&lt;p dir="ltr"&gt;Then when they got good, they didn't really need me anymore. That's ironically when I actually started picking things up myself because I was getting bored. I would take them to shows and stuff, I was pretty much just the chauffeur. They had price guides, and they had memorized everything. They knew everything about it, a lot of times more than I did. And so I was just dropping them off, more or less, but they were too small to leave there by themselves.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: So why vintage when you can watch any team play any week? Why collect items from players who played decades or even a century ago?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: I'm a big history buff around sports. When I was a kid, I read all the time. I read every sports book that was in the elementary school library, middle school library, high school library. And I just always liked the history of sports.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, my collecting, even as a kid, even though I was really just opening packs. I grew up in the '70s, and trading with friends and whatnot. Did find a couple of antique malls and a couple of card shops that occasionally we'd arm-twist our parents into taking us to. And they had older things.&lt;/p&gt;
&lt;p dir="ltr"&gt;I would see cards that I had read about these players. I'd read about Red Grange or Jim Thorpe, or on the baseball side, Mickey Mantle or Babe Ruth or Ty Cobb, or whoever it was. And here I'm seeing things from their playing days. And I just thought that was really cool, being so interested in the history of sports, that I just gravitated toward the old timers. Plus I always joke I don't have to worry about Babe Ruth getting hurt and his card value going down, right? At this point, he's pretty pristine.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: To most collectors, there's something cathartic about it. It's more than just a way to spend time and money on a frivolous pursuit. It's a path to fulfillment.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: I think it's similar to what I hear a lot of collectors say, which is it's a way to reconnect with your childhood, right? I mean, most look back pretty fondly on their childhood. Not everything was perfect. But collecting to me is just, it reminds me of a simpler time. The only care in the world I had was, what was going to be for dinner; on the weekends, running down the street playing with friends; opening packs, trading them; chewing gum; hitting a ball; catching a ball; whatever you were doing. I feel like it connects you with your youth.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: This is the Ready, Test, Go podcast, brought to you by Applause. I'm David Carty.&lt;/p&gt;
&lt;p dir="ltr"&gt;Today's guest is vintage football collector and CEO, Jeff Payne. Jeff's company Coveros is a consultancy that helps clients modernize their software processes. From technical lead to CEO, Jeff Payne has made a 35-year career of building secure software and solving digital quality concerns. Jeff has published more than 30 papers on software development and testing, and has even testified before Congress on digital issues. Coveros owns TechWell, which hosts the popular STAR and Agile + DevOps conferences, including STARWEST, which was held in Anaheim earlier this month. Let's talk with Jeff.&lt;/p&gt;
&lt;p dir="ltr"&gt;Let's start out with your definition of what Agile testing is. What is Agile testing really, and what sorts of processes does it involve?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Yeah, great question. So I always hate when people define terms with the terms themselves. So I'm tempted to say Agile testing is testing in an Agile development process, but it actually is more than that. Certainly, any kind of testing you're doing within an Agile process you could describe as Agile testing, or at least testing for Agile. But I think if you go deeper into it, there are particular techniques and approaches to testing that are Agile that you could apply to any software development process.&lt;/p&gt;
&lt;p dir="ltr"&gt;A great example is exploratory testing. It's a very iterative, learn-as-you-go, plan-as-you-go testing technique that is very popular in Agile. But there's no reason you can't apply --- and it is applied to Waterfall and other types of development processes.&lt;/p&gt;
&lt;p dir="ltr"&gt;So I always say that Agile testing is testing that is performed in an incremental or exploratory manner that allows you to plan your testing as you go. That's my definition.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: In your discussions with clients and people attending STARWEST, what sorts of challenges did you hear around Agile testing adoption? Were those challenges the organization, or were they financial, a little bit of both, something in between? What can you share from some of the conversations you had.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Yeah, just got back from Agile testing. It was this past week. Great show. Had a lot of good, engaging conversations with people. What I heard --- and I did give a talk on Agile testing and some of the challenges. What I heard from people is, unfortunately, in an Agile process, and I'm talking really here about the testing we're doing in an Agile development process, the first challenge is we're still doing little mini Waterfalls in our sprints. Call it Scrummerfall, Scrumbutt --- there's lots of names given to it. But it's where we've just tried to shrink down the Waterfall and stick it in a very short increment of time. And that doesn't work very well. We'll talk about some of the ramifications of that later. But that is still very common out there in the industry, unfortunately.&lt;/p&gt;
&lt;p dir="ltr"&gt;The other thing I heard a lot was challenges around test automation. I mean, we could do a whole podcast on automation of testing and its importance in an Agile process, but people are still struggling with what kinds of things should we automate, what kinds of tests should we automate, how much do we automate, and all those kinds of things. I heard a lot at STARWEST.&lt;/p&gt;
&lt;p dir="ltr"&gt;I think the last thing was just trying to figure out, how do I get my developers and testers to work together every day? If I'm not going to do a little mini Waterfall, what's the model look like? How do we interact every day, and how does that all work?&lt;/p&gt;
&lt;p dir="ltr"&gt;So, those are some of the things I heard that seem pretty prevalent at STARWEST.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: So there's a lot that we could pick apart there. Let's focus on the mini Waterfall version of releasing here. So Agile testing, it involves a lot of ceremonies, many of which I think our audience would be familiar with --- daily stand up, sprint reviews, retrospectives, et cetera. These can be helpful, but these don't make you Agile, right? So what are some of the issues that can pop up if you're focusing too heavily on the ceremonies?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Yeah. So a couple of things. First of all, in the ceremonies, inherently, the entire team is supposed to be doing those ceremonies together. A lot of times I see people aren't necessarily doing the entire ceremony as a team. So for instance, if one of your ceremonies in Scrum is your kickoff, kind of kicking off your sprint or your iteration, if in that process not everybody is involved in the sprint activities, the kickoff activities, reviewing the stories, estimating those stories, or at least finalizing an estimate for those stories, coming up with acceptance criteria, maybe even creating some initial tests, then you're going to have problems because you're not doing everything collectively. One of the goals in the kickoff is to make sure everybody is on the same page about what we're doing in the sprint. And, if you don't work all together, there's going to be communication gaps, which is what we're trying to avoid in Agile.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, that's one area where I think people need to do better.&lt;/p&gt;
&lt;p dir="ltr"&gt;The other thing is that if you --- at least in Scrum, and I use Scrum as a reference because it's so popular, all of the Scrum ceremonies --- if you read the Scrum literature, calls them inspect and adapt activities. Now the word ‘inspect,’ inspect doesn't mean a status meeting, but so often some of these activities turn into a status update. A daily status update. A sprint demo is just a demo update of what you're doing.&lt;/p&gt;
&lt;p dir="ltr"&gt;If you look up the definition of ‘inspect,’ it's not status, right? It means look at something closely. It means examine something against a standard or a criteria. It means some kind of thought process associated with it. Not just, “Here's my status.” And we don't often do that [inspection].&lt;/p&gt;
&lt;p dir="ltr"&gt;Then, the second thing is, we don't adapt. So a lot of these stand ups, we don't spend any time after we've inspected talking about, “Well, what should we do about this, or what should we do differently, or how do we handle these challenges that we just heard about?” And so we don't inspect and adapt. Also, whether you're following Scrum or not, I think that's a good kind of best practice for any of your ceremonies.&lt;/p&gt;
&lt;p dir="ltr"&gt;And, then, some people just aren't doing --- we'll run into companies all the time at Coveros that don't do retrospectives, and then they complain that they're not getting better. That's what retrospectives are for. They're to inspect and adapt what you're doing and make it better. If you're not doing that, you're not going to get better. So there needs to be more rigor around just doing the ceremonies.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: These are well-intentioned ceremonies, right? But they can actually get in the way, it sounds like, of actual process improvement, or, at the very least, they can paint a little bit of an inaccurate picture as to what your organization is trying to accomplish with some of these processes here.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: No doubt. Yeah, no, they can. And it results in all sorts of challenges, right? I mean, this mini Waterfall concept that I mentioned and not figuring out how to work together in your ceremonies and in your sprints leads to all sorts of problems. Usually, I see things like sprints getting elongated. So, organizations are trying to fix the problem that they don't have time to finish their testing by making the sprints longer. Well, you're kind of solving the wrong problem when you're doing that. It's usually not the duration of the sprint. It's how you're working in that sprint, right?&lt;/p&gt;
&lt;p dir="ltr"&gt;Or, you'll hear organizations who every so often will have a hardening sprint, they'll call it. They only call it that because they know you're not supposed to have testing sprints. So, they call it “hardening” instead. But it's just basically catch-up on all the testing we didn't get done, or fix all the bugs we still have.&lt;/p&gt;
&lt;p dir="ltr"&gt;And, again, solving the wrong problem, right? We need to figure out how to work better in our sprints to fix those kinds of things. Those are some red flags that we see a lot with people that we talk to and that are struggling with Agile and Agile testing.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: You mentioned elongating sprints, hardening sprints. What are some other red flags that you've identified that point toward organizational challenges?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: So, one red flag to me is, there's organizations out there who are trying to create a PMO [project management office] to make sure all the teams follow their quote, "Agile process." Well, if you read the manifesto, the beauty of Agile is that you inspect and adapt, you retro as a team, and you decide what's your process, right? There's some guidelines. There naturally needs to be guardrails. But you can't prescriptively tell teams this is Agile and this is what you need to do. But we see that a lot. People want to codify a process, say it's the process, and then have people audit that process. That's just not the way Agile works. That shows an organization-wide misunderstanding of Agile.&lt;/p&gt;
&lt;p dir="ltr"&gt;The other thing is, senior execs are very quick to punt on a transformation or an improvement effort before it really has a chance to succeed. We see them --- they have missed expectations. Maybe someone gave them those expectations, that transformation is going to be fast, that Agile is free. I've heard people say that. Well, Agile is free, right? You just follow these ceremonies and magically, everything works better. Nah, there's nothing free in software, you know. Software is a hard, hard, hard thing to build and get right, and nothing's easy and nothing's free. But, yet, some organizations believe that, have heard that. Because of that, they punt too quickly and never see benefits from their improvement efforts.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Right. Agile is not a tool you buy, but it's not free, rght? There's a distinction there.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Absolutely. Yeah, I think the Scrum Alliance says something like, “Scrum is amazingly simple to understand and amazingly difficult to implement correctly.” And that sums up Agile to me, right? You've got to really go into it with your eyes wide open.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Yeah. And it takes patience and it takes commitment. Absolutely.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, some of the Agile engineering practices that make for real change --- behavior-driven development, continuous integration, things like that --- these require really changing your way of working. And that's no small feat. So how can orgs accomplish that today, particularly if they are resistant to change?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Yeah, great point. I mean, the ceremonies we're talking about are kind of what I call the process side of Agile. And they're important. We need to inspect and adapt. But, in my experience, if you're not figuring out how to get your developers, your testers, other people that are building, testing and delivering software working together in a different way with different engineering practices, then you're not going to be successful. You're going to see the problems that we mentioned earlier.&lt;/p&gt;
&lt;p dir="ltr"&gt;How do you address that if you've identified some of the things that you mentioned as potential engineering practices to adopt? Well, first, you've got to start small. I'm a huge fan of piloting change. And, so, in our process at Coveros, we help lots of people transform their Agile process. Once we have a plan in place, we always start with a pilot, where we take the improvements we feel like are going to have the biggest impact, and maybe the fastest impact because we want to show some quick successes too --- if executives don't see progress, they get nervous, right? And we try to apply them to a particular product first, and measure the results, and the success, and the ROI of applying Agile techniques and engineering techniques. Get it working one place first. I think that [does] a couple of things. One, as mentioned, it demonstrates success, which is good up the chain. But, what I've also experienced is, and you know this, any kind of change, it's cultural, it's people-related. People have to change, right? Our tools don't change, our processes don't change. You get people to change. And people are interesting entities, right? Interesting beings. We don't like change in general. And so we're not typically going to jump at change.&lt;/p&gt;
&lt;p dir="ltr"&gt;Now, there's exceptions. But, one thing that gets people to change is if you see others doing something successfully, then you might think, “Huh, wow, that kind of works and it looks a lot better than what I'm doing, maybe I should try that.” So already that switch is turned, and that's sometimes the hardest part.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, piloting things. And I always say, when you pilot, you've got to demo like crazy. You should be giving constant demos of what you're doing and success. It gets buzz, and it gets other people actually coming to you and saying, “Can we be next? Can we try this next?” You've already won. You've fought half the battle when people come to you and ask for change, right, versus you having to track them down, drag them out of a cave, and beat them into Agile submission. So, you know, piloting is a great way to do that.&lt;/p&gt;
&lt;p dir="ltr"&gt;You also need to get everybody on the same page, whether it's through formal training and education types of things, or it's self-study, or brown bags and lunch and learns. You've got to get everybody up to speed on what is Agile really about, what's it mean --- how do you be Agile instead of do Agile. And that takes some education, any way you want to slice that, because you want everybody to go into it with the same expectation, including senior leadership, as mentioned. So, getting them on board and understanding what it means for them and for the org is going to be equally important.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Right, and working with a consultancy could be great because you can be the bearers of bad news instead of somebody internally trying to enforce change on a heavy-handed kind of level.&lt;/p&gt;
&lt;p dir="ltr"&gt;But, I did want to ask you, on that note about aligning expectations, and you mentioned getting developers and testers on the same page, so I want to ask you about that. It's a critical part of achieving effective Agile testing, right? So what are some ways that organizations can foster a little bit of better collaboration between those two groups?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: I would say that, for me, the thing that has been the most successful is some form of pairing between devs and testers. I wrote an article on dev-test pairing and gave some different approaches that you could use to get developers and testers working together. Whether it's one-on-one pairing with a dev and a test, whether it's using kind of the BDD Three Amigos [approach], where you've got the business, dev and test all working together on a story, or you’re mobbing where you have everybody working to build and test story by story. Whatever you're doing to get people to work together every day in some model is going to help that a lot, right?&lt;/p&gt;
&lt;p dir="ltr"&gt;Make a contest of it too. I've seen success with creating what we call a pairing board, where you take your team and you say, all right, developers are on this axis, my testers and other roles are on this axis, and we're going to set a goal. Depending on the size of your team, every sprint or every quarterly increment or whatever the time frame is, we're going to figure out how everybody works with everybody else at least once on a story. And we're going to fill the chart in, and we'll do it at the end of every --- either in your stand ups if it's just a sprint activity, or at your retros or your sprint demos. Then, if we fill in the whole chart at the end of the quarter, we're going to have a party, or something. Or you're going to get a prize, or whatever it is, right? Gamify it, make it fun, make it something to track. I've seen success with that because then it's a fun gaming activity with maybe some reward at the end of it, instead of a, “thou shall all work together,” kind of a mantra from above.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: So, if there's one takeaway today, it's that a pizza party is just as much of a motivator for adults as it is for kids.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Absolutely.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: That's what you want to leave this podcast with today.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Yeah, well, but beer helps, too. Not for kids though, but for adults. Yeah, add that into the adult motivation.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Yeah, you got my attention, for sure.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, when we spoke before, you mentioned that there's been a little bit of an interesting debate around technical debt, which is a little bit related to this topic. The thinking had been that you pay down your debt whenever you have a chance to slow things down a little bit. But you told me that lately the conversation is changing a little bit. And how is that?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Yeah, so this is something that's popped up in the last year. And we actually had an internal consultant at Coveros do a brown bag on it because they'd heard an interesting talk on it, thought it was an interesting discussion, and we had an internal discussion about it. But now I'm starting to hear it at the events. I heard some conversation about this in one of the sessions that I attended at STARWEST.&lt;/p&gt;
&lt;p dir="ltr"&gt;Really, what it gets down to is that, originally, technical debt as defined I think it was by Ward Cunningham --- one of the founders of Agile came up with the term “technical debt.” His original point was that it was the debt that you were imposing upon yourself when you decided to release something early. And because you were doing that, maybe it wasn't fully documented or maybe it wasn't fully defined, or you decided you were going to put something out that maybe wasn't yet fully ready, but it was something you felt from a market perspective made sense to do. But, now, over time what's happened is --- and then there was going to be some debt incurred that later you're going to have to fix. But what's happened over time is, people have started to lump almost any kind of issue into technical debt. So, when you ask people what they mean by technical debt, they say everything from bugs, that's technical debt; uncommented code; code that's not readable; lack of documentation are all lumped into this idea of technical debt. That wasn't the original intent of the concept. The point that people are saying is, “Hey, if you go back to the original definition, that's really where we need to focus our attention. We shouldn't be just working off technical debt because it's technical debt as now [it is] today defined, because a lot of that may not have as much ROI as building new features.&lt;/p&gt;
&lt;p dir="ltr"&gt;So the pushback now is, instead of just being a zealot and saying, “We just always have to make sure our technical debt is low,” is to evaluate that debt and make sure it is actually debt, and weigh that against the value of features that we're implementing. And we haven't, I don't feel like, done a good job of that. It's been a one-sided drive to reduce technical debt irrespective of any kind of quantifiable measure, if that makes sense.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Definitely. And it gets to be a scope issue, like anything else. I mean, as you add that debt up there, it gets to be harder to pay it off and it takes more time and resources.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Yep, absolutely.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Interesting. So, as you said in our discussion prior to recording, you're seeing more of an embrace of DevOps and continuous integration, or at least an attempt at those practices, and that's a good thing. But, what's the next step forward for some of those orgs or teams that might still be a little bit early in their maturity level?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: So, from a testing perspective, I think it's about figuring out what you should and shouldn't automate. I mentioned earlier, one of the challenges is around automation. I'm a firm believer --- and there's people that disagree --- that you're going to have to do some amount of automation in your development process as you move toward an Agile and, certainly, a DevOps process, because you're trying to accelerate delivery, because you want to be able to refactor the code as you go. You have to refactor the code in an incremental model. You're going to need a regression suite at least, and you want that regression suite to be, as much as possible, automated, otherwise you've got a huge block in your process.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, the question is figuring out, “Well, what do I spend effort and time automating for this process?” And I always look at --- there's a nice model out there, there's other models as well, that asks you to look hard at both your test suites that you have and specific tests in those test suites, and ask kind of three important questions.&lt;/p&gt;
&lt;p dir="ltr"&gt;The first one is, how important is this test? If it fails, what happens? So, is it catching a critical issue that our customers just --- we just can't ever have them see? Those tests need to be run repeatedly, and that means they should probably be automated if at all possible. So prioritize your tests, prioritize your suites, prioritize by features, and automate the things that are most important that are automatable is point one.&lt;/p&gt;
&lt;p dir="ltr"&gt;Point two is, those tests, though, have to be reliable. Nothing is worse than automated tests that you have to get on and figure out whether it passed or failed because sometimes it works, sometimes it doesn't, they're flaky, or there's some manual effort involved in the process, or there's some false positives that pop up. You want these tests to be reliable. You want them to run and give you the same result every time. They need to be reliable and not have a lot of human intervention if we're going to use them in an automated process.&lt;/p&gt;
&lt;p dir="ltr"&gt;The last is, they should be specific. That just means they're testing one particular thing. They're not catch-all tests trying to cover a lot of territory all at once. We want them to test one thing. We want them to be independent so we can run them in different orders, right? We don't want tests to rely on other tests if at all possible, because that constrains our ability to reorder them, to automate some of them and not automate others, to parallelize them and run them faster in parallel maybe in the cloud. That's a hot topic right now. So, they need to be specific, and they need to be independent.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, I always tell people, look at those three aspects of your tests, and pick out the ones that you think make the most sense. Start with those, and iteratively add to your suite as you can afford to.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Ok, Jeff, final sprint here, so I have a few quick questions for you. In one sentence, what does digital quality mean to you?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: So that's a term you hear a lot now, right? Digital quality, digital transformation. So, digital quality to me, is really assuring the success of a customer journey or a customer engagement. I feel like it's all about the customer, and the quality of that customer experience for an organization's kind of comprehensive digital platform. Anything that touches customers --- how do I make sure that customer journey is successful? [That’s] the way I would characterize digital quality.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: What will digital experiences look like five years from now?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Well, yeah, I think the whole goal in the digital transformation, digital quality movement to me is getting a better understanding of our customers and understanding, how they engage with our products and come through different touchpoints --- web, mobile, or whatever it is. And I feel like there's a lot of data being collected, and organizations are trying to use that data to make better decisions and give consumers better results and better options, and, obviously, sell them more. I really feel like AI and using artificial intelligence to take that data and start to really understand customer trends and customer needs is going to make the experience for customers a lot clearer and make a lot better customer recommendations and purchasing recommendations --- using non-AI types of analysis just isn't yet providing that level of sophistication.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, I think that's going to drive more value. It's going to drive more customer satisfaction. It's going to increase customer engagement, and should increase customer revenue for companies that adopt that. So, I think AI is going to radically change the customer experience over time.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: What's your favorite app to use in your downtime?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: That's a great question. So, I code for fun. I always tell people I got into software because it was what I like to do. It was a hobby when I was a teenager. And I figured if you're going to get paid to do something, why not get paid to do something you like to do?&lt;/p&gt;
&lt;p dir="ltr"&gt;I don't code in my job anymore. I haven't written a piece of code in one of my companies in so long it's embarrassing to mention. But, I code on the weekends. So, I'd say a good Python environment's probably my favorite app because I do code in Python for fun.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: What's something that you're hopeful for?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Yeah, so you mentioned you were going to ask this question. This is probably the question I've spent the most time thinking about because it's --- you could go a lot of different directions with this.&lt;/p&gt;
&lt;p dir="ltr"&gt;One of my hot buttons is, software is being integrated into every product. It's being integrated into every business. It's becoming more and more business-critical, mission-critical. Yet so many of our organizations are not run by technologists. They're run by people who don't understand software at all. And I deal with them every day.&lt;/p&gt;
&lt;p dir="ltr"&gt;I feel like the world would be so much better --- and I'm biased, of course --- if organizations were run by technology-oriented people who understood software. They would make better decisions; they would invest in the right places; and they would just better understand their products than currently some organizations do.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, my goal is, and my wish is that, over time, organizations recognize that people who understand software and understand technology make great senior executives, because they're going to make the right decisions for the organization and the shareholders. And I hope that comes to pass. We're not there definitely yet.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Will we get there eventually?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: I almost feel like we have to, because I feel like organizations that do go that direction, I feel like there'll be hits and misses, but over time, they will be more successful, I personally believe, because they'll better understand their products, and that means they'll better understand their customers, and should make it more successful. And, at some point, it'll tip and it will become the in fashion thing, right? Like, everybody is hiring a tech CEO, or whatever. It'll become the in thing, I hope.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Well, Jeff, I always appreciate talking with you. Next time we speak, it might have to be about sports memorabilia or cards, but I hope that we get to do that again soon.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Payne&lt;/strong&gt;: Yeah, no, this has been great. Thank you, David. I appreciate it.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: That was our conversation with Jeff Payne, CEO of Coveros. I always enjoy talking with Jeff, and I hope to get a peek at his vintage football collection again in the future. Thank you for tuning into this episode. Thanks as well to our producers, Joe Stella and Samsu Sallah and graphic designer Karley Searles. Feel free to reach out at podcasts@applause.com. That's plural, podcasts@applause.com. And we will catch you next time.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</podcastTranscript><resourceImage><item>897057</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Listen Now</resourceButtonText><resourceButtonUrl>/resources/podcasts/ep3-why-so-ceremonious</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2022-11-03T16:50:30-04:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle>{{ seomatic.helper.extractTextFromField(entry.title) }}</seoTitle><siteNamePosition></siteNamePosition><seoDescription>On this episode we discuss what real Agile testing looks like. It’s not a ceremonious approach to a Waterfall — it’s about substantive organizational change.</seoDescription><seoKeywords></seoKeywords><seoImage></seoImage><seoImageWidth></seoImageWidth><seoImageHeight></seoImageHeight><seoImageDescription></seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><seoImage>true</seoImage><seoImageDescription>true</seoImageDescription><canonicalUrl>true</canonicalUrl></inherited><overrides><seoTitle>true</seoTitle><seoDescription>true</seoDescription><robots>true</robots></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets>false</sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAssets>true</sitemapAssets><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromField</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds></seoImageIds><seoImageSource>fromAsset</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item><item><sectionId>58</sectionId><postDate>2022-10-31T17:09:00-04:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>4618</_authorId><id>897017</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>c7f3e63a-25a2-4692-981e-feaf238a49d2</uid><siteSettingsId>1351978</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>526625</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>UX Metrics That Actually Make Sense</title><slug>ep2-ux-metrics-that-make-sense</slug><uri>resources/podcasts/ep2-ux-metrics-that-make-sense</uri><dateCreated>2022-10-31T16:50:44-04:00</dateCreated><dateUpdated>2022-11-03T16:50:01-04:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>897017</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep2-ux-metrics-that-make-sense</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep2-ux-metrics-that-make-sense</url><authorId>4618</authorId><typeId>96</typeId><description>Some businesses have no clue how to measure the user experience — and it shows. In this episode of the Ready, Test, Go. podcast, we talk about how to build UX awareness from the ground up.</description><publishDate>2022-11-02 04:00:00</publishDate><episodeNumber>2</episodeNumber><episodeLength>27</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>sbh1rh97t7</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p dir="ltr"&gt;Some businesses have absolutely no clue how to measure the user experience — and it shows. Whether they’re measuring the wrong things from the start, beginning too late in the development cycle or outright abandoning UX projects, some brands consistently fail to create a high-quality customer experience.&lt;/p&gt;
&lt;p dir="ltr"&gt;Inge De Bleecker, Principal UX &amp;amp; Conversational AI Consultant and Founder at outriderUX, has seen the best and worst of it. In this episode of the Ready, Test, Go. podcast, she talks about how to build UX awareness from the ground up. It starts with having the right voices in the room to represent the necessary perspectives, which can help organizations build out cross-functional teams and actual organizational buy-in.&lt;/p&gt;
&lt;p dir="ltr"&gt;She also talks about co-creating the USERindex score to measure the user experience on a deeper level for the mobile age.&lt;br /&gt;&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>Inge De Bleecker</podcastGuestName><podcastGuestPhoto><item>897025</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p dir="ltr"&gt;For three decades, Inge De Bleecker has designed, developed, managed and consulted on the user experience. Inge has spent her career focusing on the user and making the user’s life easier when interacting with digital products.&lt;br /&gt;&lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p dir="ltr"&gt;&lt;em&gt;(This transcript has been edited for brevity.)&lt;/em&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;David Carty:&lt;/strong&gt; Everyone knows the iPhone changed the world. Whether you wanted to video chat with a friend across the world or just look at cat videos on your way to work, the iPhone changed our way of interacting with the world.&lt;/p&gt;
&lt;p dir="ltr"&gt;As it turns out, it also changed the world of usability benchmarking. The iPhone revealed an unmet need, so Inge De Bleecker co-created the USERIndex. The letters stand for…&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Inge De Bleecker:&lt;/strong&gt; Usefulness, Satisfaction, Ease of use and Reliability.&lt;/p&gt;
&lt;p dir="ltr"&gt;So those are the four dimensions that we measure in the USERIndex. And that does give a bit more of a comprehensive overview of sort of what today, I guess we define as the user's experience with the iPhone. It wasn't just any more about ease of use and learn ability, but now it was about delight. It was it was just about that satisfaction. And it just became about more. So measuring a user experience just became something that we felt needed more dimensions.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;The open source USERIndex poses ten naturally worded questions to users to gauge their experiences with a digital interface.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;We did a number of experiments just trying to fine-tune the statements and the way we calculate the scores and then also what those scores mean because what does a 3.9 mean versus a 4.2? So we have a nice scale all the way from green to red with orange in the middle. That kind of helps give an understanding of where you truly sit. In addition, of course, we have the historical data, which by now is quite significant from eight years’ worth of studies that also help companies understand where they sit within their industry in terms of their score.&lt;/p&gt;
&lt;p dir="ltr"&gt;We also use it for inclusive experiences. We've tested it out on anything from websites to mobile apps to conversational experiences. So it's quite versatile as a high-level benchmark.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty; &lt;/strong&gt;And the most rewarding part for Inge is hearing second-hand how it is helping brands all over the world.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;It's a little bit hard for us to know. We do definitely get emails from either people who are interested or people say, "Oh yeah, I've been using the USERIndex and it's really been working out well." A consultant at Porsche came to us, this is about two years ago, and he said, "You know, I am evaluating our benchmarking score for Porsche. And I looked at a number of different ones and your score, the statements, they really hit the values that the brand is going for. And because of that, I've recommended your score." And so that really kind of hit something where it's like, okay, yes, these statements are relevant to the user experience, so relevant, they're relevant to the brands themselves.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;This is the Ready, Test, Go. podcast brought to you by applause. I'm David Carty.&lt;/p&gt;
&lt;p dir="ltr"&gt;Today's guest is UX expert Inge De Bleecker. Inge is a UX designer, a UX researcher. She has built and managed UX teams and been the driving force behind UX process implementation across organizations. Inge is the founder of outriderUX and also its principal UX and conversational AI consultant. Over 30 years in the business, she's seen it all the good, the bad and the ugly when it comes to UK strategy. And that's what we spoke about.&lt;/p&gt;
&lt;p dir="ltr"&gt;Inge, thank you for joining us. Let's start by setting the stage. Why do we want to capture usability metrics? This takes time, investment and effort. So what's the benefit and why do we go to the trouble in the first place?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker:&lt;/strong&gt; It does indeed. It takes time, investment and effort. Absolutely. Why do we want to measure it? Because we want to understand our progress. We start with a certain baseline, obviously, after having identified what the metrics are that we want to actually keep track of, very important part. And once we have that, we set a baseline. Where are we at right now? And we understand the goal. Where do we want to go? And so the way to gauge that is by doing something that is metrics- or data-driven. The other advantage of doing that is that hopefully there is improvement. Right. You are working towards your goal. Now there's a very clear way to communicate progress, to communicate let's call it success. And so if you are part of a team that is working towards improving that user experience and you need more budget, you need a tool, you need whatever you may need, now, you really do have some hard facts to go to an executive team and a manager and say, "Look, this is what we've done so far. This is good stuff. Give us some more."&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;We live in a data-driven world now more than ever. Data drives business decisions, hiring decisions, even personal decisions, right? So with that in mind, explain to us the importance of not only measuring usability in a comprehensive way, but also in a comprehensible way that everybody in the business can understand.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;Yeah, that's a big challenge. I think it's a challenge with data anyways. So [with] data, you've got numbers and then really right away it's like, “Well, okay, I've got numbers. What do they mean? What does this really represent?” The key there is to be able to provide the right nuances behind the numbers and to also look at the numbers in the right way.&lt;/p&gt;
&lt;p dir="ltr"&gt;I actually worked with a client earlier this year, and they were very interested in a metrics-driven approach to their product development. So we set up a quite comprehensive strategy for that, and it was great. But, then as we started measuring early on in product development, we got this super high numbers like, oh, yes, you know, "90%. 100%. We're there!" Clearly, of course, we were not there. We're still very early in development. So what happened was, say, there was somebody who was who was working against a script. Doing QA against the script. You know, they've got 100%. Well, they'd better! You know, that was exactly what was laid out. That is very different from measuring a user out in the wild using the product. At the end of the day, those targets that we want to get to are for the user experience out in the wild.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, it does really need to be nuanced. It needs to be explained. The first thing I said is that, look, you know, we cannot go to the executive management with these numbers. It's misrepresenting things. So, it needs to be comprehensible. It needs to be meaningful across the board. And, more often than not, you actually need some qualitative information, some user feedback or some additional background information in order to be able to understand what those numbers truly mean.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;And you work with clients all over the world, right? So you must see this somewhat often, well-intentioned efforts around usability and UX, but they miss the mark. Any other examples come to mind of how you see that in the real world?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;Yeah, so as I was alluding to just earlier is it's not only about the measuring, it's also about what you measure. That is where, very often, is where it goes wrong in the first place. That really is something that does require thought.&lt;/p&gt;
&lt;p dir="ltr"&gt;I've run workshops with clients where we were going to have a workshop, we were going to talk about what metrics, the metrics strategy, what metrics to focus on, and how to collect these and etc. I give them the homework of thinking about their company's KPIs because at the end of the day, when you think about, "What do I want to measure?" A part of it is the user, and definitely part of it is a business as well. And then there's technology and a number of different aspects as well. But if we just think about the user and the business, that that is where I'm like, "All right, come to us with your KPIs. What is your business focused on? What does success mean for this product, for your business?" And it is amazing to me how that is very difficult for teams, especially in larger corporations. It's very difficult for teams to come to the table with that, to really start building out a meaningful set of metrics. And, again, that's step zero, honestly, right? If you don't get that right…&lt;/p&gt;
&lt;p dir="ltr"&gt;It's really interesting because just this morning I was on LinkedIn and I saw a thread. It was about metrics, and gathering metrics, and the comments were all about how the metrics are measuring the wrong things more often than not and are not understandable. So, this does seem to be a very systemic problem throughout the industry.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;As part of this, not having the right voices in the room. When you're deciding what to measure in the first place, who should who should be in the room to help define those metrics that the business will ultimately measure and make business decisions against?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker:&lt;/strong&gt; Yeah, absolutely. You need a lot of different people in the room because you need all those different voices to be heard. The user’s voice, the business voice, and at times the metrics and sort of the decisions on what is success will have to include a compromise between some of those voices. I mean, when you think about sort of IVR systems, for instance, you've got the user experience that's important, but then you also have the containment rates and those business constraints that are important as well. So you do need to come to sort of a comprehensive overview. To do that, you really do want all the different stakeholders across the board, including the engineers, including the product managers, UX, business, everybody in the room from the start.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;Do you find yourself being the negotiator between two sides a lot? Is that is very conflict driven? Are you put in that position a lot?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;There's definitely an aspect of that. Now, depending on the organizations, that may be easier or more difficult to navigate. But, yeah, absolutely, at least making sure that there's an awareness across the room, right? A mutual awareness of the importance of all of these different aspects, because everybody tends to think from their own silo and they're also quite focused on their objectives. But, at the end of the day, and this is kind of where it comes down to the customer experience, right? Something that really has to be driven across the board, across the different parts of the organization.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;Ideally, what perspectives do you like to have in the room to help give input toward this problem? Is there anybody that's commonly left out or anything like that?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker:&lt;/strong&gt; I think the UX team is probably commonly left out, or at least that's a voice I hear a lot. Yeah. So I think it's, it's pretty common for the product team and the business side to come together and understand that this is an effort that needs to happen. And I think it's sort of the other, to some extent maybe more auxiliary, in a way, teams that are left out or people just, people just don't think about it. That really, I think largely can be the case is just people just don't think about the fact that more different people really should be involved from the start.&lt;/p&gt;
&lt;p dir="ltr"&gt;That's the other part of it, right? I mean, we're really talking about defining these metrics very early on. So who's involved at that point in time? It is very often largely the product teams, the business side.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Bias is going to come into play. Institutional thinking is going to come into play, right? So what are some ways that we can reduce or eliminate those issues?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;Communication. You know, as difficult as sometimes it is, and I know that these things take time, they're some of the efforts that I've suggested, and [what] teams have adopted is a committee of sorts. So a sort of a cross-functional committee to come together. I mean, [there’s] pros and cons there as well, because everybody's already very overburdened. They don't need another meeting for something that may not be directly in their daily line of sight. So, there's that, of course. And then I think just in general, HR, for instance, can play a role in fostering across the across the board some communication and inclusion, right? Inclusion in the customer experience. Again, that's sort of what it all comes down to at the end of the day. And, also, you have to have executive buy-in, a strong executive voice. That definitely is very helpful as well.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt; How soon in a life cycle can you begin to gather this kind of feedback, and are businesses missing an opportunity by not shifting usability testing left in the lifecycle?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;So usability testing should absolutely be as left as possible in the life cycle. As soon as you have a prototype. Start usability testing now.&lt;/p&gt;
&lt;p dir="ltr"&gt;In terms of gathering the data, it's a good practice to start gathering the data. But, it is, as I mentioned earlier, with the example of my client, that data does need to be taken with a grain of salt. In terms of the bigger picture. But at least it gives you an opportunity to, again, get those best practices in there and maybe refine your metrics as well, just because you have to put these metrics together at the very beginning so that you can use them throughout. It's sort of a big task because you'd like to refine it a little bit. It gives you a little bit of an opportunity to refine as you go early on. Not quite high stakes. You won't have to go up on stage and say, "Oh, yeah, well, we don't have data for that particular time there because we were still fiddling with it." So, that's sort of an advantage and a disadvantage there.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: You spoke before about getting executive buy-in. I'm sure that one of the big obstacles to process change is just that internal resistance in that kind of inertia that you have to work through. How do you go about handling that and getting people on board with the program?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;Again, a lot of communication, evangelizing, raising awareness, whether that's through campaigns, brown bags, any and every way possible. It very much depends on the type of organization, sort of the way the organization works. What are they most receptive to? [There are] actually a lot of teams these days are very distributed even without their tools. What are they using as tools? What do they already have in place in terms of monthly meetings, knowledge sharing, things like that? And then it’s] just really gauging what people are open to. You've got some teams that are actually very open to just learning new things and embracing that, have some sense of innovation, I guess, within the company.&lt;/p&gt;
&lt;p dir="ltr"&gt;So you kind of look at what the company values are, and what's already underway and then you kind of pick and choose a little bit and kind of tack on to that. That's for sure the best way to gain traction if you're starting something completely from scratch. It just takes a longer time. And there's a lot more risk of things just dying. You start something up and then, poof, and it's done. Maybe you did some good, right? But that may be not so.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt; So, really, aligning it behind shared business goals is super important there, right?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;Yeah, shared business goals. Just understanding what do business, the employee experience, what the business focuses on in that, all of those.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Great. Now, it's one thing to put together a logical strategy, and it's another to execute it at scale. So what kinds of challenges do organizations face with scaling usability, data collection and analysis as they're launching new markets and add to their products?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;Yeah, so new markets are definitely, if we're thinking globally, a really interesting topic. Specifically also because if you're looking at, if you're gathering usability metrics or you're really gathering user feedback in a data driven way, what we've seen time and time again is that users will rate things differently in different cultures, whether it's because they're more positively inclined, more negative, and negatively inclined, they're too polite.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;So not over here, right? Not in our country.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;The British can be very polite, you know, and some European countries will be a little more straightforward. And it's really fascinating because I've seen, especially in studies where you have the quantitative aspects, you've got the data, and you also have the qualitative user feedback, and you run that on the same products in different countries. If you have a little bit of a sample size, you really have a nice little, little project there in terms of really looking at truly what it means and that, that is what we see over and over again.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, that's, again, the metrics are not the end all, be all in a sense. They're just one tool, right? They're one tool that needs to be handled carefully, but if handled correctly can be very helpful. And, yeah, looking at, for instance, a global audience is very interesting. Then things like, if you've previously looked at your entire product and now you're looking at just one new feature that's different, right? So again, sort of keeping all of that in mind.&lt;/p&gt;
&lt;p dir="ltr"&gt;Then I do run into clients who say, "Well, what's the point then, at the end of the day, right? What am I truly going to learn from this?" And that, again, is where I sort of come in and say, "Okay, I understand this is not magic." Nothing is magic. So just use it, use it wisely. And that's the goal there, really.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;So you have to fight that nihilistic perspective on top of everything else, "Well, then why try in the first place?"&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;Yeah, and that's okay. I mean, I think it only helps to be able to explain how or where the value lies. So, that's all right. I don't mind those conversations.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Sure. Now, if you're consistently failing to see ROI on your usability or UX efforts, is it time to take out the wrecking ball? How should organizations rebuild their approach from the ground up?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker:&lt;/strong&gt; Yeah, if there's consistent issues, then there are issues, no doubt. My suggestion would be to perform an assessment across the board to really understand where the challenges lie. It can be anything — very many things. It could be a lack of bandwidth for, just people don't have enough time to do X, Y or Z. It could be that the metrics are wrong. I mean, there are so many things, factors, and it's probably a combination of a number of factors.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, you know, even if the answer is to rebuild, which I think it's just generally more [about] fixing than completely rebuilding, but even so, you would want to know where exactly the challenges were so that you can avoid those in the future. So an independent assessment from literally an outside vendor or an outside set of eyes, that is definitely a good first step.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;So that you don't fail all over again.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;That's right.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;Moving on to our final sprint questions here, our lightning round. In one sentence, what does digital quality mean to you?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;So, to me, it means that a digital product works. [That means], as a user, I don't encounter any bugs. It is easy to use. So usability is there. And it is appealing to use. Something that people will feel that they like using it. Maybe that's a bit of a bonus, but if we can get there, then I would say that that's, to me, digital quality. There's other aspects [too], obviously performance, things like that that will have an impact on my user experience.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;What will digital experiences look like five years from now?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;Yeah, that's always a great question, isn't it?&lt;/p&gt;
&lt;p dir="ltr"&gt;So, on the one hand, David, it is going to look completely different. I mean, it is going to be nothing like what we do today. We are going to be talking to everything. AI is going to be everywhere, right?&lt;/p&gt;
&lt;p dir="ltr"&gt;No, it's not. I mean, that's sort of you know — because the other side is that, I think five years from now, in many ways, very little will have changed. We will still be using the websites we use today to check our email [or], I don't know, to submit a claim.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;So a little bit in the middle there somewhere?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker: &lt;/strong&gt;I think so. I think you'll continue to see sort of the mainstream of what we already have.&lt;/p&gt;
&lt;p dir="ltr"&gt;I think submitting a claim will only become easier. Little bits, because by now we're quite mature on some of that side of the house, so to speak, when it comes to innovation. When it comes to AI, I think we'll continue to see a lot of different experiments, I call them, to some extent, or trying to do things a certain novel way, some of them will land, and some of them will not.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, you know, at the end of the day, where do we end up there five years from now? Probably some progress. But, no, the world is not going to have completely changed.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;Not exactly flying cars and things like that.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker:&lt;/strong&gt; I know, that was exactly what I was thinking about when I was saying this. Yeah, no, not yet. Still, not yet.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;We'll keep an eye out. You never know.&lt;/p&gt;
&lt;p dir="ltr"&gt;What is something that you are hopeful for?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker:&lt;/strong&gt; I would like to think I call it discipline. I'm now specifically going to be talking about conversational AI and that industry because it is a little bit newer, a little bit more nascent, less mature. So what we see when we think about digital products in general is, we have the frameworks, we have the processes to do the software development lifecycle, right? We've been doing this — it doesn't really matter what type of product it is, it largely can be applied, right? So, we know how to do these things. We know what the right way to do things is.&lt;/p&gt;
&lt;p dir="ltr"&gt;I think we lack discipline. I see that then, especially in something like conversational AI where it's still very innovative. But, at the end of the day, if we want to develop those products in a way that they are successful, they're great products to use, we're just going to have to be a little bit more disciplined and truly go through the right processes, apply the frameworks, and just do it right rather than doing something kind of haphazard, not quite right.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Inge De Bleecker, thank you so much for joining us today.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;De Bleecker&lt;/strong&gt;: Thank you so much for having me, David.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: That was our conversation with Inge De Bleecker, co-founder of outriderUX. Really interesting stuff.&lt;/p&gt;
&lt;p dir="ltr"&gt;Thank you for tuning into this episode. Thanks as well to our producers Joe Stella and Samsu Sallah and graphic designer Karley Searles.&lt;/p&gt;
&lt;p&gt;If you'd like to reach out, please contact us at &lt;a href="mailto:podcasts@applause.com"&gt;podcasts@applause.com&lt;/a&gt;. That's plural, &lt;a href="mailto:podcasts@applause.com"&gt;podcasts@applause.com&lt;/a&gt;. We'll catch you next time.&lt;/p&gt;</podcastTranscript><resourceImage><item>897027</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Listen Now</resourceButtonText><resourceButtonUrl>/resources/podcasts/ep2-ux-metrics-that-make-sense</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2022-11-03T16:50:01-04:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle>Ready, Test, Go. Ep. 2 UX Metrics That Actually Make Sense</seoTitle><siteNamePosition></siteNamePosition><seoDescription>Some businesses have no clue how to measure the user experience — and it shows. In this episode of the Ready, Test, Go. podcast, we talk about how to build UX awareness from the ground up.</seoDescription><seoKeywords></seoKeywords><seoImage></seoImage><seoImageWidth></seoImageWidth><seoImageHeight></seoImageHeight><seoImageDescription></seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><seoImage>true</seoImage><seoImageDescription>true</seoImageDescription><canonicalUrl>true</canonicalUrl></inherited><overrides><seoTitle>true</seoTitle><seoDescription>true</seoDescription><robots>true</robots></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets>false</sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAssets>true</sitemapAssets><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromCustom</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds></seoImageIds><seoImageSource>fromAsset</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item><item><sectionId>58</sectionId><postDate>2022-10-27T16:01:00-04:00</postDate><expiryDate></expiryDate><deletedWithEntryType>false</deletedWithEntryType><_authorId>4618</_authorId><id>896297</id><tempId></tempId><draftId></draftId><revisionId></revisionId><isProvisionalDraft>false</isProvisionalDraft><uid>9b424c9b-6aa1-40af-9973-c64994bfc2c0</uid><siteSettingsId>1350704</siteSettingsId><fieldLayoutId></fieldLayoutId><contentId>525690</contentId><enabled>true</enabled><archived>false</archived><siteId>1</siteId><title>Testing in Two Days</title><slug>ep1-testing-two-days</slug><uri>resources/podcasts/ep1-testing-two-days</uri><dateCreated>2022-10-27T16:00:25-04:00</dateCreated><dateUpdated>2022-12-19T13:52:14-05:00</dateUpdated><dateLastMerged></dateLastMerged><dateDeleted></dateDeleted><trashed>false</trashed><isNewForSite>false</isNewForSite><canonicalId>896297</canonicalId><isDraft>false</isDraft><isRevision>false</isRevision><isUnpublishedDraft>false</isUnpublishedDraft><ref>resources/ep1-testing-two-days</ref><status>live</status><structureId></structureId><url>https://www.applause.com/resources/podcasts/ep1-testing-two-days</url><authorId>4618</authorId><typeId>96</typeId><description>For software testers, it’s always crunch time. Striking the balance between effective and expedient testing isn’t always easy, but it’s necessary in our fast-moving digital world.</description><publishDate>2022-11-02 04:00:00</publishDate><episodeNumber>1</episodeNumber><episodeLength>18</episodeLength><wistiaVideo><item><type>video</type><enabled>true</enabled><collapsed>false</collapsed><fields><videoId>30g94qlivi</videoId></fields></item></wistiaVideo><podcastLinks><item><type>106</type><fields><applePodcasts>https://podcasts.apple.com/us/podcast/ready-test-go/id1647403384</applePodcasts><googlePodcasts>https://podcasts.google.com/feed/aHR0cHM6Ly9mYXN0Lndpc3RpYS5jb20vY2hhbm5lbHMvMWI4NDYybHQwcS9yc3M</googlePodcasts><spotify>https://open.spotify.com/show/78JBYDrzD1Z18L3CxF6nSL</spotify><castbox>https://castbox.fm/channel/id5115078</castbox><podcastAddict>https://podcastaddict.com/podcast/4105546</podcastAddict><stitcher>https://www.stitcher.com/show/1027735</stitcher></fields></item></podcastLinks><podcastAbout>&lt;p dir="ltr"&gt;For software testers, it’s always crunch time. Striking the balance between effective and expedient testing isn’t always easy, but it’s necessary in our increasingly fast-moving digital world. Digital quality must never come at the expense of speed. Cutting back on quality always comes with consequences that directly affect the business and the bottom line.&lt;/p&gt;
&lt;p dir="ltr"&gt;Amy Reichert, freelance QA SME, tutor and writer, has made a career of testing efficiently and effectively. She’s felt the crunch, and she’s seen fellow testers thrive or collapse under the pressure. &lt;/p&gt;
&lt;p dir="ltr"&gt;Amy joins the Ready, Test, Go. podcast to discuss how to fit all of your testing into increasingly tight windows. She also discusses how to best work with developers to create a shared quality-first culture that benefits organizations well into the future.&lt;br /&gt;&lt;/p&gt;</podcastAbout><podcastGuest><item><type>107</type><fields><podcastGuestName>Amy Reichert</podcastGuestName><podcastGuestPhoto><item>728996</item></podcastGuestPhoto><podcastGuestBlurb>&lt;p dir="ltr"&gt;Amy Reichert is a freelance QA engineer SME tutor and consultant. She has nearly two decades of experience in software testing, as an analyst, team lead, engineer and tutor. Amy is also a freelance technical writer, including for Applause, where she writes about software testing topics.&lt;br /&gt;&lt;/p&gt;</podcastGuestBlurb></fields></item></podcastGuest><podcastTranscript>&lt;p dir="ltr"&gt;&lt;em&gt;(This transcript has been edited for brevity.)&lt;/em&gt;&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;David Carty&lt;/strong&gt;: Over two decades in software testing, Amy Reichert has trained the best of them.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Amy Reichert&lt;/strong&gt;: Well, I guess, with perseverance and a calm and assertive attitude.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: She knows how to tap into their strengths.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Well, you have to keep in mind, when you're training, you can't outmuscle one of them. So regardless of who you are, you're not going to force one to do anything.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: She even knows the power of motivation, and snacks can go a long way.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Sliced apples, carrots. They make cookies and treats.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Oh, right. We're talking about horses. Amy Reichert also trains horses. Probably should've led with that.&lt;/p&gt;
&lt;p dir="ltr"&gt;At any rate, just like managing software testers, horses have different personalities and require different ways of communicating.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: One has broken just about every piece of equipment I had just to go in a simple circle to the left because they were so right handed. They refused to go to the left. He broke everything I own, from the bridle to the reins, to the harness to everything. Just because they didn't want to go left.&lt;/p&gt;
&lt;p dir="ltr"&gt;You have to be clever, not necessarily manipulative, but clever in communicating and encouraging them to do what you want them to do, without having to force them.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Amy's training was in dressage, which involves very specific motions and movements. Perhaps that's not too dissimilar from a software launch. A simple, yet coordinated balance of tasks falling right into place, just so.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Everything is very focused on finesse. You don't see the cues. You don't really see the rider doing anything. You don't see all the work they're putting into it. You just see the reaction of the horse and the steps that they take.&lt;/p&gt;
&lt;p dir="ltr"&gt;So, [that’s] similar to testing or release. You don't see the actions of the testers. So you see a little bit of the developer's actions, but mostly you don't see all the background work that goes into it, like support and testing.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: This is the Ready, Test, Go. podcast brought to you by Applause. I'm David Carty.&lt;/p&gt;
&lt;p dir="ltr"&gt;Today's guest is horse wrangler and software testing expert Amy Reichert. Over the years, Amy has done it all, working as an analyst, team lead, engineer and tutor. Amy prides herself on performing thorough testing in half of the allotted time. Now she is a QA tutor and consultant for DevMountain and a writer, teaching the next generation of testers how to get their work done more efficiently. Let's talk with Amy.&lt;/p&gt;
&lt;p dir="ltr"&gt;Today, we're talking about having effective two-day testing periods. But, first, I'd like to get an idea of the industry trend. So, what has that been like in terms of how much testing time an organization has and how dramatically has that been reduced over the years?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: When I first started, like 20 some years ago, we were more in a Waterfall methodology, so you would do a much larger release. So there was much more planning, much more documentation, the coding period and then the testing period. So you'd get a month [or] 4 to 6 weeks to actually test an application through. And you'd be testing a lot more stuff, but you'd have enough time to cover it — usually.&lt;/p&gt;
&lt;p dir="ltr"&gt;Over the years, however, though, as you moved into Agile methodology and then continuous deployment — even faster than Agile — you get less and less time. So with Agile, if you have any regression testing at all, it's usually much condensed. It's gone anywhere from a week [to] five days, ten days. And, to me, the most common amount of time I actually get is about two days, tops.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Gotcha. And you can't test everything in this time frame, right? That's for sure. So how should you begin to prioritize those tests to be able to make sure you're getting the most important things done?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Well, I think it depends on the experience of your team. So if you have an experienced testing team, or at least maybe not so much experience, but familiarity with the application, both the back end and the front end — the more experience or understanding of the application the tester has [enables them to] go off script and basically use different techniques. So instead of testing script by script by script and prioritizing those scripts, you can prioritize the functional areas you want to test.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Because they know where the bodies are buried, right? They know where the problems are with the product. So they can begin to kind of probe in ways that maybe a lesser, less-experienced tester might not be able to jump into and try that out.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Yes, exactly. The more familiar [they are] with where the fragile parts of the code is, where the parts of the code that customers use the most, and where those kind of fragile gaps are between connections or integrated pieces of the application.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: And you mentioned the customer component, the user component as well. You can prioritize according to customer feedback. So, you know what sorts of defects or areas might take precedence there?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Well, I would usually like to talk to a support person who's familiar with the app, who's taking in the calls from customers and find out just how annoying certain sections of the application are. Where do they find the most defects? What are the most critical defects they find? And make sure you always look for those so they don't repeat. No one wants to experience it twice.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Let's talk about running some tests parallel with development. Now, this often means that a developer is running unit tests, which they might want to do or might not want to do. So how can you work to build trust between developers and testers to get them talking and collaborating toward that same end goal and help the testing team really deal with that tight time crunch that we're talking about?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: I usually start by directly communicating with the developers. So finding out how they want to be contacted is a first step because some people do not like to be surprised. When you work in the office, you don't want to just walk over and show up. Do they prefer you to IM them, send them a text message, send them an email or schedule time with them? Whatever works for them so it doesn't interrupt their train of thought when they're coding.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: So, you're saying don't jump up from behind the computer screen and yell, ‘Surprise!’&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: No. No, they don't like that.&lt;/p&gt;
&lt;p dir="ltr"&gt;But if you contact them and and you show interest. So, like, if I've gone in and I've done some analysis on an issue, I see, for example, when we're testing in a short timeframe, and I think I see an issue, then I go to them and say, ‘Hey, is this really an issue or is this something in the setup of the test server? Can you show me how it works in the back end?’ And I think the more open you are to learning, but doing some of the work first, I think you get a better response.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: And how do you deal with it if you're not getting a lot of receptiveness back from the developer?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Well, often there's usually more than one. So I'll go to another developer and see if I can get further. Or I'll circle back and say, ‘Okay, give me some advice on what I can troubleshoot on my own,’ and see if that helps them. Because you'll get some developers who feel like, if you ask them a question, then they're doing the testing for you, which isn't really true. But I can do more research and then come back, and I will do that if I can. But, if not, I find another developer.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty:&lt;/strong&gt; Right. Might as well cast a wide net,&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Exactly.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty:&lt;/strong&gt; Great. So you're already rushing to get all these tests done in 48 hours. How does exploratory testing fit into that? Like we talked about, I'm sure if you're an experienced tester, you might have some particular areas that you might aim for based on your past experience with the product. But does it vary from one testing period to the next, or do you try to fit some of that in to run parallel with automated tests? What's the best way to approach that?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: What I really like to do is use exploratory testing in conjunction with automated smoke tests or automated tests, regression tests, if those exist. So when the developers are testing — I mean, we usually start with starting the automation, but we don't wait for it to just end. We just go ahead and jump in with the exploratory testing because automated tests will fail sometimes for no particular reason, and it sucks down a lot of time to figure out why they failed, if it's really for a script reason or an actual defect. So what we'll do is we'll kick them off, and then we will go ahead and start our exploratory testing. And that kind of helps you save time because then I can take the exploratory tests and use them to cover a lot more test coverage, if that makes sense. I can cover the UI, the back and the front end, all of it without interrupting my flow.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Right. And part of what we're talking about is the logistics issue where you just have so many tests to get through. But then you're also dealing with people, right? So, 48 hours, you hope everybody is up to the task. You hope everybody's ready to go. But, realistically, you could run into a motivation issue or some something similar to that [which] can really drag down productivity at a really tough time. So is there anything that you can do to incentivize or help push testers through the process, through to the finish line if they're running into difficulties?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: I kind of try to make a game out of it. So, you know, make a game out of -- not necessarily defects. A game out of, who can find the biggest defect? Who can find the most critical defect in the program? Or who can actually take down the system, make it hang? Make the connections fail? Whatever it is. Especially [with] security testing. That's always fun. Who can break into it, take it down?&lt;/p&gt;
&lt;p dir="ltr"&gt;So we kind of make a game out of it. It gives people some motivation because I can't usually offer them more money. And that's not always motivating [anyway]. Some people are motivated, again, like horses, with food, encouragement, vacations, whatever you want. But that's not always realistic. So I try to make a game out of it, and when people find really good defects, then at least internally to the testing team or the software development team, if I'm Agile, we celebrate that.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: So, kind of like a bug hunt.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Yeah, like a bug hunt. But we don't call it that because it offends developers. We call it a game or a challenge.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Right. Gotcha. So let's say we're hitting the stretch run, right? Final couple hours in the testing period and there is still so much to test. How do you hit the gas pedal and make sure you're getting to all that important stuff that's left before you get to launch?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: I think you just keep going .It takes some perseverance, and staying calm and not getting sucked into the chaos — and just steady on testing. Use your exploratory tests, have someone check on the automation. We just keep soldiering on, I guess you could call it.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Let's say you do have a hair-on-fire testing emergency, something that is definitely causing a problem, something out of the norm. How do you handle that? Is it possible to ask for more time? Under what circumstances do you ask for more time? How do you maneuver that situation?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Well, when you find a problem like that, where everyone's running around in circles with their hair on fire, you usually try to stop. Instead of having a developer merely jump in and fix it, we actually sit down as a team and discuss it, at least for a few minutes — make sure we're not making any obvious mistakes that we don't see right away that will cause more defects, and then we won't be able to release.&lt;/p&gt;
&lt;p dir="ltr"&gt;So depending on how long it takes to fix and test, you could ask for a day or half a day. Usually it depends on how bad the defect is, whether you get it or not. But if there's at least one of those hair-on-fire defects, then usually you can push for at least a few more hours to make sure it really is fixed and doesn't cause any additional repercussions.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;Right. So it might even sound counterintuitive, but in order to help get the job done, sometimes you have to slow down to make sure you're not making the situation worse.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Exactly. It's very important to slow down and make sure you talk it over before you make changes that may be based on the development experience or the tester experience. They don't understand the implications of [the defect], and to make sure that's well thought out before we fix it.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: Right. And I know you mentioned perseverance before, but if you could pick one key characteristic that a tester really needs to help deal with the pressure of these situations, because it can be pressure-packed sometimes, what is that characteristic that you would pick?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: I think the ability to remain calm. Not asleep, but calm and be able to keep chaos at bay. So, you can be calm and still working hard and doing your thing. Calm and focused, but you're not responding to the chaos or the hair-on-fire, and you're not going to panic or stress out.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;And let me ask a devil's advocate question, if you don't mind, Amy. We say that we're getting used to two-day testing periods .If it seems like [defects] are getting through that shouldn't be, or that testing is insufficient, how do you go about trying to find more time for future testing periods? How do you circle the wagons in order to get what you need to properly test the app and make sure things are ready for launch in the future?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert: &lt;/strong&gt;Well, I'm a big proponent of continuous testing, so continuously regression testing. So, when I am — say we don't get all the tests done, then when we're starting in our next sprint or next iteration, where we're waiting for code to come across or stories to come across for testing, then I'd like everyone to start regression testing. So let's finish all the tests we didn't finish, see if we find anything. And let's just take it, and create a test suite, and just start on it. And everybody runs, picks tests as they can, and runs them while we're going through development, so that you're always looking for defects, you're always looking for failures. You don't wait because there may be some times you don't get the two days. So, if you continuously test, there's less chance you might get a defect.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: And in what other ways are testers like horses?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Oh, stubborn. [They] can be stubborn. Well, I kind of agree, but you have testers who work really rapidly and can cover a lot of ground and do it productively, but they may miss some things. And then you'll have testers — or horses — who have to have every little thing in place. And if they take a piece of functionality, they have to break it down to the minute level. So, they're getting something tested, takes a great deal of time, but is actually more thoroughly done, if that makes sense. So, you have to balance between speed and thorough testing.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;Okay, Amy, lightning round here. I'm going to ask you a few quick questions. First off, in one sentence, can you tell me what digital quality means to you?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Digital quality means achieving exceptional customer experience with an application.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: But will digital experiences look like five years from now?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Well, I think you'll see people using applications easier on mobile devices primarily, and then you'll have a lot of intersection with AI or machine learning and virtual reality. Those will all be wrapped together to produce data and more data analytics, but I think that will be helpful. You'll get digital apps that maybe solve problems for you, or help you solve problems.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: What is your favorite app to use in your downtime?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Actually, the app I like the most — and it's still kind of clunky, but it's gotten better — is the ESPN Fantasy Football app. It's very handy. And like I said, it's still a little clunky, could use some improvement, but it's gotten a lot better.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty&lt;/strong&gt;: What is something that you are hopeful for?&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Oh, less tragedy, less war, or less distractions that don't need to be there.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;I hear you on that. All right, Amy. Well, this has been fun. Thank you so much for joining us.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Reichert&lt;/strong&gt;: Yeah, thank you for having me.&lt;/p&gt;
&lt;p dir="ltr"&gt;&lt;strong&gt;Carty: &lt;/strong&gt;I'd like to once again thank our guest, Amy Reichert. You can read some of her work at Applause.com/blog. We'd also like to thank you for tuning in to our first episode, and there's lots more to come. If you'd like to reach out, please contact us at podcasts@applause.com.That's plural, podcasts@applause.com. Until next time.&lt;br /&gt;&lt;/p&gt;</podcastTranscript><resourceImage><item>896434</item></resourceImage><resourceButton><item><type>resourceButton</type><enabled>true</enabled><collapsed>false</collapsed><fields><resourceButtonText>Listen Now</resourceButtonText><resourceButtonUrl>/resources/podcasts/ep1-testing-two-days</resourceButtonUrl><resourceButtonNewTab>false</resourceButtonNewTab></fields></item></resourceButton><featured>false</featured><visible>false</visible><cssClasses></cssClasses><seo><bundleVersion>1.0.23</bundleVersion><sourceBundleType>field</sourceBundleType><sourceId></sourceId><sourceName></sourceName><sourceHandle></sourceHandle><sourceType>field</sourceType><typeId></typeId><sourceTemplate></sourceTemplate><sourceSiteId></sourceSiteId><sourceAltSiteSettings/><sourceDateUpdated>2022-11-03T16:49:26-04:00</sourceDateUpdated><metaGlobalVars><language></language><mainEntityOfPage></mainEntityOfPage><seoTitle>Ready, Test, Go. Ep. 1 Testing in Two Days</seoTitle><siteNamePosition></siteNamePosition><seoDescription>Amy Reichert, QA SME, joins the Ready, Test, Go. podcast to discuss how to fit all of your testing into tight windows and how to create a quality-first culture.</seoDescription><seoKeywords></seoKeywords><seoImage></seoImage><seoImageWidth></seoImageWidth><seoImageHeight></seoImageHeight><seoImageDescription></seoImageDescription><canonicalUrl></canonicalUrl><robots>all</robots><ogType></ogType><ogTitle></ogTitle><ogSiteNamePosition></ogSiteNamePosition><ogDescription></ogDescription><ogImage></ogImage><ogImageWidth></ogImageWidth><ogImageHeight></ogImageHeight><ogImageDescription></ogImageDescription><twitterCard></twitterCard><twitterCreator></twitterCreator><twitterTitle></twitterTitle><twitterSiteNamePosition></twitterSiteNamePosition><twitterDescription></twitterDescription><twitterImage></twitterImage><twitterImageWidth></twitterImageWidth><twitterImageHeight></twitterImageHeight><twitterImageDescription></twitterImageDescription><inherited><seoImage>true</seoImage><seoImageDescription>true</seoImageDescription><canonicalUrl>true</canonicalUrl></inherited><overrides><seoTitle>true</seoTitle><seoDescription>true</seoDescription><robots>true</robots></overrides></metaGlobalVars><metaSiteVars><siteName>English</siteName><identity></identity><creator></creator><twitterHandle></twitterHandle><facebookProfileId></facebookProfileId><facebookAppId></facebookAppId><googleSiteVerification></googleSiteVerification><bingSiteVerification></bingSiteVerification><pinterestSiteVerification></pinterestSiteVerification><facebookSiteVerification></facebookSiteVerification><sameAsLinks/><siteLinksSearchTarget></siteLinksSearchTarget><siteLinksQueryInput></siteLinksQueryInput><referrer>no-referrer-when-downgrade</referrer><additionalSitemapUrls/><additionalSitemapUrlsDateUpdated></additionalSitemapUrlsDateUpdated><additionalSitemaps/></metaSiteVars><metaSitemapVars><sitemapUrls>false</sitemapUrls><sitemapAssets>false</sitemapAssets><sitemapFiles></sitemapFiles><sitemapAltLinks>false</sitemapAltLinks><sitemapChangeFreq></sitemapChangeFreq><sitemapPriority></sitemapPriority><sitemapLimit></sitemapLimit><structureDepth></structureDepth><sitemapImageFieldMap/><sitemapVideoFieldMap/><inherited><sitemapUrls>true</sitemapUrls><sitemapAssets>true</sitemapAssets><sitemapAltLinks>true</sitemapAltLinks></inherited><overrides/></metaSitemapVars><metaContainers><MetaTagContainergeneral><data/><name>General</name><description>General Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainergeneral><MetaTagContaineropengraph><data/><name>Facebook</name><description>Facebook OpenGraph Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>opengraph</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContaineropengraph><MetaTagContainertwitter><data/><name>Twitter</name><description>Twitter Card Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>twitter</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainertwitter><MetaTagContainermiscellaneous><data/><name>Miscellaneous</name><description>Miscellaneous Meta Tags</description><class>nystudio107\seomatic\models\MetaTagContainer</class><handle>miscellaneous</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTagContainermiscellaneous><MetaLinkContainergeneral><data/><name>General</name><description>Link Tags</description><class>nystudio107\seomatic\models\MetaLinkContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaLinkContainergeneral><MetaScriptContainergeneral><data/><position>1</position><name>General</name><description>Script Tags</description><class>nystudio107\seomatic\models\MetaScriptContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaScriptContainergeneral><MetaJsonLdContainergeneral><data/><name>General</name><description>JsonLd Tags</description><class>nystudio107\seomatic\models\MetaJsonLdContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaJsonLdContainergeneral><MetaTitleContainergeneral><data/><name>General</name><description>Meta Title Tag</description><class>nystudio107\seomatic\models\MetaTitleContainer</class><handle>general</handle><include>true</include><dependencies/><clearCache>false</clearCache></MetaTitleContainergeneral></metaContainers><redirectsContainer/><frontendTemplatesContainer><data/><name></name><description></description><class>nystudio107\seomatic\models\FrontendTemplateContainer</class><handle></handle><include>true</include><dependencies></dependencies><clearCache>false</clearCache></frontendTemplatesContainer><metaBundleSettings><siteType></siteType><siteSubType></siteSubType><siteSpecificType></siteSpecificType><seoTitleSource>fromCustom</seoTitleSource><seoTitleField>title</seoTitleField><siteNamePositionSource></siteNamePositionSource><seoDescriptionSource>fromCustom</seoDescriptionSource><seoDescriptionField>title</seoDescriptionField><seoKeywordsSource>fromCustom</seoKeywordsSource><seoKeywordsField></seoKeywordsField><seoImageIds></seoImageIds><seoImageSource>fromAsset</seoImageSource><seoImageField>resourceImage</seoImageField><seoImageTransform>1</seoImageTransform><seoImageTransformMode>crop</seoImageTransformMode><seoImageDescriptionSource>fromCustom</seoImageDescriptionSource><seoImageDescriptionField>title</seoImageDescriptionField><twitterCreatorSource></twitterCreatorSource><twitterCreatorField></twitterCreatorField><twitterTitleSource></twitterTitleSource><twitterTitleField></twitterTitleField><twitterSiteNamePositionSource></twitterSiteNamePositionSource><twitterDescriptionSource></twitterDescriptionSource><twitterDescriptionField></twitterDescriptionField><twitterImageIds/><twitterImageSource></twitterImageSource><twitterImageField></twitterImageField><twitterImageTransform>true</twitterImageTransform><twitterImageTransformMode>crop</twitterImageTransformMode><twitterImageDescriptionSource></twitterImageDescriptionSource><twitterImageDescriptionField></twitterImageDescriptionField><ogTitleSource></ogTitleSource><ogTitleField></ogTitleField><ogSiteNamePositionSource></ogSiteNamePositionSource><ogDescriptionSource></ogDescriptionSource><ogDescriptionField></ogDescriptionField><ogImageIds/><ogImageSource></ogImageSource><ogImageField></ogImageField><ogImageTransform>true</ogImageTransform><ogImageTransformMode>crop</ogImageTransformMode><ogImageDescriptionSource></ogImageDescriptionSource><ogImageDescriptionField></ogImageDescriptionField></metaBundleSettings></seo></item></entries>
